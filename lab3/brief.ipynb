{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from os.path import isfile, join\n",
    "\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ONLY PHOTOS FOR TRAINING\n",
    "path_serg = 'F://pictures_cv_serg/'\n",
    "path_nick = 'F://pictures_cv_nick/'\n",
    "path_roma = 'F://pictures_cv_roma/'\n",
    "path_noobject = 'F://pictures_cv_noobject/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LOADING IMAGES\n",
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    for filename in os.listdir(folder):\n",
    "        img = cv2.imread(os.path.join(folder,filename), cv2.IMREAD_GRAYSCALE)\n",
    "        if img is not None:\n",
    "            images.append(img)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "serg_images = load_images_from_folder(path_serg)\n",
    "nick_images = load_images_from_folder(path_nick)\n",
    "roma_images = load_images_from_folder(path_roma)\n",
    "noobject_images = load_images_from_folder(path_noobject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(serg_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nick_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(roma_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(noobject_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CREATING brief OBJECT\n",
    "brief = cv2.brief_create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GETTING DESCRIPTORS\n",
    "def des_with_brief(images):\n",
    "    dico = []\n",
    "    for image in images:\n",
    "        #### FOR BETTER ACCURACY CHANGE RESIZE SHAPE 4 ->(2, 3) (CAN PROCEED MUCH LONGER, BE CAREFUL)\n",
    "        image = cv2.resize(image, (image.shape[0]//4, image.shape[1]//4), interpolation = cv2.INTER_AREA)\n",
    "        kp, des = brief.detectAndCompute(image, None)\n",
    "        \n",
    "        for d in des:\n",
    "            dico.append(d)\n",
    "    return dico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "des_roma = des_with_brief(roma_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "des_serg = des_with_brief(serg_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "des_nick = des_with_brief(nick_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "des_noobject = des_with_brief(noobject_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "182297"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(des_noobject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### CLUSTERING WITH K-MEANS (MINIK-MEANS TO AVOID OUT_OF_MEMORY_ERROR)\n",
    "def clustering(descriptors):\n",
    "    k = 40 #### HYPERPARAM, CHANGE TO SEE IF BETTER/WORSE ACCURACY\n",
    "    kmeans = MiniBatchKMeans(n_clusters=k, batch_size=789, verbose=1).fit(descriptors)\n",
    "    return kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptors = des_serg + des_nick+des_roma+des_noobject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init 1/3 with method: k-means++\n",
      "Inertia for init 1/3: 212355556.000866\n",
      "Init 2/3 with method: k-means++\n",
      "Inertia for init 2/3: 211357735.063264\n",
      "Init 3/3 with method: k-means++\n",
      "Inertia for init 3/3: 213063173.456655\n",
      "Minibatch iteration 1/73500: mean batch inertia: 91337.471442, ewa inertia: 91337.471442 \n",
      "Minibatch iteration 2/73500: mean batch inertia: 92964.475028, ewa inertia: 91341.904480 \n",
      "Minibatch iteration 3/73500: mean batch inertia: 90252.171046, ewa inertia: 91338.935323 \n",
      "Minibatch iteration 4/73500: mean batch inertia: 90463.864111, ewa inertia: 91336.551048 \n",
      "Minibatch iteration 5/73500: mean batch inertia: 89460.905943, ewa inertia: 91331.440545 \n",
      "Minibatch iteration 6/73500: mean batch inertia: 89426.999218, ewa inertia: 91326.251583 \n",
      "Minibatch iteration 7/73500: mean batch inertia: 91349.781386, ewa inertia: 91326.315693 \n",
      "Minibatch iteration 8/73500: mean batch inertia: 90251.440633, ewa inertia: 91323.387020 \n",
      "Minibatch iteration 9/73500: mean batch inertia: 89509.345108, ewa inertia: 91318.444366 \n",
      "Minibatch iteration 10/73500: mean batch inertia: 87958.791770, ewa inertia: 91309.290441 \n",
      "Minibatch iteration 11/73500: mean batch inertia: 88237.052759, ewa inertia: 91300.919626 \n",
      "Minibatch iteration 12/73500: mean batch inertia: 90208.738849, ewa inertia: 91297.943801 \n",
      "Minibatch iteration 13/73500: mean batch inertia: 89157.280457, ewa inertia: 91292.111213 \n",
      "Minibatch iteration 14/73500: mean batch inertia: 90645.742440, ewa inertia: 91290.350075 \n",
      "Minibatch iteration 15/73500: mean batch inertia: 87786.717934, ewa inertia: 91280.803855 \n",
      "Minibatch iteration 16/73500: mean batch inertia: 88539.611473, ewa inertia: 91273.335027 \n",
      "Minibatch iteration 17/73500: mean batch inertia: 87810.486796, ewa inertia: 91263.899929 \n",
      "Minibatch iteration 18/73500: mean batch inertia: 90770.176752, ewa inertia: 91262.554700 \n",
      "Minibatch iteration 19/73500: mean batch inertia: 87508.045943, ewa inertia: 91252.324925 \n",
      "Minibatch iteration 20/73500: mean batch inertia: 88511.770941, ewa inertia: 91244.857837 \n",
      "Minibatch iteration 21/73500: mean batch inertia: 89000.155307, ewa inertia: 91238.741777 \n",
      "Minibatch iteration 22/73500: mean batch inertia: 85821.178232, ewa inertia: 91223.980737 \n",
      "Minibatch iteration 23/73500: mean batch inertia: 90020.168917, ewa inertia: 91220.700754 \n",
      "Minibatch iteration 24/73500: mean batch inertia: 88533.274617, ewa inertia: 91213.378421 \n",
      "Minibatch iteration 25/73500: mean batch inertia: 89123.504892, ewa inertia: 91207.684218 \n",
      "Minibatch iteration 26/73500: mean batch inertia: 89282.596663, ewa inertia: 91202.439002 \n",
      "Minibatch iteration 27/73500: mean batch inertia: 87931.400486, ewa inertia: 91193.526521 \n",
      "Minibatch iteration 28/73500: mean batch inertia: 89340.238556, ewa inertia: 91188.476934 \n",
      "Minibatch iteration 29/73500: mean batch inertia: 87783.799191, ewa inertia: 91179.200332 \n",
      "Minibatch iteration 30/73500: mean batch inertia: 88745.700071, ewa inertia: 91172.569861 \n",
      "Minibatch iteration 31/73500: mean batch inertia: 88702.426347, ewa inertia: 91165.839550 \n",
      "Minibatch iteration 32/73500: mean batch inertia: 88170.163604, ewa inertia: 91157.677340 \n",
      "Minibatch iteration 33/73500: mean batch inertia: 89472.624982, ewa inertia: 91153.086139 \n",
      "Minibatch iteration 34/73500: mean batch inertia: 87553.722625, ewa inertia: 91143.279083 \n",
      "Minibatch iteration 35/73500: mean batch inertia: 87380.113101, ewa inertia: 91133.025721 \n",
      "Minibatch iteration 36/73500: mean batch inertia: 88349.295876, ewa inertia: 91125.440992 \n",
      "Minibatch iteration 37/73500: mean batch inertia: 87587.350554, ewa inertia: 91115.800885 \n",
      "Minibatch iteration 38/73500: mean batch inertia: 88058.252484, ewa inertia: 91107.470093 \n",
      "Minibatch iteration 39/73500: mean batch inertia: 87564.369196, ewa inertia: 91097.816334 \n",
      "Minibatch iteration 40/73500: mean batch inertia: 88797.292458, ewa inertia: 91091.548180 \n",
      "Minibatch iteration 41/73500: mean batch inertia: 87586.784192, ewa inertia: 91081.998876 \n",
      "Minibatch iteration 42/73500: mean batch inertia: 88443.991263, ewa inertia: 91074.811192 \n",
      "Minibatch iteration 43/73500: mean batch inertia: 87404.223955, ewa inertia: 91064.810075 \n",
      "Minibatch iteration 44/73500: mean batch inertia: 89117.105401, ewa inertia: 91059.503235 \n",
      "Minibatch iteration 45/73500: mean batch inertia: 88790.030723, ewa inertia: 91053.319685 \n",
      "Minibatch iteration 46/73500: mean batch inertia: 87830.218402, ewa inertia: 91044.537817 \n",
      "Minibatch iteration 47/73500: mean batch inertia: 88707.695872, ewa inertia: 91038.170708 \n",
      "Minibatch iteration 48/73500: mean batch inertia: 87938.250992, ewa inertia: 91029.724469 \n",
      "Minibatch iteration 49/73500: mean batch inertia: 88466.437409, ewa inertia: 91022.740373 \n",
      "Minibatch iteration 50/73500: mean batch inertia: 89160.177870, ewa inertia: 91017.665516 \n",
      "Minibatch iteration 51/73500: mean batch inertia: 89462.893592, ewa inertia: 91013.429285 \n",
      "Minibatch iteration 52/73500: mean batch inertia: 87044.977993, ewa inertia: 91002.616589 \n",
      "Minibatch iteration 53/73500: mean batch inertia: 88841.124287, ewa inertia: 90996.727249 \n",
      "Minibatch iteration 54/73500: mean batch inertia: 86962.198627, ewa inertia: 90985.734515 \n",
      "Minibatch iteration 55/73500: mean batch inertia: 88365.634602, ewa inertia: 90978.595623 \n",
      "Minibatch iteration 56/73500: mean batch inertia: 88778.216768, ewa inertia: 90972.600330 \n",
      "Minibatch iteration 57/73500: mean batch inertia: 89260.490526, ewa inertia: 90967.935406 \n",
      "Minibatch iteration 58/73500: mean batch inertia: 88946.101207, ewa inertia: 90962.426588 \n",
      "Minibatch iteration 59/73500: mean batch inertia: 89087.827325, ewa inertia: 90957.318935 \n",
      "Minibatch iteration 60/73500: mean batch inertia: 88800.430506, ewa inertia: 90951.442139 \n",
      "Minibatch iteration 61/73500: mean batch inertia: 88085.452509, ewa inertia: 90943.633280 \n",
      "Minibatch iteration 62/73500: mean batch inertia: 87846.697018, ewa inertia: 90935.195170 \n",
      "Minibatch iteration 63/73500: mean batch inertia: 88229.765858, ewa inertia: 90927.823784 \n",
      "Minibatch iteration 64/73500: mean batch inertia: 88107.148718, ewa inertia: 90920.138393 \n",
      "Minibatch iteration 65/73500: mean batch inertia: 89557.211425, ewa inertia: 90916.424875 \n",
      "Minibatch iteration 66/73500: mean batch inertia: 89248.500520, ewa inertia: 90911.880341 \n",
      "Minibatch iteration 67/73500: mean batch inertia: 87110.000399, ewa inertia: 90901.521496 \n",
      "Minibatch iteration 68/73500: mean batch inertia: 90420.253054, ewa inertia: 90900.210202 \n",
      "Minibatch iteration 69/73500: mean batch inertia: 88673.218084, ewa inertia: 90894.142397 \n",
      "Minibatch iteration 70/73500: mean batch inertia: 86469.337465, ewa inertia: 90882.086290 \n",
      "Minibatch iteration 71/73500: mean batch inertia: 87846.299038, ewa inertia: 90873.814790 \n",
      "Minibatch iteration 72/73500: mean batch inertia: 89148.930505, ewa inertia: 90869.115060 \n",
      "Minibatch iteration 73/73500: mean batch inertia: 88580.884911, ewa inertia: 90862.880402 \n",
      "Minibatch iteration 74/73500: mean batch inertia: 88472.398095, ewa inertia: 90856.367141 \n",
      "Minibatch iteration 75/73500: mean batch inertia: 87731.532336, ewa inertia: 90847.853017 \n",
      "Minibatch iteration 76/73500: mean batch inertia: 87143.432772, ewa inertia: 90837.759717 \n",
      "Minibatch iteration 77/73500: mean batch inertia: 88364.512366, ewa inertia: 90831.020949 \n",
      "Minibatch iteration 78/73500: mean batch inertia: 86168.308407, ewa inertia: 90818.316624 \n",
      "Minibatch iteration 79/73500: mean batch inertia: 87961.716748, ewa inertia: 90810.533350 \n",
      "Minibatch iteration 80/73500: mean batch inertia: 88718.231483, ewa inertia: 90804.832530 \n",
      "Minibatch iteration 81/73500: mean batch inertia: 87541.382307, ewa inertia: 90795.940725 \n",
      "Minibatch iteration 82/73500: mean batch inertia: 87349.531747, ewa inertia: 90786.550419 \n",
      "Minibatch iteration 83/73500: mean batch inertia: 89501.285520, ewa inertia: 90783.048504 \n",
      "Minibatch iteration 84/73500: mean batch inertia: 87418.611988, ewa inertia: 90773.881546 \n",
      "Minibatch iteration 85/73500: mean batch inertia: 87389.465103, ewa inertia: 90764.660148 \n",
      "Minibatch iteration 86/73500: mean batch inertia: 88790.451928, ewa inertia: 90759.281094 \n",
      "Minibatch iteration 87/73500: mean batch inertia: 87959.295393, ewa inertia: 90751.652074 \n",
      "Minibatch iteration 88/73500: mean batch inertia: 88757.685556, ewa inertia: 90746.219186 \n",
      "Minibatch iteration 89/73500: mean batch inertia: 87988.416559, ewa inertia: 90738.705100 \n",
      "Minibatch iteration 90/73500: mean batch inertia: 86892.302446, ewa inertia: 90728.224946 \n",
      "Minibatch iteration 91/73500: mean batch inertia: 87905.003381, ewa inertia: 90720.532616 \n",
      "Minibatch iteration 92/73500: mean batch inertia: 88242.289002, ewa inertia: 90713.780235 \n",
      "Minibatch iteration 93/73500: mean batch inertia: 88731.610102, ewa inertia: 90708.379488 \n",
      "Minibatch iteration 94/73500: mean batch inertia: 88033.203663, ewa inertia: 90701.090533 \n",
      "Minibatch iteration 95/73500: mean batch inertia: 87542.830909, ewa inertia: 90692.485337 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch iteration 96/73500: mean batch inertia: 87023.224475, ewa inertia: 90682.487834 \n",
      "Minibatch iteration 97/73500: mean batch inertia: 87484.619383, ewa inertia: 90673.774717 \n",
      "Minibatch iteration 98/73500: mean batch inertia: 88644.178328, ewa inertia: 90668.244749 \n",
      "Minibatch iteration 99/73500: mean batch inertia: 87367.126173, ewa inertia: 90659.250311 \n",
      "Minibatch iteration 100/73500: mean batch inertia: 87387.337958, ewa inertia: 90650.335449 \n",
      "Minibatch iteration 101/73500: mean batch inertia: 87785.304956, ewa inertia: 90642.529204 \n",
      "Minibatch iteration 102/73500: mean batch inertia: 88528.965108, ewa inertia: 90636.770452 \n",
      "Minibatch iteration 103/73500: mean batch inertia: 87264.735660, ewa inertia: 90627.582791 \n",
      "Minibatch iteration 104/73500: mean batch inertia: 86848.834700, ewa inertia: 90617.286972 \n",
      "Minibatch iteration 105/73500: mean batch inertia: 87029.273688, ewa inertia: 90607.510842 \n",
      "Minibatch iteration 106/73500: mean batch inertia: 88774.151011, ewa inertia: 90602.515553 \n",
      "Minibatch iteration 107/73500: mean batch inertia: 88480.485033, ewa inertia: 90596.733733 \n",
      "Minibatch iteration 108/73500: mean batch inertia: 87260.182287, ewa inertia: 90587.642751 \n",
      "Minibatch iteration 109/73500: mean batch inertia: 86745.066359, ewa inertia: 90577.173022 \n",
      "Minibatch iteration 110/73500: mean batch inertia: 87167.615902, ewa inertia: 90567.883125 \n",
      "Minibatch iteration 111/73500: mean batch inertia: 87143.980831, ewa inertia: 90558.554142 \n",
      "Minibatch iteration 112/73500: mean batch inertia: 89130.027188, ewa inertia: 90554.661886 \n",
      "Minibatch iteration 113/73500: mean batch inertia: 88216.720579, ewa inertia: 90548.291782 \n",
      "Minibatch iteration 114/73500: mean batch inertia: 87563.482028, ewa inertia: 90540.159178 \n",
      "Minibatch iteration 115/73500: mean batch inertia: 89512.288853, ewa inertia: 90537.358577 \n",
      "Minibatch iteration 116/73500: mean batch inertia: 89288.293212, ewa inertia: 90533.955294 \n",
      "Minibatch iteration 117/73500: mean batch inertia: 89559.271825, ewa inertia: 90531.299609 \n",
      "Minibatch iteration 118/73500: mean batch inertia: 88676.449935, ewa inertia: 90526.245767 \n",
      "Minibatch iteration 119/73500: mean batch inertia: 87082.161412, ewa inertia: 90516.861795 \n",
      "Minibatch iteration 120/73500: mean batch inertia: 87435.443554, ewa inertia: 90508.465966 \n",
      "Minibatch iteration 121/73500: mean batch inertia: 88907.605520, ewa inertia: 90504.104159 \n",
      "Minibatch iteration 122/73500: mean batch inertia: 86576.587954, ewa inertia: 90493.402997 \n",
      "Minibatch iteration 123/73500: mean batch inertia: 87090.684446, ewa inertia: 90484.131733 \n",
      "Minibatch iteration 124/73500: mean batch inertia: 88081.562977, ewa inertia: 90477.585541 \n",
      "Minibatch iteration 125/73500: mean batch inertia: 87398.425571, ewa inertia: 90469.195864 \n",
      "Minibatch iteration 126/73500: mean batch inertia: 87438.486236, ewa inertia: 90460.938199 \n",
      "Minibatch iteration 127/73500: mean batch inertia: 90850.366129, ewa inertia: 90461.999260 \n",
      "Minibatch iteration 128/73500: mean batch inertia: 89516.978705, ewa inertia: 90459.424396 \n",
      "Minibatch iteration 129/73500: mean batch inertia: 88547.771476, ewa inertia: 90454.215784 \n",
      "Minibatch iteration 130/73500: mean batch inertia: 87556.469238, ewa inertia: 90446.320399 \n",
      "Minibatch iteration 131/73500: mean batch inertia: 88911.777562, ewa inertia: 90442.139286 \n",
      "Minibatch iteration 132/73500: mean batch inertia: 87333.996043, ewa inertia: 90433.670640 \n",
      "Minibatch iteration 133/73500: mean batch inertia: 87275.626132, ewa inertia: 90425.066030 \n",
      "Minibatch iteration 134/73500: mean batch inertia: 87986.003201, ewa inertia: 90418.420404 \n",
      "Minibatch iteration 135/73500: mean batch inertia: 87927.541895, ewa inertia: 90411.633597 \n",
      "Minibatch iteration 136/73500: mean batch inertia: 87590.118532, ewa inertia: 90403.945916 \n",
      "Minibatch iteration 137/73500: mean batch inertia: 85438.235823, ewa inertia: 90390.416025 \n",
      "Minibatch iteration 138/73500: mean batch inertia: 87820.751726, ewa inertia: 90383.414554 \n",
      "Minibatch iteration 139/73500: mean batch inertia: 86951.139446, ewa inertia: 90374.062758 \n",
      "Minibatch iteration 140/73500: mean batch inertia: 87453.058090, ewa inertia: 90366.104002 \n",
      "Minibatch iteration 141/73500: mean batch inertia: 87555.806088, ewa inertia: 90358.446884 \n",
      "Minibatch iteration 142/73500: mean batch inertia: 87663.692078, ewa inertia: 90351.104583 \n",
      "Minibatch iteration 143/73500: mean batch inertia: 87361.231223, ewa inertia: 90342.958183 \n",
      "Minibatch iteration 144/73500: mean batch inertia: 87950.726795, ewa inertia: 90336.440157 \n",
      "Minibatch iteration 145/73500: mean batch inertia: 86465.107200, ewa inertia: 90325.892076 \n",
      "Minibatch iteration 146/73500: mean batch inertia: 89042.549120, ewa inertia: 90322.395397 \n",
      "Minibatch iteration 147/73500: mean batch inertia: 86717.813459, ewa inertia: 90312.574123 \n",
      "Minibatch iteration 148/73500: mean batch inertia: 86576.873712, ewa inertia: 90302.395595 \n",
      "Minibatch iteration 149/73500: mean batch inertia: 87421.840910, ewa inertia: 90294.547052 \n",
      "Minibatch iteration 150/73500: mean batch inertia: 86777.584824, ewa inertia: 90284.964512 \n",
      "Minibatch iteration 151/73500: mean batch inertia: 87885.939030, ewa inertia: 90278.427973 \n",
      "Minibatch iteration 152/73500: mean batch inertia: 87567.005241, ewa inertia: 90271.040258 \n",
      "Minibatch iteration 153/73500: mean batch inertia: 88373.035925, ewa inertia: 90265.868834 \n",
      "Minibatch iteration 154/73500: mean batch inertia: 86798.122886, ewa inertia: 90256.420392 \n",
      "Minibatch iteration 155/73500: mean batch inertia: 88192.299735, ewa inertia: 90250.796357 \n",
      "Minibatch iteration 156/73500: mean batch inertia: 89253.974314, ewa inertia: 90248.080352 \n",
      "Minibatch iteration 157/73500: mean batch inertia: 88023.407071, ewa inertia: 90242.018865 \n",
      "Minibatch iteration 158/73500: mean batch inertia: 88833.312810, ewa inertia: 90238.180614 \n",
      "Minibatch iteration 159/73500: mean batch inertia: 88997.257977, ewa inertia: 90234.799517 \n",
      "Minibatch iteration 160/73500: mean batch inertia: 87481.371054, ewa inertia: 90227.297350 \n",
      "Minibatch iteration 161/73500: mean batch inertia: 88695.624882, ewa inertia: 90223.124057 \n",
      "Minibatch iteration 162/73500: mean batch inertia: 85630.318827, ewa inertia: 90210.610206 \n",
      "Minibatch iteration 163/73500: mean batch inertia: 88368.803707, ewa inertia: 90205.591903 \n",
      "Minibatch iteration 164/73500: mean batch inertia: 87069.302037, ewa inertia: 90197.046567 \n",
      "Minibatch iteration 165/73500: mean batch inertia: 87751.254129, ewa inertia: 90190.382605 \n",
      "Minibatch iteration 166/73500: mean batch inertia: 86156.494587, ewa inertia: 90179.391615 \n",
      "Minibatch iteration 167/73500: mean batch inertia: 88180.961380, ewa inertia: 90173.946565 \n",
      "Minibatch iteration 168/73500: mean batch inertia: 88271.445252, ewa inertia: 90168.762888 \n",
      "Minibatch iteration 169/73500: mean batch inertia: 88878.354305, ewa inertia: 90165.246958 \n",
      "Minibatch iteration 170/73500: mean batch inertia: 87765.863478, ewa inertia: 90158.709445 \n",
      "Minibatch iteration 171/73500: mean batch inertia: 86886.890948, ewa inertia: 90149.794839 \n",
      "Minibatch iteration 172/73500: mean batch inertia: 88154.538857, ewa inertia: 90144.358437 \n",
      "Minibatch iteration 173/73500: mean batch inertia: 89160.419214, ewa inertia: 90141.677533 \n",
      "Minibatch iteration 174/73500: mean batch inertia: 87809.378259, ewa inertia: 90135.322802 \n",
      "Minibatch iteration 175/73500: mean batch inertia: 89235.448049, ewa inertia: 90132.870945 \n",
      "Minibatch iteration 176/73500: mean batch inertia: 88933.327673, ewa inertia: 90129.602593 \n",
      "Minibatch iteration 177/73500: mean batch inertia: 88108.921501, ewa inertia: 90124.096916 \n",
      "Minibatch iteration 178/73500: mean batch inertia: 87727.210278, ewa inertia: 90117.566206 \n",
      "Minibatch iteration 179/73500: mean batch inertia: 87284.283815, ewa inertia: 90109.846464 \n",
      "Minibatch iteration 180/73500: mean batch inertia: 87609.005603, ewa inertia: 90103.032513 \n",
      "Minibatch iteration 181/73500: mean batch inertia: 86859.694393, ewa inertia: 90094.195506 \n",
      "Minibatch iteration 182/73500: mean batch inertia: 87289.495547, ewa inertia: 90086.553642 \n",
      "Minibatch iteration 183/73500: mean batch inertia: 88096.443262, ewa inertia: 90081.131260 \n",
      "Minibatch iteration 184/73500: mean batch inertia: 89342.391462, ewa inertia: 90079.118442 \n",
      "Minibatch iteration 185/73500: mean batch inertia: 86903.757995, ewa inertia: 90070.466652 \n",
      "Minibatch iteration 186/73500: mean batch inertia: 88398.107481, ewa inertia: 90065.910035 \n",
      "Minibatch iteration 187/73500: mean batch inertia: 86383.413958, ewa inertia: 90055.876471 \n",
      "Minibatch iteration 188/73500: mean batch inertia: 88771.170085, ewa inertia: 90052.376078 \n",
      "Minibatch iteration 189/73500: mean batch inertia: 88654.893913, ewa inertia: 90048.568409 \n",
      "Minibatch iteration 190/73500: mean batch inertia: 87222.388432, ewa inertia: 90040.868018 \n",
      "Minibatch iteration 191/73500: mean batch inertia: 87721.250218, ewa inertia: 90034.547840 \n",
      "Minibatch iteration 192/73500: mean batch inertia: 88283.468867, ewa inertia: 90029.776738 \n",
      "Minibatch iteration 193/73500: mean batch inertia: 84977.679082, ewa inertia: 90016.011470 \n",
      "Minibatch iteration 194/73500: mean batch inertia: 87046.390465, ewa inertia: 90007.920251 \n",
      "Minibatch iteration 195/73500: mean batch inertia: 88278.783841, ewa inertia: 90003.208935 \n",
      "Minibatch iteration 196/73500: mean batch inertia: 88787.497305, ewa inertia: 89999.896529 \n",
      "Minibatch iteration 197/73500: mean batch inertia: 87644.362860, ewa inertia: 89993.478492 \n",
      "Minibatch iteration 198/73500: mean batch inertia: 88509.109250, ewa inertia: 89989.434085 \n",
      "Minibatch iteration 199/73500: mean batch inertia: 87088.956931, ewa inertia: 89981.531259 \n",
      "Minibatch iteration 200/73500: mean batch inertia: 86665.615635, ewa inertia: 89972.496503 \n",
      "Minibatch iteration 201/73500: mean batch inertia: 89601.154335, ewa inertia: 89971.484721 \n",
      "Minibatch iteration 202/73500: mean batch inertia: 87150.639599, ewa inertia: 89963.798866 \n",
      "Minibatch iteration 203/73500: mean batch inertia: 87577.573979, ewa inertia: 89957.297205 \n",
      "Minibatch iteration 204/73500: mean batch inertia: 87805.934099, ewa inertia: 89951.435464 \n",
      "Minibatch iteration 205/73500: mean batch inertia: 88418.774088, ewa inertia: 89947.259477 \n",
      "Minibatch iteration 206/73500: mean batch inertia: 86814.354868, ewa inertia: 89938.723365 \n",
      "Minibatch iteration 207/73500: mean batch inertia: 85909.147827, ewa inertia: 89927.744126 \n",
      "Minibatch iteration 208/73500: mean batch inertia: 88528.501250, ewa inertia: 89923.931659 \n",
      "Minibatch iteration 209/73500: mean batch inertia: 87761.621371, ewa inertia: 89918.040090 \n",
      "Minibatch iteration 210/73500: mean batch inertia: 87981.602874, ewa inertia: 89912.763950 \n",
      "Minibatch iteration 211/73500: mean batch inertia: 88166.655707, ewa inertia: 89908.006391 \n",
      "Minibatch iteration 212/73500: mean batch inertia: 85694.556022, ewa inertia: 89896.526155 \n",
      "Minibatch iteration 213/73500: mean batch inertia: 88526.620184, ewa inertia: 89892.793622 \n",
      "Minibatch iteration 214/73500: mean batch inertia: 87916.093054, ewa inertia: 89887.407777 \n",
      "Minibatch iteration 215/73500: mean batch inertia: 86565.212456, ewa inertia: 89878.355912 \n",
      "Minibatch iteration 216/73500: mean batch inertia: 88315.732230, ewa inertia: 89874.098287 \n",
      "Minibatch iteration 217/73500: mean batch inertia: 85815.801942, ewa inertia: 89863.040794 \n",
      "Minibatch iteration 218/73500: mean batch inertia: 88692.219445, ewa inertia: 89859.850699 \n",
      "Minibatch iteration 219/73500: mean batch inertia: 88026.594238, ewa inertia: 89854.855691 \n",
      "Minibatch iteration 220/73500: mean batch inertia: 88106.988104, ewa inertia: 89850.093340 \n",
      "Minibatch iteration 221/73500: mean batch inertia: 87394.304364, ewa inertia: 89843.402140 \n",
      "Minibatch iteration 222/73500: mean batch inertia: 87028.378276, ewa inertia: 89835.732146 \n",
      "Minibatch iteration 223/73500: mean batch inertia: 87822.829303, ewa inertia: 89830.247662 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch iteration 224/73500: mean batch inertia: 90483.107519, ewa inertia: 89832.026486 \n",
      "Minibatch iteration 225/73500: mean batch inertia: 89438.016643, ewa inertia: 89830.952942 \n",
      "Minibatch iteration 226/73500: mean batch inertia: 89631.788527, ewa inertia: 89830.410286 \n",
      "Minibatch iteration 227/73500: mean batch inertia: 88157.142608, ewa inertia: 89825.851193 \n",
      "Minibatch iteration 228/73500: mean batch inertia: 85496.348768, ewa inertia: 89814.054755 \n",
      "Minibatch iteration 229/73500: mean batch inertia: 87941.688294, ewa inertia: 89808.953185 \n",
      "Minibatch iteration 230/73500: mean batch inertia: 87840.891902, ewa inertia: 89803.590880 \n",
      "Minibatch iteration 231/73500: mean batch inertia: 88563.045768, ewa inertia: 89800.210811 \n",
      "Minibatch iteration 232/73500: mean batch inertia: 86151.385969, ewa inertia: 89790.268990 \n",
      "Minibatch iteration 233/73500: mean batch inertia: 87491.910120, ewa inertia: 89784.006734 \n",
      "Minibatch iteration 234/73500: mean batch inertia: 88760.989052, ewa inertia: 89781.219355 \n",
      "Minibatch iteration 235/73500: mean batch inertia: 88055.507612, ewa inertia: 89776.517370 \n",
      "Minibatch iteration 236/73500: mean batch inertia: 87826.757627, ewa inertia: 89771.204930 \n",
      "Minibatch iteration 237/73500: mean batch inertia: 87816.676788, ewa inertia: 89765.879498 \n",
      "Minibatch iteration 238/73500: mean batch inertia: 86926.945621, ewa inertia: 89758.144358 \n",
      "Minibatch iteration 239/73500: mean batch inertia: 86847.350966, ewa inertia: 89750.213424 \n",
      "Minibatch iteration 240/73500: mean batch inertia: 86624.579648, ewa inertia: 89741.697122 \n",
      "Minibatch iteration 241/73500: mean batch inertia: 86532.398068, ewa inertia: 89732.952861 \n",
      "Minibatch iteration 242/73500: mean batch inertia: 87766.376730, ewa inertia: 89727.594602 \n",
      "Minibatch iteration 243/73500: mean batch inertia: 86909.725352, ewa inertia: 89719.916855 \n",
      "Minibatch iteration 244/73500: mean batch inertia: 87931.503524, ewa inertia: 89715.044030 \n",
      "Minibatch iteration 245/73500: mean batch inertia: 88419.733401, ewa inertia: 89711.514744 \n",
      "Minibatch iteration 246/73500: mean batch inertia: 89249.107902, ewa inertia: 89710.254841 \n",
      "Minibatch iteration 247/73500: mean batch inertia: 86924.794539, ewa inertia: 89702.665398 \n",
      "Minibatch iteration 248/73500: mean batch inertia: 87345.859399, ewa inertia: 89696.243893 \n",
      "Minibatch iteration 249/73500: mean batch inertia: 87405.735476, ewa inertia: 89690.003028 \n",
      "Minibatch iteration 250/73500: mean batch inertia: 88024.855672, ewa inertia: 89685.466061 \n",
      "Minibatch iteration 251/73500: mean batch inertia: 85472.607780, ewa inertia: 89673.987438 \n",
      "Minibatch iteration 252/73500: mean batch inertia: 88069.594479, ewa inertia: 89669.616006 \n",
      "Minibatch iteration 253/73500: mean batch inertia: 88051.554670, ewa inertia: 89665.207333 \n",
      "Minibatch iteration 254/73500: mean batch inertia: 87564.015948, ewa inertia: 89659.482293 \n",
      "Minibatch iteration 255/73500: mean batch inertia: 88329.735824, ewa inertia: 89655.859181 \n",
      "Minibatch iteration 256/73500: mean batch inertia: 88590.196736, ewa inertia: 89652.955609 \n",
      "Minibatch iteration 257/73500: mean batch inertia: 87507.704768, ewa inertia: 89647.110521 \n",
      "Minibatch iteration 258/73500: mean batch inertia: 88755.634896, ewa inertia: 89644.681550 \n",
      "Minibatch iteration 259/73500: mean batch inertia: 88009.374361, ewa inertia: 89640.225887 \n",
      "Minibatch iteration 260/73500: mean batch inertia: 86453.700303, ewa inertia: 89631.543676 \n",
      "Minibatch iteration 261/73500: mean batch inertia: 89032.472631, ewa inertia: 89629.911409 \n",
      "Minibatch iteration 262/73500: mean batch inertia: 87192.743799, ewa inertia: 89623.270946 \n",
      "Minibatch iteration 263/73500: mean batch inertia: 87464.556553, ewa inertia: 89617.389175 \n",
      "Minibatch iteration 264/73500: mean batch inertia: 89548.903856, ewa inertia: 89617.202575 \n",
      "Minibatch iteration 265/73500: mean batch inertia: 87363.486689, ewa inertia: 89611.061957 \n",
      "Minibatch iteration 266/73500: mean batch inertia: 86958.931110, ewa inertia: 89603.835792 \n",
      "Minibatch iteration 267/73500: mean batch inertia: 86923.913560, ewa inertia: 89596.533904 \n",
      "Minibatch iteration 268/73500: mean batch inertia: 86525.941121, ewa inertia: 89588.167571 \n",
      "Minibatch iteration 269/73500: mean batch inertia: 87500.516095, ewa inertia: 89582.479422 \n",
      "Minibatch iteration 270/73500: mean batch inertia: 88864.681156, ewa inertia: 89580.523663 \n",
      "Minibatch iteration 271/73500: mean batch inertia: 88846.882561, ewa inertia: 89578.524738 \n",
      "Minibatch iteration 272/73500: mean batch inertia: 86710.063789, ewa inertia: 89570.709146 \n",
      "Minibatch iteration 273/73500: mean batch inertia: 88299.961776, ewa inertia: 89567.246786 \n",
      "Minibatch iteration 274/73500: mean batch inertia: 87446.146327, ewa inertia: 89561.467501 \n",
      "Minibatch iteration 275/73500: mean batch inertia: 88565.345207, ewa inertia: 89558.753402 \n",
      "Minibatch iteration 276/73500: mean batch inertia: 87639.181514, ewa inertia: 89553.523214 \n",
      "Minibatch iteration 277/73500: mean batch inertia: 88812.913765, ewa inertia: 89551.505302 \n",
      "Minibatch iteration 278/73500: mean batch inertia: 88791.767636, ewa inertia: 89549.435272 \n",
      "Minibatch iteration 279/73500: mean batch inertia: 87195.613552, ewa inertia: 89543.021899 \n",
      "Minibatch iteration 280/73500: mean batch inertia: 85418.048149, ewa inertia: 89531.782732 \n",
      "Minibatch iteration 281/73500: mean batch inertia: 87401.333122, ewa inertia: 89525.977973 \n",
      "Minibatch iteration 282/73500: mean batch inertia: 87895.794898, ewa inertia: 89521.536272 \n",
      "Minibatch iteration 283/73500: mean batch inertia: 87601.347614, ewa inertia: 89516.304403 \n",
      "Minibatch iteration 284/73500: mean batch inertia: 87594.972861, ewa inertia: 89511.069421 \n",
      "Minibatch iteration 285/73500: mean batch inertia: 87537.077212, ewa inertia: 89505.690955 \n",
      "Minibatch iteration 286/73500: mean batch inertia: 87345.911705, ewa inertia: 89499.806283 \n",
      "Minibatch iteration 287/73500: mean batch inertia: 87081.641461, ewa inertia: 89493.217596 \n",
      "Minibatch iteration 288/73500: mean batch inertia: 87981.558716, ewa inertia: 89489.098834 \n",
      "Minibatch iteration 289/73500: mean batch inertia: 86806.779598, ewa inertia: 89481.790415 \n",
      "Minibatch iteration 290/73500: mean batch inertia: 86276.846038, ewa inertia: 89473.058019 \n",
      "Minibatch iteration 291/73500: mean batch inertia: 87862.518634, ewa inertia: 89468.669841 \n",
      "Minibatch iteration 292/73500: mean batch inertia: 87858.804934, ewa inertia: 89464.283500 \n",
      "Minibatch iteration 293/73500: mean batch inertia: 88279.092412, ewa inertia: 89461.054252 \n",
      "Minibatch iteration 294/73500: mean batch inertia: 85991.135939, ewa inertia: 89451.599891 \n",
      "Minibatch iteration 295/73500: mean batch inertia: 85169.561846, ewa inertia: 89439.932777 \n",
      "Minibatch iteration 296/73500: mean batch inertia: 87396.495367, ewa inertia: 89434.365097 \n",
      "Minibatch iteration 297/73500: mean batch inertia: 90022.331122, ewa inertia: 89435.967106 \n",
      "Minibatch iteration 298/73500: mean batch inertia: 86051.149233, ewa inertia: 89426.744615 \n",
      "Minibatch iteration 299/73500: mean batch inertia: 86779.513952, ewa inertia: 89419.531801 \n",
      "Minibatch iteration 300/73500: mean batch inertia: 85492.088474, ewa inertia: 89408.830838 \n",
      "Minibatch iteration 301/73500: mean batch inertia: 87793.890080, ewa inertia: 89404.430668 \n",
      "Minibatch iteration 302/73500: mean batch inertia: 88575.550198, ewa inertia: 89402.172247 \n",
      "Minibatch iteration 303/73500: mean batch inertia: 87858.495889, ewa inertia: 89397.966248 \n",
      "Minibatch iteration 304/73500: mean batch inertia: 88978.349902, ewa inertia: 89396.822934 \n",
      "Minibatch iteration 305/73500: mean batch inertia: 88455.580930, ewa inertia: 89394.258366 \n",
      "Minibatch iteration 306/73500: mean batch inertia: 88994.140979, ewa inertia: 89393.168181 \n",
      "Minibatch iteration 307/73500: mean batch inertia: 88232.828379, ewa inertia: 89390.006645 \n",
      "Minibatch iteration 308/73500: mean batch inertia: 87006.737757, ewa inertia: 89383.513038 \n",
      "Minibatch iteration 309/73500: mean batch inertia: 88412.107025, ewa inertia: 89380.866283 \n",
      "Minibatch iteration 310/73500: mean batch inertia: 86904.779568, ewa inertia: 89374.119779 \n",
      "Minibatch iteration 311/73500: mean batch inertia: 88852.381444, ewa inertia: 89372.698217 \n",
      "Minibatch iteration 312/73500: mean batch inertia: 88370.953711, ewa inertia: 89369.968800 \n",
      "Minibatch iteration 313/73500: mean batch inertia: 87438.665211, ewa inertia: 89364.706647 \n",
      "Minibatch iteration 314/73500: mean batch inertia: 87143.759750, ewa inertia: 89358.655313 \n",
      "Minibatch iteration 315/73500: mean batch inertia: 86807.750681, ewa inertia: 89351.704955 \n",
      "Minibatch iteration 316/73500: mean batch inertia: 89801.064151, ewa inertia: 89352.929308 \n",
      "Minibatch iteration 317/73500: mean batch inertia: 87255.367040, ewa inertia: 89347.214156 \n",
      "Minibatch iteration 318/73500: mean batch inertia: 87604.500930, ewa inertia: 89342.465848 \n",
      "Minibatch iteration 319/73500: mean batch inertia: 88131.704871, ewa inertia: 89339.166931 \n",
      "Minibatch iteration 320/73500: mean batch inertia: 87471.162606, ewa inertia: 89334.077247 \n",
      "Minibatch iteration 321/73500: mean batch inertia: 86598.621854, ewa inertia: 89326.624051 \n",
      "Minibatch iteration 322/73500: mean batch inertia: 86830.546333, ewa inertia: 89319.823078 \n",
      "Minibatch iteration 323/73500: mean batch inertia: 86765.940238, ewa inertia: 89312.864606 \n",
      "Minibatch iteration 324/73500: mean batch inertia: 89358.925683, ewa inertia: 89312.990107 \n",
      "Minibatch iteration 325/73500: mean batch inertia: 86739.712050, ewa inertia: 89305.978789 \n",
      "Minibatch iteration 326/73500: mean batch inertia: 89111.878537, ewa inertia: 89305.449931 \n",
      "Minibatch iteration 327/73500: mean batch inertia: 87087.681552, ewa inertia: 89299.407257 \n",
      "Minibatch iteration 328/73500: mean batch inertia: 87727.295656, ewa inertia: 89295.123782 \n",
      "Minibatch iteration 329/73500: mean batch inertia: 89640.612746, ewa inertia: 89296.065123 \n",
      "Minibatch iteration 330/73500: mean batch inertia: 86469.022799, ewa inertia: 89288.362383 \n",
      "Minibatch iteration 331/73500: mean batch inertia: 87119.916364, ewa inertia: 89282.454096 \n",
      "Minibatch iteration 332/73500: mean batch inertia: 90473.005528, ewa inertia: 89285.697949 \n",
      "Minibatch iteration 333/73500: mean batch inertia: 87788.752745, ewa inertia: 89281.619276 \n",
      "Minibatch iteration 334/73500: mean batch inertia: 87075.504770, ewa inertia: 89275.608355 \n",
      "Minibatch iteration 335/73500: mean batch inertia: 88182.119631, ewa inertia: 89272.628966 \n",
      "Minibatch iteration 336/73500: mean batch inertia: 89630.855679, ewa inertia: 89273.605014 \n",
      "Minibatch iteration 337/73500: mean batch inertia: 87973.920987, ewa inertia: 89270.063811 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch iteration 338/73500: mean batch inertia: 87919.974143, ewa inertia: 89266.385271 \n",
      "Minibatch iteration 339/73500: mean batch inertia: 86034.783141, ewa inertia: 89257.580241 \n",
      "Minibatch iteration 340/73500: mean batch inertia: 88871.468494, ewa inertia: 89256.528216 \n",
      "Minibatch iteration 341/73500: mean batch inertia: 87886.085148, ewa inertia: 89252.794220 \n",
      "Minibatch iteration 342/73500: mean batch inertia: 88995.437389, ewa inertia: 89252.093009 \n",
      "Minibatch iteration 343/73500: mean batch inertia: 88217.607601, ewa inertia: 89249.274384 \n",
      "Minibatch iteration 344/73500: mean batch inertia: 87794.088744, ewa inertia: 89245.309492 \n",
      "Minibatch iteration 345/73500: mean batch inertia: 87610.270413, ewa inertia: 89240.854560 \n",
      "Minibatch iteration 346/73500: mean batch inertia: 88711.067574, ewa inertia: 89239.411069 \n",
      "Minibatch iteration 347/73500: mean batch inertia: 85807.357405, ewa inertia: 89230.059876 \n",
      "Minibatch iteration 348/73500: mean batch inertia: 87440.577133, ewa inertia: 89225.184137 \n",
      "Minibatch iteration 349/73500: mean batch inertia: 86040.755811, ewa inertia: 89216.507640 \n",
      "Minibatch iteration 350/73500: mean batch inertia: 88772.733469, ewa inertia: 89215.298504 \n",
      "Minibatch iteration 351/73500: mean batch inertia: 88552.188724, ewa inertia: 89213.491753 \n",
      "Minibatch iteration 352/73500: mean batch inertia: 86663.216403, ewa inertia: 89206.543110 \n",
      "Minibatch iteration 353/73500: mean batch inertia: 87432.669179, ewa inertia: 89201.709900 \n",
      "Minibatch iteration 354/73500: mean batch inertia: 87214.986730, ewa inertia: 89196.296747 \n",
      "Minibatch iteration 355/73500: mean batch inertia: 88289.759370, ewa inertia: 89193.826737 \n",
      "Minibatch iteration 356/73500: mean batch inertia: 87461.217221, ewa inertia: 89189.105958 \n",
      "Minibatch iteration 357/73500: mean batch inertia: 87363.097182, ewa inertia: 89184.130698 \n",
      "Minibatch iteration 358/73500: mean batch inertia: 87024.284983, ewa inertia: 89178.245845 \n",
      "Minibatch iteration 359/73500: mean batch inertia: 87328.872580, ewa inertia: 89173.206924 \n",
      "Minibatch iteration 360/73500: mean batch inertia: 86567.381221, ewa inertia: 89166.106925 \n",
      "Minibatch iteration 361/73500: mean batch inertia: 89286.707925, ewa inertia: 89166.435522 \n",
      "Minibatch iteration 362/73500: mean batch inertia: 87731.006731, ewa inertia: 89162.524461 \n",
      "Minibatch iteration 363/73500: mean batch inertia: 87159.897219, ewa inertia: 89157.067975 \n",
      "Minibatch iteration 364/73500: mean batch inertia: 87348.063447, ewa inertia: 89152.139045 \n",
      "Minibatch iteration 365/73500: mean batch inertia: 90462.993261, ewa inertia: 89155.710683 \n",
      "Minibatch iteration 366/73500: mean batch inertia: 88195.852746, ewa inertia: 89153.095392 \n",
      "Minibatch iteration 367/73500: mean batch inertia: 87642.512486, ewa inertia: 89148.979562 \n",
      "Minibatch iteration 368/73500: mean batch inertia: 86874.288688, ewa inertia: 89142.781793 \n",
      "Minibatch iteration 369/73500: mean batch inertia: 87206.288951, ewa inertia: 89137.505501 \n",
      "Minibatch iteration 370/73500: mean batch inertia: 86507.753489, ewa inertia: 89130.340311 \n",
      "Minibatch iteration 371/73500: mean batch inertia: 88731.199447, ewa inertia: 89129.252786 \n",
      "Minibatch iteration 372/73500: mean batch inertia: 87705.064176, ewa inertia: 89125.372351 \n",
      "Minibatch iteration 373/73500: mean batch inertia: 86141.961516, ewa inertia: 89117.243559 \n",
      "Minibatch iteration 374/73500: mean batch inertia: 88805.424482, ewa inertia: 89116.393957 \n",
      "Minibatch iteration 375/73500: mean batch inertia: 86926.725012, ewa inertia: 89110.427845 \n",
      "Minibatch iteration 376/73500: mean batch inertia: 88938.914458, ewa inertia: 89109.960529 \n",
      "Minibatch iteration 377/73500: mean batch inertia: 88819.502587, ewa inertia: 89109.169128 \n",
      "Minibatch iteration 378/73500: mean batch inertia: 86618.339210, ewa inertia: 89102.382454 \n",
      "Minibatch iteration 379/73500: mean batch inertia: 87499.044754, ewa inertia: 89098.013898 \n",
      "Minibatch iteration 380/73500: mean batch inertia: 89312.991803, ewa inertia: 89098.599640 \n",
      "Minibatch iteration 381/73500: mean batch inertia: 87647.048009, ewa inertia: 89094.644650 \n",
      "Minibatch iteration 382/73500: mean batch inertia: 86051.749872, ewa inertia: 89086.353784 \n",
      "Minibatch iteration 383/73500: mean batch inertia: 86742.040623, ewa inertia: 89079.966319 \n",
      "Minibatch iteration 384/73500: mean batch inertia: 88236.742614, ewa inertia: 89077.668818 \n",
      "Minibatch iteration 385/73500: mean batch inertia: 87433.248130, ewa inertia: 89073.188324 \n",
      "Minibatch iteration 386/73500: mean batch inertia: 86685.534632, ewa inertia: 89066.682770 \n",
      "Minibatch iteration 387/73500: mean batch inertia: 88691.644910, ewa inertia: 89065.660918 \n",
      "Minibatch iteration 388/73500: mean batch inertia: 87553.459726, ewa inertia: 89061.540678 \n",
      "Minibatch iteration 389/73500: mean batch inertia: 87642.657470, ewa inertia: 89057.674698 \n",
      "Minibatch iteration 390/73500: mean batch inertia: 88873.092865, ewa inertia: 89057.171775 \n",
      "Minibatch iteration 391/73500: mean batch inertia: 87972.770416, ewa inertia: 89054.217145 \n",
      "Minibatch iteration 392/73500: mean batch inertia: 89314.819708, ewa inertia: 89054.927200 \n",
      "Minibatch iteration 393/73500: mean batch inertia: 86862.976083, ewa inertia: 89048.954870 \n",
      "Minibatch iteration 394/73500: mean batch inertia: 90549.176729, ewa inertia: 89053.042470 \n",
      "Minibatch iteration 395/73500: mean batch inertia: 89638.773597, ewa inertia: 89054.638390 \n",
      "Minibatch iteration 396/73500: mean batch inertia: 87969.662131, ewa inertia: 89051.682195 \n",
      "Minibatch iteration 397/73500: mean batch inertia: 86928.418748, ewa inertia: 89045.897016 \n",
      "Minibatch iteration 398/73500: mean batch inertia: 85013.217225, ewa inertia: 89034.909318 \n",
      "Minibatch iteration 399/73500: mean batch inertia: 87645.546540, ewa inertia: 89031.123772 \n",
      "Minibatch iteration 400/73500: mean batch inertia: 87106.741586, ewa inertia: 89025.880477 \n",
      "Minibatch iteration 401/73500: mean batch inertia: 88028.532957, ewa inertia: 89023.163040 \n",
      "Minibatch iteration 402/73500: mean batch inertia: 88072.678577, ewa inertia: 89020.573290 \n",
      "Minibatch iteration 403/73500: mean batch inertia: 87676.489491, ewa inertia: 89016.911113 \n",
      "Minibatch iteration 404/73500: mean batch inertia: 88442.810187, ewa inertia: 89015.346881 \n",
      "Minibatch iteration 405/73500: mean batch inertia: 86452.852178, ewa inertia: 89008.364944 \n",
      "Minibatch iteration 406/73500: mean batch inertia: 87671.651774, ewa inertia: 89004.722850 \n",
      "Minibatch iteration 407/73500: mean batch inertia: 88601.398188, ewa inertia: 89003.623926 \n",
      "Minibatch iteration 408/73500: mean batch inertia: 85873.756531, ewa inertia: 88995.096089 \n",
      "Minibatch iteration 409/73500: mean batch inertia: 87943.858091, ewa inertia: 88992.231819 \n",
      "Minibatch iteration 410/73500: mean batch inertia: 87523.146999, ewa inertia: 88988.229057 \n",
      "Minibatch iteration 411/73500: mean batch inertia: 89066.976574, ewa inertia: 88988.443617 \n",
      "Minibatch iteration 412/73500: mean batch inertia: 86464.253633, ewa inertia: 88981.566048 \n",
      "Minibatch iteration 413/73500: mean batch inertia: 86605.892495, ewa inertia: 88975.093136 \n",
      "Minibatch iteration 414/73500: mean batch inertia: 87315.157566, ewa inertia: 88970.570369 \n",
      "Minibatch iteration 415/73500: mean batch inertia: 85730.434546, ewa inertia: 88961.742088 \n",
      "Minibatch iteration 416/73500: mean batch inertia: 87811.895961, ewa inertia: 88958.609144 \n",
      "Minibatch iteration 417/73500: mean batch inertia: 89380.855175, ewa inertia: 88959.759622 \n",
      "Minibatch iteration 418/73500: mean batch inertia: 88331.495019, ewa inertia: 88958.047812 \n",
      "Minibatch iteration 419/73500: mean batch inertia: 87799.595978, ewa inertia: 88954.891421 \n",
      "Minibatch iteration 420/73500: mean batch inertia: 87766.801061, ewa inertia: 88951.654274 \n",
      "Minibatch iteration 421/73500: mean batch inertia: 87130.894268, ewa inertia: 88946.693315 \n",
      "Minibatch iteration 422/73500: mean batch inertia: 86645.256184, ewa inertia: 88940.422672 \n",
      "Minibatch iteration 423/73500: mean batch inertia: 87146.463035, ewa inertia: 88935.534735 \n",
      "Minibatch iteration 424/73500: mean batch inertia: 87001.552113, ewa inertia: 88930.265282 \n",
      "Minibatch iteration 425/73500: mean batch inertia: 86354.811124, ewa inertia: 88923.248035 \n",
      "Minibatch iteration 426/73500: mean batch inertia: 87505.097488, ewa inertia: 88919.384052 \n",
      "Minibatch iteration 427/73500: mean batch inertia: 88232.464506, ewa inertia: 88917.512427 \n",
      "Minibatch iteration 428/73500: mean batch inertia: 87232.670227, ewa inertia: 88912.921798 \n",
      "Minibatch iteration 429/73500: mean batch inertia: 87028.835530, ewa inertia: 88907.788296 \n",
      "Minibatch iteration 430/73500: mean batch inertia: 87220.647191, ewa inertia: 88903.191404 \n",
      "Minibatch iteration 431/73500: mean batch inertia: 89851.029312, ewa inertia: 88905.773943 \n",
      "Minibatch iteration 432/73500: mean batch inertia: 86296.229543, ewa inertia: 88898.663812 \n",
      "Minibatch iteration 433/73500: mean batch inertia: 88436.178618, ewa inertia: 88897.403695 \n",
      "Minibatch iteration 434/73500: mean batch inertia: 89216.153176, ewa inertia: 88898.272180 \n",
      "Minibatch iteration 435/73500: mean batch inertia: 87750.384984, ewa inertia: 88895.144574 \n",
      "Minibatch iteration 436/73500: mean batch inertia: 87683.510736, ewa inertia: 88891.843279 \n",
      "Minibatch iteration 437/73500: mean batch inertia: 87562.175940, ewa inertia: 88888.220382 \n",
      "Minibatch iteration 438/73500: mean batch inertia: 86566.644584, ewa inertia: 88881.894868 \n",
      "Minibatch iteration 439/73500: mean batch inertia: 89701.997163, ewa inertia: 88884.129371 \n",
      "Minibatch iteration 440/73500: mean batch inertia: 87246.922554, ewa inertia: 88879.668533 \n",
      "Minibatch iteration 441/73500: mean batch inertia: 89194.836594, ewa inertia: 88880.527260 \n",
      "Minibatch iteration 442/73500: mean batch inertia: 88386.067882, ewa inertia: 88879.180024 \n",
      "Minibatch iteration 443/73500: mean batch inertia: 87298.513937, ewa inertia: 88874.873241 \n",
      "Minibatch iteration 444/73500: mean batch inertia: 87426.124817, ewa inertia: 88870.925888 \n",
      "Minibatch iteration 445/73500: mean batch inertia: 89860.198761, ewa inertia: 88873.621324 \n",
      "Minibatch iteration 446/73500: mean batch inertia: 88214.552308, ewa inertia: 88871.825583 \n",
      "Minibatch iteration 447/73500: mean batch inertia: 88690.193736, ewa inertia: 88871.330697 \n",
      "Minibatch iteration 448/73500: mean batch inertia: 88851.084657, ewa inertia: 88871.275533 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch iteration 449/73500: mean batch inertia: 88545.032980, ewa inertia: 88870.386632 \n",
      "Minibatch iteration 450/73500: mean batch inertia: 86784.908726, ewa inertia: 88864.704406 \n",
      "Minibatch iteration 451/73500: mean batch inertia: 86464.708757, ewa inertia: 88858.165224 \n",
      "Minibatch iteration 452/73500: mean batch inertia: 87081.227977, ewa inertia: 88853.323667 \n",
      "Minibatch iteration 453/73500: mean batch inertia: 87973.646617, ewa inertia: 88850.926843 \n",
      "Minibatch iteration 454/73500: mean batch inertia: 86432.267944, ewa inertia: 88844.336810 \n",
      "Minibatch iteration 455/73500: mean batch inertia: 85940.125514, ewa inertia: 88836.423811 \n",
      "Minibatch iteration 456/73500: mean batch inertia: 87865.850621, ewa inertia: 88833.779325 \n",
      "Minibatch iteration 457/73500: mean batch inertia: 87438.299839, ewa inertia: 88829.977112 \n",
      "Minibatch iteration 458/73500: mean batch inertia: 88504.492766, ewa inertia: 88829.090277 \n",
      "Minibatch iteration 459/73500: mean batch inertia: 88384.155417, ewa inertia: 88827.877979 \n",
      "Minibatch iteration 460/73500: mean batch inertia: 87865.957933, ewa inertia: 88825.257070 \n",
      "Minibatch iteration 461/73500: mean batch inertia: 87185.387586, ewa inertia: 88820.788977 \n",
      "Minibatch iteration 462/73500: mean batch inertia: 88284.966374, ewa inertia: 88819.329040 \n",
      "Minibatch iteration 463/73500: mean batch inertia: 87955.640000, ewa inertia: 88816.975778 \n",
      "Minibatch iteration 464/73500: mean batch inertia: 86813.679424, ewa inertia: 88811.517469 \n",
      "Minibatch iteration 465/73500: mean batch inertia: 88170.463705, ewa inertia: 88809.770813 \n",
      "Minibatch iteration 466/73500: mean batch inertia: 86980.413228, ewa inertia: 88804.786428 \n",
      "Minibatch iteration 467/73500: mean batch inertia: 87841.949455, ewa inertia: 88802.163021 \n",
      "Minibatch iteration 468/73500: mean batch inertia: 87075.149370, ewa inertia: 88797.457489 \n",
      "Minibatch iteration 469/73500: mean batch inertia: 88086.520717, ewa inertia: 88795.520425 \n",
      "Minibatch iteration 470/73500: mean batch inertia: 87641.075706, ewa inertia: 88792.374952 \n",
      "Minibatch iteration 471/73500: mean batch inertia: 88274.490203, ewa inertia: 88790.963890 \n",
      "Minibatch iteration 472/73500: mean batch inertia: 86492.127086, ewa inertia: 88784.700332 \n",
      "Minibatch iteration 473/73500: mean batch inertia: 89158.138909, ewa inertia: 88785.717827 \n",
      "Minibatch iteration 474/73500: mean batch inertia: 89030.547634, ewa inertia: 88786.384906 \n",
      "Minibatch iteration 475/73500: mean batch inertia: 87853.490935, ewa inertia: 88783.843083 \n",
      "Minibatch iteration 476/73500: mean batch inertia: 86296.144635, ewa inertia: 88777.064941 \n",
      "Minibatch iteration 477/73500: mean batch inertia: 89263.117999, ewa inertia: 88778.389272 \n",
      "Minibatch iteration 478/73500: mean batch inertia: 87406.083835, ewa inertia: 88774.650201 \n",
      "Minibatch iteration 479/73500: mean batch inertia: 87168.931601, ewa inertia: 88770.275158 \n",
      "Minibatch iteration 480/73500: mean batch inertia: 86900.015047, ewa inertia: 88765.179327 \n",
      "Minibatch iteration 481/73500: mean batch inertia: 88106.548829, ewa inertia: 88763.384781 \n",
      "Minibatch iteration 482/73500: mean batch inertia: 86529.528567, ewa inertia: 88757.298273 \n",
      "Minibatch iteration 483/73500: mean batch inertia: 87436.564809, ewa inertia: 88753.699718 \n",
      "Minibatch iteration 484/73500: mean batch inertia: 86859.725848, ewa inertia: 88748.539276 \n",
      "Minibatch iteration 485/73500: mean batch inertia: 86386.466519, ewa inertia: 88742.103422 \n",
      "Minibatch iteration 486/73500: mean batch inertia: 87963.440356, ewa inertia: 88739.981827 \n",
      "Minibatch iteration 487/73500: mean batch inertia: 88753.979257, ewa inertia: 88740.019965 \n",
      "Minibatch iteration 488/73500: mean batch inertia: 87500.226891, ewa inertia: 88736.641945 \n",
      "Minibatch iteration 489/73500: mean batch inertia: 86253.072506, ewa inertia: 88729.875053 \n",
      "Minibatch iteration 490/73500: mean batch inertia: 87948.290886, ewa inertia: 88727.745499 \n",
      "Minibatch iteration 491/73500: mean batch inertia: 87591.154385, ewa inertia: 88724.648670 \n",
      "Minibatch iteration 492/73500: mean batch inertia: 88054.731756, ewa inertia: 88722.823372 \n",
      "Minibatch iteration 493/73500: mean batch inertia: 86371.477742, ewa inertia: 88716.416745 \n",
      "Minibatch iteration 494/73500: mean batch inertia: 87750.240233, ewa inertia: 88713.784239 \n",
      "Minibatch iteration 495/73500: mean batch inertia: 87599.592334, ewa inertia: 88710.748441 \n",
      "Minibatch iteration 496/73500: mean batch inertia: 87539.087118, ewa inertia: 88707.556057 \n",
      "Minibatch iteration 497/73500: mean batch inertia: 88565.588583, ewa inertia: 88707.169244 \n",
      "Minibatch iteration 498/73500: mean batch inertia: 87092.311639, ewa inertia: 88702.769300 \n",
      "Minibatch iteration 499/73500: mean batch inertia: 87771.931012, ewa inertia: 88700.233078 \n",
      "Minibatch iteration 500/73500: mean batch inertia: 87935.056356, ewa inertia: 88698.148229 \n",
      "Minibatch iteration 501/73500: mean batch inertia: 88292.561202, ewa inertia: 88697.043140 \n",
      "Minibatch iteration 502/73500: mean batch inertia: 88655.053702, ewa inertia: 88696.928733 \n",
      "Minibatch iteration 503/73500: mean batch inertia: 87832.935183, ewa inertia: 88694.574641 \n",
      "Minibatch iteration 504/73500: mean batch inertia: 86909.931174, ewa inertia: 88689.712088 \n",
      "Minibatch iteration 505/73500: mean batch inertia: 86833.942152, ewa inertia: 88684.655738 \n",
      "Minibatch iteration 506/73500: mean batch inertia: 85694.959216, ewa inertia: 88676.509820 \n",
      "Minibatch iteration 507/73500: mean batch inertia: 87374.441372, ewa inertia: 88672.962121 \n",
      "Minibatch iteration 508/73500: mean batch inertia: 86850.027991, ewa inertia: 88667.995238 \n",
      "Minibatch iteration 509/73500: mean batch inertia: 88150.248506, ewa inertia: 88666.584552 \n",
      "Minibatch iteration 510/73500: mean batch inertia: 86610.853942, ewa inertia: 88660.983377 \n",
      "Minibatch iteration 511/73500: mean batch inertia: 87107.796358, ewa inertia: 88656.751465 \n",
      "Minibatch iteration 512/73500: mean batch inertia: 87874.739671, ewa inertia: 88654.620745 \n",
      "Minibatch iteration 513/73500: mean batch inertia: 86616.819099, ewa inertia: 88649.068421 \n",
      "Minibatch iteration 514/73500: mean batch inertia: 86975.384983, ewa inertia: 88644.508196 \n",
      "Minibatch iteration 515/73500: mean batch inertia: 87444.374945, ewa inertia: 88641.238236 \n",
      "Minibatch iteration 516/73500: mean batch inertia: 86726.938258, ewa inertia: 88636.022412 \n",
      "Minibatch iteration 517/73500: mean batch inertia: 89415.911388, ewa inertia: 88638.147348 \n",
      "Minibatch iteration 518/73500: mean batch inertia: 86186.093705, ewa inertia: 88631.466325 \n",
      "Minibatch iteration 519/73500: mean batch inertia: 87752.649773, ewa inertia: 88629.071846 \n",
      "Minibatch iteration 520/73500: mean batch inertia: 87817.754601, ewa inertia: 88626.861279 \n",
      "Minibatch iteration 521/73500: mean batch inertia: 86265.243998, ewa inertia: 88620.426666 \n",
      "Minibatch iteration 522/73500: mean batch inertia: 88394.364873, ewa inertia: 88619.810723 \n",
      "Minibatch iteration 523/73500: mean batch inertia: 88758.770009, ewa inertia: 88620.189341 \n",
      "Minibatch iteration 524/73500: mean batch inertia: 89038.593288, ewa inertia: 88621.329351 \n",
      "Minibatch iteration 525/73500: mean batch inertia: 87739.032272, ewa inertia: 88618.925388 \n",
      "Minibatch iteration 526/73500: mean batch inertia: 86494.300000, ewa inertia: 88613.136498 \n",
      "Minibatch iteration 527/73500: mean batch inertia: 87897.160822, ewa inertia: 88611.185704 \n",
      "Minibatch iteration 528/73500: mean batch inertia: 88559.808723, ewa inertia: 88611.045719 \n",
      "Minibatch iteration 529/73500: mean batch inertia: 86681.954621, ewa inertia: 88605.789595 \n",
      "Minibatch iteration 530/73500: mean batch inertia: 90167.188852, ewa inertia: 88610.043883 \n",
      "Minibatch iteration 531/73500: mean batch inertia: 87600.266361, ewa inertia: 88607.292578 \n",
      "Minibatch iteration 532/73500: mean batch inertia: 88070.511906, ewa inertia: 88605.830031 \n",
      "Minibatch iteration 533/73500: mean batch inertia: 85994.454387, ewa inertia: 88598.714911 \n",
      "Minibatch iteration 534/73500: mean batch inertia: 88492.238291, ewa inertia: 88598.424798 \n",
      "Minibatch iteration 535/73500: mean batch inertia: 87193.841916, ewa inertia: 88594.597781 \n",
      "Minibatch iteration 536/73500: mean batch inertia: 87757.153642, ewa inertia: 88592.316027 \n",
      "Minibatch iteration 537/73500: mean batch inertia: 86806.444288, ewa inertia: 88587.450127 \n",
      "Minibatch iteration 538/73500: mean batch inertia: 88947.611004, ewa inertia: 88588.431444 \n",
      "Minibatch iteration 539/73500: mean batch inertia: 86678.360908, ewa inertia: 88583.227144 \n",
      "Minibatch iteration 540/73500: mean batch inertia: 88846.569048, ewa inertia: 88583.944662 \n",
      "Minibatch iteration 541/73500: mean batch inertia: 87511.978004, ewa inertia: 88581.023914 \n",
      "Minibatch iteration 542/73500: mean batch inertia: 86329.746436, ewa inertia: 88574.889939 \n",
      "Minibatch iteration 543/73500: mean batch inertia: 87587.897441, ewa inertia: 88572.200716 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch iteration 544/73500: mean batch inertia: 86949.350240, ewa inertia: 88567.778994 \n",
      "Minibatch iteration 545/73500: mean batch inertia: 86861.004727, ewa inertia: 88563.128608 \n",
      "Minibatch iteration 546/73500: mean batch inertia: 87811.193398, ewa inertia: 88561.079837 \n",
      "Minibatch iteration 547/73500: mean batch inertia: 86309.889154, ewa inertia: 88554.946099 \n",
      "Minibatch iteration 548/73500: mean batch inertia: 85769.754457, ewa inertia: 88547.357388 \n",
      "Minibatch iteration 549/73500: mean batch inertia: 87507.255989, ewa inertia: 88544.523461 \n",
      "Minibatch iteration 550/73500: mean batch inertia: 86596.050330, ewa inertia: 88539.214527 \n",
      "Minibatch iteration 551/73500: mean batch inertia: 89033.107093, ewa inertia: 88540.560218 \n",
      "Minibatch iteration 552/73500: mean batch inertia: 87770.293910, ewa inertia: 88538.461501 \n",
      "Minibatch iteration 553/73500: mean batch inertia: 87749.384621, ewa inertia: 88536.311532 \n",
      "Minibatch iteration 554/73500: mean batch inertia: 88804.736928, ewa inertia: 88537.042901 \n",
      "Minibatch iteration 555/73500: mean batch inertia: 85374.934144, ewa inertia: 88528.427217 \n",
      "Minibatch iteration 556/73500: mean batch inertia: 87446.282641, ewa inertia: 88525.478737 \n",
      "Minibatch iteration 557/73500: mean batch inertia: 87374.715220, ewa inertia: 88522.343293 \n",
      "Minibatch iteration 558/73500: mean batch inertia: 85069.623578, ewa inertia: 88512.935793 \n",
      "Minibatch iteration 559/73500: mean batch inertia: 88203.191125, ewa inertia: 88512.091842 \n",
      "Minibatch iteration 560/73500: mean batch inertia: 85983.234538, ewa inertia: 88505.201556 \n",
      "Minibatch iteration 561/73500: mean batch inertia: 85712.782128, ewa inertia: 88497.593152 \n",
      "Minibatch iteration 562/73500: mean batch inertia: 87827.086577, ewa inertia: 88495.766247 \n",
      "Minibatch iteration 563/73500: mean batch inertia: 88305.081015, ewa inertia: 88495.246694 \n",
      "Minibatch iteration 564/73500: mean batch inertia: 87600.379950, ewa inertia: 88492.808482 \n",
      "Minibatch iteration 565/73500: mean batch inertia: 86807.542393, ewa inertia: 88488.216699 \n",
      "Minibatch iteration 566/73500: mean batch inertia: 87083.923255, ewa inertia: 88484.390471 \n",
      "Minibatch iteration 567/73500: mean batch inertia: 88891.994541, ewa inertia: 88485.501055 \n",
      "Minibatch iteration 568/73500: mean batch inertia: 88012.955362, ewa inertia: 88484.213527 \n",
      "Minibatch iteration 569/73500: mean batch inertia: 86170.755727, ewa inertia: 88477.910132 \n",
      "Minibatch iteration 570/73500: mean batch inertia: 88460.883190, ewa inertia: 88477.863739 \n",
      "Minibatch iteration 571/73500: mean batch inertia: 87618.386962, ewa inertia: 88475.521954 \n",
      "Minibatch iteration 572/73500: mean batch inertia: 86204.602475, ewa inertia: 88469.334462 \n",
      "Minibatch iteration 573/73500: mean batch inertia: 88434.009480, ewa inertia: 88469.238213 \n",
      "Minibatch iteration 574/73500: mean batch inertia: 87652.065978, ewa inertia: 88467.011693 \n",
      "Minibatch iteration 575/73500: mean batch inertia: 86117.260841, ewa inertia: 88460.609412 \n",
      "Minibatch iteration 576/73500: mean batch inertia: 87159.969099, ewa inertia: 88457.065604 \n",
      "Minibatch iteration 577/73500: mean batch inertia: 87138.678240, ewa inertia: 88453.473442 \n",
      "Minibatch iteration 578/73500: mean batch inertia: 88667.991591, ewa inertia: 88454.057932 \n",
      "Minibatch iteration 579/73500: mean batch inertia: 87457.504624, ewa inertia: 88451.342659 \n",
      "Minibatch iteration 580/73500: mean batch inertia: 85972.863909, ewa inertia: 88444.589637 \n",
      "Minibatch iteration 581/73500: mean batch inertia: 87783.016540, ewa inertia: 88442.787073 \n",
      "Minibatch iteration 582/73500: mean batch inertia: 86730.883136, ewa inertia: 88438.122710 \n",
      "Minibatch iteration 583/73500: mean batch inertia: 87001.832669, ewa inertia: 88434.209302 \n",
      "Minibatch iteration 584/73500: mean batch inertia: 88307.026719, ewa inertia: 88433.862772 \n",
      "Minibatch iteration 585/73500: mean batch inertia: 87131.265718, ewa inertia: 88430.313633 \n",
      "Minibatch iteration 586/73500: mean batch inertia: 87015.167598, ewa inertia: 88426.457836 \n",
      "Minibatch iteration 587/73500: mean batch inertia: 86818.681766, ewa inertia: 88422.077187 \n",
      "Minibatch iteration 588/73500: mean batch inertia: 88154.647794, ewa inertia: 88421.348531 \n",
      "Minibatch iteration 589/73500: mean batch inertia: 85077.864302, ewa inertia: 88412.238661 \n",
      "Minibatch iteration 590/73500: mean batch inertia: 86288.072153, ewa inertia: 88406.451021 \n",
      "Minibatch iteration 591/73500: mean batch inertia: 87351.233757, ewa inertia: 88403.575908 \n",
      "Minibatch iteration 592/73500: mean batch inertia: 87400.428276, ewa inertia: 88400.842668 \n",
      "Minibatch iteration 593/73500: mean batch inertia: 88246.899072, ewa inertia: 88400.423224 \n",
      "Minibatch iteration 594/73500: mean batch inertia: 88112.404195, ewa inertia: 88399.638469 \n",
      "Minibatch iteration 595/73500: mean batch inertia: 86952.317526, ewa inertia: 88395.695005 \n",
      "Minibatch iteration 596/73500: mean batch inertia: 88178.523677, ewa inertia: 88395.103287 \n",
      "Minibatch iteration 597/73500: mean batch inertia: 87845.205125, ewa inertia: 88393.604999 \n",
      "Minibatch iteration 598/73500: mean batch inertia: 87146.270947, ewa inertia: 88390.206433 \n",
      "Minibatch iteration 599/73500: mean batch inertia: 86437.322171, ewa inertia: 88384.885480 \n",
      "Minibatch iteration 600/73500: mean batch inertia: 88495.819256, ewa inertia: 88385.187737 \n",
      "Minibatch iteration 601/73500: mean batch inertia: 89593.755890, ewa inertia: 88388.480679 \n",
      "Minibatch iteration 602/73500: mean batch inertia: 87993.436021, ewa inertia: 88387.404315 \n",
      "Minibatch iteration 603/73500: mean batch inertia: 89125.167199, ewa inertia: 88389.414471 \n",
      "Minibatch iteration 604/73500: mean batch inertia: 86909.285549, ewa inertia: 88385.381617 \n",
      "Minibatch iteration 605/73500: mean batch inertia: 87198.431402, ewa inertia: 88382.147577 \n",
      "Minibatch iteration 606/73500: mean batch inertia: 87281.407932, ewa inertia: 88379.148431 \n",
      "Minibatch iteration 607/73500: mean batch inertia: 86053.883577, ewa inertia: 88372.812866 \n",
      "Minibatch iteration 608/73500: mean batch inertia: 85864.179773, ewa inertia: 88365.977684 \n",
      "Minibatch iteration 609/73500: mean batch inertia: 88873.798939, ewa inertia: 88367.361326 \n",
      "Minibatch iteration 610/73500: mean batch inertia: 86944.660357, ewa inertia: 88363.484944 \n",
      "Minibatch iteration 611/73500: mean batch inertia: 87646.400009, ewa inertia: 88361.531129 \n",
      "Minibatch iteration 612/73500: mean batch inertia: 87330.116965, ewa inertia: 88358.720872 \n",
      "Minibatch iteration 613/73500: mean batch inertia: 87751.985427, ewa inertia: 88357.067721 \n",
      "Minibatch iteration 614/73500: mean batch inertia: 87466.564635, ewa inertia: 88354.641400 \n",
      "Minibatch iteration 615/73500: mean batch inertia: 86760.765080, ewa inertia: 88350.298623 \n",
      "Minibatch iteration 616/73500: mean batch inertia: 87946.315452, ewa inertia: 88349.197904 \n",
      "Minibatch iteration 617/73500: mean batch inertia: 89058.436997, ewa inertia: 88351.130342 \n",
      "Minibatch iteration 618/73500: mean batch inertia: 89040.071816, ewa inertia: 88353.007476 \n",
      "Minibatch iteration 619/73500: mean batch inertia: 88979.120780, ewa inertia: 88354.713425 \n",
      "Minibatch iteration 620/73500: mean batch inertia: 86791.158157, ewa inertia: 88350.453262 \n",
      "Minibatch iteration 621/73500: mean batch inertia: 85947.952857, ewa inertia: 88343.907256 \n",
      "Minibatch iteration 622/73500: mean batch inertia: 86308.159682, ewa inertia: 88338.360528 \n",
      "Minibatch iteration 623/73500: mean batch inertia: 87220.363600, ewa inertia: 88335.314362 \n",
      "Minibatch iteration 624/73500: mean batch inertia: 87306.801290, ewa inertia: 88332.512010 \n",
      "Minibatch iteration 625/73500: mean batch inertia: 88064.752944, ewa inertia: 88331.782456 \n",
      "Minibatch iteration 626/73500: mean batch inertia: 88493.049616, ewa inertia: 88332.221855 \n",
      "Minibatch iteration 627/73500: mean batch inertia: 88903.592982, ewa inertia: 88333.778649 \n",
      "Minibatch iteration 628/73500: mean batch inertia: 87984.822841, ewa inertia: 88332.827862 \n",
      "Minibatch iteration 629/73500: mean batch inertia: 87443.415120, ewa inertia: 88330.404511 \n",
      "Minibatch iteration 630/73500: mean batch inertia: 85809.458244, ewa inertia: 88323.535780 \n",
      "Minibatch iteration 631/73500: mean batch inertia: 87744.233280, ewa inertia: 88321.957375 \n",
      "Minibatch iteration 632/73500: mean batch inertia: 87639.351662, ewa inertia: 88320.097504 \n",
      "Minibatch iteration 633/73500: mean batch inertia: 87638.536725, ewa inertia: 88318.240480 \n",
      "Minibatch iteration 634/73500: mean batch inertia: 88425.268446, ewa inertia: 88318.532095 \n",
      "Minibatch iteration 635/73500: mean batch inertia: 85605.842912, ewa inertia: 88311.140929 \n",
      "Minibatch iteration 636/73500: mean batch inertia: 87875.556788, ewa inertia: 88309.954109 \n",
      "Minibatch iteration 637/73500: mean batch inertia: 88359.848237, ewa inertia: 88310.090053 \n",
      "Minibatch iteration 638/73500: mean batch inertia: 86859.215667, ewa inertia: 88306.136908 \n",
      "Minibatch iteration 639/73500: mean batch inertia: 88513.625248, ewa inertia: 88306.702244 \n",
      "Minibatch iteration 640/73500: mean batch inertia: 88078.654261, ewa inertia: 88306.080890 \n",
      "Minibatch iteration 641/73500: mean batch inertia: 87816.563381, ewa inertia: 88304.747119 \n",
      "Minibatch iteration 642/73500: mean batch inertia: 86978.055491, ewa inertia: 88301.132331 \n",
      "Minibatch iteration 643/73500: mean batch inertia: 86859.354688, ewa inertia: 88297.203971 \n",
      "Minibatch iteration 644/73500: mean batch inertia: 87539.381179, ewa inertia: 88295.139159 \n",
      "Minibatch iteration 645/73500: mean batch inertia: 87807.707258, ewa inertia: 88293.811071 \n",
      "Minibatch iteration 646/73500: mean batch inertia: 87295.799089, ewa inertia: 88291.091823 \n",
      "Minibatch iteration 647/73500: mean batch inertia: 87194.640982, ewa inertia: 88288.104363 \n",
      "Minibatch iteration 648/73500: mean batch inertia: 87455.400421, ewa inertia: 88285.835525 \n",
      "Minibatch iteration 649/73500: mean batch inertia: 87798.179087, ewa inertia: 88284.506825 \n",
      "Minibatch iteration 650/73500: mean batch inertia: 87610.443466, ewa inertia: 88282.670229 \n",
      "Minibatch iteration 651/73500: mean batch inertia: 86308.099652, ewa inertia: 88277.290188 \n",
      "Minibatch iteration 652/73500: mean batch inertia: 87509.114000, ewa inertia: 88275.197166 \n",
      "Minibatch iteration 653/73500: mean batch inertia: 87849.302613, ewa inertia: 88274.036746 \n",
      "Minibatch iteration 654/73500: mean batch inertia: 85822.304987, ewa inertia: 88267.356601 \n",
      "Minibatch iteration 655/73500: mean batch inertia: 87220.098348, ewa inertia: 88264.503175 \n",
      "Minibatch iteration 656/73500: mean batch inertia: 87324.646354, ewa inertia: 88261.942381 \n",
      "Minibatch iteration 657/73500: mean batch inertia: 86523.244641, ewa inertia: 88257.205014 \n",
      "Minibatch iteration 658/73500: mean batch inertia: 87014.951041, ewa inertia: 88253.820289 \n",
      "Minibatch iteration 659/73500: mean batch inertia: 87104.856106, ewa inertia: 88250.689748 \n",
      "Minibatch iteration 660/73500: mean batch inertia: 87756.370668, ewa inertia: 88249.342894 \n",
      "Minibatch iteration 661/73500: mean batch inertia: 86802.358853, ewa inertia: 88245.400349 \n",
      "Minibatch iteration 662/73500: mean batch inertia: 87098.332594, ewa inertia: 88242.274975 \n",
      "Minibatch iteration 663/73500: mean batch inertia: 87612.222778, ewa inertia: 88240.558295 \n",
      "Minibatch iteration 664/73500: mean batch inertia: 85491.045788, ewa inertia: 88233.066797 \n",
      "Minibatch iteration 665/73500: mean batch inertia: 87045.175633, ewa inertia: 88229.830193 \n",
      "Minibatch iteration 666/73500: mean batch inertia: 88390.559095, ewa inertia: 88230.268125 \n",
      "Minibatch iteration 667/73500: mean batch inertia: 87967.543595, ewa inertia: 88229.552289 \n",
      "Minibatch iteration 668/73500: mean batch inertia: 86183.659940, ewa inertia: 88223.977920 \n",
      "Minibatch iteration 669/73500: mean batch inertia: 89534.653859, ewa inertia: 88227.549072 \n",
      "Minibatch iteration 670/73500: mean batch inertia: 88002.147691, ewa inertia: 88226.934929 \n",
      "Minibatch iteration 671/73500: mean batch inertia: 88974.772705, ewa inertia: 88228.972535 \n",
      "Minibatch iteration 672/73500: mean batch inertia: 88047.304380, ewa inertia: 88228.477551 \n",
      "Minibatch iteration 673/73500: mean batch inertia: 86760.387266, ewa inertia: 88224.477498 \n",
      "Minibatch iteration 674/73500: mean batch inertia: 88759.362523, ewa inertia: 88225.934880 \n",
      "Minibatch iteration 675/73500: mean batch inertia: 87682.501479, ewa inertia: 88224.454206 \n",
      "Minibatch iteration 676/73500: mean batch inertia: 85803.714801, ewa inertia: 88217.858505 \n",
      "Minibatch iteration 677/73500: mean batch inertia: 88042.103714, ewa inertia: 88217.379632 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch iteration 678/73500: mean batch inertia: 88389.394721, ewa inertia: 88217.848316 \n",
      "Minibatch iteration 679/73500: mean batch inertia: 88295.328395, ewa inertia: 88218.059423 \n",
      "Minibatch iteration 680/73500: mean batch inertia: 87319.529483, ewa inertia: 88215.611231 \n",
      "Minibatch iteration 681/73500: mean batch inertia: 86739.481033, ewa inertia: 88211.589272 \n",
      "Minibatch iteration 682/73500: mean batch inertia: 87353.737938, ewa inertia: 88209.251916 \n",
      "Minibatch iteration 683/73500: mean batch inertia: 86889.473173, ewa inertia: 88205.655962 \n",
      "Minibatch iteration 684/73500: mean batch inertia: 88421.675359, ewa inertia: 88206.244542 \n",
      "Minibatch iteration 685/73500: mean batch inertia: 86188.841542, ewa inertia: 88200.747797 \n",
      "Minibatch iteration 686/73500: mean batch inertia: 87816.105590, ewa inertia: 88199.699776 \n",
      "Minibatch iteration 687/73500: mean batch inertia: 87985.103281, ewa inertia: 88199.115073 \n",
      "Minibatch iteration 688/73500: mean batch inertia: 87971.364052, ewa inertia: 88198.494528 \n",
      "Minibatch iteration 689/73500: mean batch inertia: 89356.605289, ewa inertia: 88201.649991 \n",
      "Minibatch iteration 690/73500: mean batch inertia: 86079.172912, ewa inertia: 88195.866954 \n",
      "Minibatch iteration 691/73500: mean batch inertia: 87035.924984, ewa inertia: 88192.706502 \n",
      "Minibatch iteration 692/73500: mean batch inertia: 86754.657322, ewa inertia: 88188.788301 \n",
      "Minibatch iteration 693/73500: mean batch inertia: 87920.788485, ewa inertia: 88188.058092 \n",
      "Minibatch iteration 694/73500: mean batch inertia: 86916.835414, ewa inertia: 88184.594437 \n",
      "Minibatch iteration 695/73500: mean batch inertia: 87744.054894, ewa inertia: 88183.394115 \n",
      "Minibatch iteration 696/73500: mean batch inertia: 88627.062112, ewa inertia: 88184.602961 \n",
      "Minibatch iteration 697/73500: mean batch inertia: 86678.293475, ewa inertia: 88180.498774 \n",
      "Minibatch iteration 698/73500: mean batch inertia: 89549.283587, ewa inertia: 88184.228253 \n",
      "Minibatch iteration 699/73500: mean batch inertia: 86709.865044, ewa inertia: 88180.211109 \n",
      "Minibatch iteration 700/73500: mean batch inertia: 88297.241329, ewa inertia: 88180.529977 \n",
      "Minibatch iteration 701/73500: mean batch inertia: 87371.628039, ewa inertia: 88178.325991 \n",
      "Minibatch iteration 702/73500: mean batch inertia: 87403.770068, ewa inertia: 88176.215586 \n",
      "Minibatch iteration 703/73500: mean batch inertia: 87946.959708, ewa inertia: 88175.590941 \n",
      "Minibatch iteration 704/73500: mean batch inertia: 86579.719208, ewa inertia: 88171.242727 \n",
      "Minibatch iteration 705/73500: mean batch inertia: 87413.996589, ewa inertia: 88169.179486 \n",
      "Minibatch iteration 706/73500: mean batch inertia: 87158.910781, ewa inertia: 88166.426843 \n",
      "Minibatch iteration 707/73500: mean batch inertia: 87894.083872, ewa inertia: 88165.684800 \n",
      "Minibatch iteration 708/73500: mean batch inertia: 88246.950167, ewa inertia: 88165.906221 \n",
      "Minibatch iteration 709/73500: mean batch inertia: 88144.990036, ewa inertia: 88165.849231 \n",
      "Minibatch iteration 710/73500: mean batch inertia: 88101.258869, ewa inertia: 88165.673244 \n",
      "Minibatch iteration 711/73500: mean batch inertia: 89398.357152, ewa inertia: 88169.031893 \n",
      "Minibatch iteration 712/73500: mean batch inertia: 89422.863183, ewa inertia: 88172.448162 \n",
      "Minibatch iteration 713/73500: mean batch inertia: 89673.454031, ewa inertia: 88176.537899 \n",
      "Minibatch iteration 714/73500: mean batch inertia: 88285.335646, ewa inertia: 88176.834336 \n",
      "Minibatch iteration 715/73500: mean batch inertia: 86800.233085, ewa inertia: 88173.083560 \n",
      "Minibatch iteration 716/73500: mean batch inertia: 87232.983022, ewa inertia: 88170.522102 \n",
      "Minibatch iteration 717/73500: mean batch inertia: 87708.770929, ewa inertia: 88169.263986 \n",
      "Minibatch iteration 718/73500: mean batch inertia: 89143.261331, ewa inertia: 88171.917801 \n",
      "Minibatch iteration 719/73500: mean batch inertia: 87836.326971, ewa inertia: 88171.003429 \n",
      "Minibatch iteration 720/73500: mean batch inertia: 86640.771354, ewa inertia: 88166.834061 \n",
      "Converged (lack of improvement in inertia) at iteration 720/73500\n",
      "Computing label assignment and total inertia\n"
     ]
    }
   ],
   "source": [
    "kmeans = clustering(descriptors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CREATING BAG-OF-WORDS\n",
    "def histo_create(images, kmeans):\n",
    "    kmeans.verbose = False\n",
    "    histo_list = []\n",
    "    for image in images:\n",
    "        image = cv2.resize(image, (image.shape[0]//4, image.shape[1]//4), interpolation = cv2.INTER_AREA)\n",
    "        kp, des = brief.detectAndCompute(image, None)\n",
    "        histo = np.zeros(40)\n",
    "        nkp = np.size(kp)\n",
    "\n",
    "        for d in des:\n",
    "            idx = kmeans.predict([d])\n",
    "            histo[idx] += 1/nkp # Because we need normalized histograms, I prefer to add 1/nkp directly\n",
    "\n",
    "        histo_list.append(histo)\n",
    "    return histo_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "histo_serg = histo_create(serg_images, kmeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "histo_roma = histo_create(roma_images, kmeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "histo_nick = histo_create(nick_images, kmeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "histo_noobject = histo_create(noobject_images, kmeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CREATING LABELS\n",
    "serg_labels = [[1, 0, 0]]\n",
    "nick_labels = [[0, 1, 0]]\n",
    "roma_labels = [[0, 0, 1]]\n",
    "noobject_labels = [[0, 0, 0]]\n",
    "\n",
    "serg_labels = serg_labels*len(histo_serg)\n",
    "nick_labels = nick_labels*len(histo_nick)\n",
    "roma_labels = roma_labels*len(histo_roma)\n",
    "noobject_labels = noobject_labels*len(histo_noobject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = histo_serg + histo_nick + histo_roma + histo_noobject\n",
    "train_labels = serg_labels + nick_labels + roma_labels + noobject_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "un = list(zip(train_data, train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(un)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_labels = zip(*un)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.01832786\n",
      "Iteration 2, loss = 2.00061292\n",
      "Iteration 3, loss = 1.98320711\n",
      "Iteration 4, loss = 1.96605679\n",
      "Iteration 5, loss = 1.94984750\n",
      "Iteration 6, loss = 1.93362709\n",
      "Iteration 7, loss = 1.91821543\n",
      "Iteration 8, loss = 1.90310585\n",
      "Iteration 9, loss = 1.88855324\n",
      "Iteration 10, loss = 1.87405260\n",
      "Iteration 11, loss = 1.86061348\n",
      "Iteration 12, loss = 1.84715649\n",
      "Iteration 13, loss = 1.83422620\n",
      "Iteration 14, loss = 1.82154086\n",
      "Iteration 15, loss = 1.80918502\n",
      "Iteration 16, loss = 1.79712953\n",
      "Iteration 17, loss = 1.78572661\n",
      "Iteration 18, loss = 1.77430870\n",
      "Iteration 19, loss = 1.76373210\n",
      "Iteration 20, loss = 1.75318916\n",
      "Iteration 21, loss = 1.74351894\n",
      "Iteration 22, loss = 1.73402736\n",
      "Iteration 23, loss = 1.72502120\n",
      "Iteration 24, loss = 1.71647180\n",
      "Iteration 25, loss = 1.70856963\n",
      "Iteration 26, loss = 1.70102951\n",
      "Iteration 27, loss = 1.69372022\n",
      "Iteration 28, loss = 1.68705609\n",
      "Iteration 29, loss = 1.68078140\n",
      "Iteration 30, loss = 1.67511677\n",
      "Iteration 31, loss = 1.66929149\n",
      "Iteration 32, loss = 1.66424332\n",
      "Iteration 33, loss = 1.65954684\n",
      "Iteration 34, loss = 1.65510636\n",
      "Iteration 35, loss = 1.65117744\n",
      "Iteration 36, loss = 1.64763129\n",
      "Iteration 37, loss = 1.64440054\n",
      "Iteration 38, loss = 1.64159419\n",
      "Iteration 39, loss = 1.63925458\n",
      "Iteration 40, loss = 1.63717024\n",
      "Iteration 41, loss = 1.63508344\n",
      "Iteration 42, loss = 1.63335181\n",
      "Iteration 43, loss = 1.63187509\n",
      "Iteration 44, loss = 1.63027285\n",
      "Iteration 45, loss = 1.62890741\n",
      "Iteration 46, loss = 1.62749818\n",
      "Iteration 47, loss = 1.62612750\n",
      "Iteration 48, loss = 1.62485709\n",
      "Iteration 49, loss = 1.62346228\n",
      "Iteration 50, loss = 1.62213397\n",
      "Iteration 51, loss = 1.62072268\n",
      "Iteration 52, loss = 1.61935135\n",
      "Iteration 53, loss = 1.61793344\n",
      "Iteration 54, loss = 1.61642100\n",
      "Iteration 55, loss = 1.61516097\n",
      "Iteration 56, loss = 1.61360399\n",
      "Iteration 57, loss = 1.61215164\n",
      "Iteration 58, loss = 1.61076386\n",
      "Iteration 59, loss = 1.60951785\n",
      "Iteration 60, loss = 1.60803858\n",
      "Iteration 61, loss = 1.60670226\n",
      "Iteration 62, loss = 1.60508654\n",
      "Iteration 63, loss = 1.60356420\n",
      "Iteration 64, loss = 1.60204431\n",
      "Iteration 65, loss = 1.60041049\n",
      "Iteration 66, loss = 1.59890008\n",
      "Iteration 67, loss = 1.59734375\n",
      "Iteration 68, loss = 1.59571765\n",
      "Iteration 69, loss = 1.59415849\n",
      "Iteration 70, loss = 1.59254915\n",
      "Iteration 71, loss = 1.59080390\n",
      "Iteration 72, loss = 1.58931520\n",
      "Iteration 73, loss = 1.58760138\n",
      "Iteration 74, loss = 1.58593915\n",
      "Iteration 75, loss = 1.58433051\n",
      "Iteration 76, loss = 1.58264288\n",
      "Iteration 77, loss = 1.58094512\n",
      "Iteration 78, loss = 1.57922543\n",
      "Iteration 79, loss = 1.57755262\n",
      "Iteration 80, loss = 1.57583441\n",
      "Iteration 81, loss = 1.57399379\n",
      "Iteration 82, loss = 1.57221318\n",
      "Iteration 83, loss = 1.57041563\n",
      "Iteration 84, loss = 1.56868874\n",
      "Iteration 85, loss = 1.56686406\n",
      "Iteration 86, loss = 1.56507045\n",
      "Iteration 87, loss = 1.56332632\n",
      "Iteration 88, loss = 1.56141882\n",
      "Iteration 89, loss = 1.55946958\n",
      "Iteration 90, loss = 1.55747760\n",
      "Iteration 91, loss = 1.55546807\n",
      "Iteration 92, loss = 1.55347558\n",
      "Iteration 93, loss = 1.55153826\n",
      "Iteration 94, loss = 1.54943970\n",
      "Iteration 95, loss = 1.54731992\n",
      "Iteration 96, loss = 1.54531373\n",
      "Iteration 97, loss = 1.54321315\n",
      "Iteration 98, loss = 1.54122678\n",
      "Iteration 99, loss = 1.53909100\n",
      "Iteration 100, loss = 1.53707899\n",
      "Iteration 101, loss = 1.53488884\n",
      "Iteration 102, loss = 1.53271588\n",
      "Iteration 103, loss = 1.53066021\n",
      "Iteration 104, loss = 1.52838339\n",
      "Iteration 105, loss = 1.52611322\n",
      "Iteration 106, loss = 1.52397006\n",
      "Iteration 107, loss = 1.52177856\n",
      "Iteration 108, loss = 1.51937683\n",
      "Iteration 109, loss = 1.51724216\n",
      "Iteration 110, loss = 1.51497569\n",
      "Iteration 111, loss = 1.51264470\n",
      "Iteration 112, loss = 1.51035841\n",
      "Iteration 113, loss = 1.50803974\n",
      "Iteration 114, loss = 1.50558695\n",
      "Iteration 115, loss = 1.50331208\n",
      "Iteration 116, loss = 1.50092716\n",
      "Iteration 117, loss = 1.49858992\n",
      "Iteration 118, loss = 1.49621976\n",
      "Iteration 119, loss = 1.49392977\n",
      "Iteration 120, loss = 1.49132288\n",
      "Iteration 121, loss = 1.48887890\n",
      "Iteration 122, loss = 1.48647581\n",
      "Iteration 123, loss = 1.48400953\n",
      "Iteration 124, loss = 1.48146304\n",
      "Iteration 125, loss = 1.47897097\n",
      "Iteration 126, loss = 1.47669724\n",
      "Iteration 127, loss = 1.47398191\n",
      "Iteration 128, loss = 1.47132473\n",
      "Iteration 129, loss = 1.46889947\n",
      "Iteration 130, loss = 1.46626913\n",
      "Iteration 131, loss = 1.46367257\n",
      "Iteration 132, loss = 1.46101294\n",
      "Iteration 133, loss = 1.45836250\n",
      "Iteration 134, loss = 1.45573532\n",
      "Iteration 135, loss = 1.45304701\n",
      "Iteration 136, loss = 1.45047122\n",
      "Iteration 137, loss = 1.44762685\n",
      "Iteration 138, loss = 1.44510543\n",
      "Iteration 139, loss = 1.44241879\n",
      "Iteration 140, loss = 1.43973410\n",
      "Iteration 141, loss = 1.43701888\n",
      "Iteration 142, loss = 1.43429057\n",
      "Iteration 143, loss = 1.43148157\n",
      "Iteration 144, loss = 1.42884270\n",
      "Iteration 145, loss = 1.42618324\n",
      "Iteration 146, loss = 1.42345458\n",
      "Iteration 147, loss = 1.42085067\n",
      "Iteration 148, loss = 1.41832154\n",
      "Iteration 149, loss = 1.41561341\n",
      "Iteration 150, loss = 1.41280770\n",
      "Iteration 151, loss = 1.41012671\n",
      "Iteration 152, loss = 1.40715885\n",
      "Iteration 153, loss = 1.40437938\n",
      "Iteration 154, loss = 1.40184643\n",
      "Iteration 155, loss = 1.39900710\n",
      "Iteration 156, loss = 1.39633997\n",
      "Iteration 157, loss = 1.39354345\n",
      "Iteration 158, loss = 1.39101482\n",
      "Iteration 159, loss = 1.38819008\n",
      "Iteration 160, loss = 1.38539057\n",
      "Iteration 161, loss = 1.38267302\n",
      "Iteration 162, loss = 1.38007281\n",
      "Iteration 163, loss = 1.37728975\n",
      "Iteration 164, loss = 1.37455926\n",
      "Iteration 165, loss = 1.37187016\n",
      "Iteration 166, loss = 1.36925907\n",
      "Iteration 167, loss = 1.36651179\n",
      "Iteration 168, loss = 1.36409632\n",
      "Iteration 169, loss = 1.36120433\n",
      "Iteration 170, loss = 1.35851032\n",
      "Iteration 171, loss = 1.35545065\n",
      "Iteration 172, loss = 1.35273273\n",
      "Iteration 173, loss = 1.35015354\n",
      "Iteration 174, loss = 1.34768425\n",
      "Iteration 175, loss = 1.34509130\n",
      "Iteration 176, loss = 1.34238299\n",
      "Iteration 177, loss = 1.33984864\n",
      "Iteration 178, loss = 1.33676522\n",
      "Iteration 179, loss = 1.33397703\n",
      "Iteration 180, loss = 1.33134932\n",
      "Iteration 181, loss = 1.32856724\n",
      "Iteration 182, loss = 1.32605206\n",
      "Iteration 183, loss = 1.32337502\n",
      "Iteration 184, loss = 1.32097218\n",
      "Iteration 185, loss = 1.31839828\n",
      "Iteration 186, loss = 1.31577402\n",
      "Iteration 187, loss = 1.31319952\n",
      "Iteration 188, loss = 1.31047561\n",
      "Iteration 189, loss = 1.30792721\n",
      "Iteration 190, loss = 1.30530347\n",
      "Iteration 191, loss = 1.30271543\n",
      "Iteration 192, loss = 1.30048174\n",
      "Iteration 193, loss = 1.29782312\n",
      "Iteration 194, loss = 1.29533183\n",
      "Iteration 195, loss = 1.29280117\n",
      "Iteration 196, loss = 1.29026100\n",
      "Iteration 197, loss = 1.28794363\n",
      "Iteration 198, loss = 1.28525864\n",
      "Iteration 199, loss = 1.28284851\n",
      "Iteration 200, loss = 1.28075779\n",
      "Iteration 201, loss = 1.27846427\n",
      "Iteration 202, loss = 1.27612041\n",
      "Iteration 203, loss = 1.27343503\n",
      "Iteration 204, loss = 1.27075802\n",
      "Iteration 205, loss = 1.26851508\n",
      "Iteration 206, loss = 1.26599764\n",
      "Iteration 207, loss = 1.26376724\n",
      "Iteration 208, loss = 1.26144052\n",
      "Iteration 209, loss = 1.25929341\n",
      "Iteration 210, loss = 1.25696455\n",
      "Iteration 211, loss = 1.25502975\n",
      "Iteration 212, loss = 1.25253494\n",
      "Iteration 213, loss = 1.25003137\n",
      "Iteration 214, loss = 1.24751655\n",
      "Iteration 215, loss = 1.24539100\n",
      "Iteration 216, loss = 1.24325964\n",
      "Iteration 217, loss = 1.24095828\n",
      "Iteration 218, loss = 1.23886616\n",
      "Iteration 219, loss = 1.23673122\n",
      "Iteration 220, loss = 1.23450934\n",
      "Iteration 221, loss = 1.23222762\n",
      "Iteration 222, loss = 1.22999526\n",
      "Iteration 223, loss = 1.22785238\n",
      "Iteration 224, loss = 1.22553446\n",
      "Iteration 225, loss = 1.22334133\n",
      "Iteration 226, loss = 1.22129834\n",
      "Iteration 227, loss = 1.21915831\n",
      "Iteration 228, loss = 1.21712304\n",
      "Iteration 229, loss = 1.21493190\n",
      "Iteration 230, loss = 1.21291373\n",
      "Iteration 231, loss = 1.21080662\n",
      "Iteration 232, loss = 1.20859072\n",
      "Iteration 233, loss = 1.20673050\n",
      "Iteration 234, loss = 1.20454596\n",
      "Iteration 235, loss = 1.20255692\n",
      "Iteration 236, loss = 1.20090795\n",
      "Iteration 237, loss = 1.19862777\n",
      "Iteration 238, loss = 1.19658874\n",
      "Iteration 239, loss = 1.19457254\n",
      "Iteration 240, loss = 1.19260438\n",
      "Iteration 241, loss = 1.19051691\n",
      "Iteration 242, loss = 1.18846314\n",
      "Iteration 243, loss = 1.18656922\n",
      "Iteration 244, loss = 1.18446041\n",
      "Iteration 245, loss = 1.18257700\n",
      "Iteration 246, loss = 1.18091173\n",
      "Iteration 247, loss = 1.17904306\n",
      "Iteration 248, loss = 1.17733308\n",
      "Iteration 249, loss = 1.17562658\n",
      "Iteration 250, loss = 1.17386054\n",
      "Iteration 251, loss = 1.17209324\n",
      "Iteration 252, loss = 1.17003681\n",
      "Iteration 253, loss = 1.16835331\n",
      "Iteration 254, loss = 1.16625852\n",
      "Iteration 255, loss = 1.16443668\n",
      "Iteration 256, loss = 1.16276411\n",
      "Iteration 257, loss = 1.16096946\n",
      "Iteration 258, loss = 1.15916623\n",
      "Iteration 259, loss = 1.15745765\n",
      "Iteration 260, loss = 1.15571069\n",
      "Iteration 261, loss = 1.15392617\n",
      "Iteration 262, loss = 1.15213070\n",
      "Iteration 263, loss = 1.15049929\n",
      "Iteration 264, loss = 1.14894240\n",
      "Iteration 265, loss = 1.14709272\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 266, loss = 1.14531853\n",
      "Iteration 267, loss = 1.14359368\n",
      "Iteration 268, loss = 1.14205185\n",
      "Iteration 269, loss = 1.14035510\n",
      "Iteration 270, loss = 1.13866512\n",
      "Iteration 271, loss = 1.13704853\n",
      "Iteration 272, loss = 1.13539254\n",
      "Iteration 273, loss = 1.13400768\n",
      "Iteration 274, loss = 1.13222001\n",
      "Iteration 275, loss = 1.13057673\n",
      "Iteration 276, loss = 1.12888066\n",
      "Iteration 277, loss = 1.12734669\n",
      "Iteration 278, loss = 1.12574903\n",
      "Iteration 279, loss = 1.12426700\n",
      "Iteration 280, loss = 1.12315440\n",
      "Iteration 281, loss = 1.12223094\n",
      "Iteration 282, loss = 1.12082390\n",
      "Iteration 283, loss = 1.11921767\n",
      "Iteration 284, loss = 1.11739556\n",
      "Iteration 285, loss = 1.11570562\n",
      "Iteration 286, loss = 1.11396004\n",
      "Iteration 287, loss = 1.11277662\n",
      "Iteration 288, loss = 1.11148481\n",
      "Iteration 289, loss = 1.11010439\n",
      "Iteration 290, loss = 1.10859325\n",
      "Iteration 291, loss = 1.10685267\n",
      "Iteration 292, loss = 1.10511428\n",
      "Iteration 293, loss = 1.10330664\n",
      "Iteration 294, loss = 1.10178300\n",
      "Iteration 295, loss = 1.10069590\n",
      "Iteration 296, loss = 1.09939306\n",
      "Iteration 297, loss = 1.09832000\n",
      "Iteration 298, loss = 1.09705465\n",
      "Iteration 299, loss = 1.09562249\n",
      "Iteration 300, loss = 1.09395945\n",
      "Iteration 301, loss = 1.09234715\n",
      "Iteration 302, loss = 1.09078140\n",
      "Iteration 303, loss = 1.08923954\n",
      "Iteration 304, loss = 1.08811513\n",
      "Iteration 305, loss = 1.08691245\n",
      "Iteration 306, loss = 1.08533991\n",
      "Iteration 307, loss = 1.08398420\n",
      "Iteration 308, loss = 1.08246235\n",
      "Iteration 309, loss = 1.08119599\n",
      "Iteration 310, loss = 1.07963407\n",
      "Iteration 311, loss = 1.07862870\n",
      "Iteration 312, loss = 1.07730433\n",
      "Iteration 313, loss = 1.07609937\n",
      "Iteration 314, loss = 1.07473388\n",
      "Iteration 315, loss = 1.07353426\n",
      "Iteration 316, loss = 1.07203154\n",
      "Iteration 317, loss = 1.07076757\n",
      "Iteration 318, loss = 1.06944376\n",
      "Iteration 319, loss = 1.06825184\n",
      "Iteration 320, loss = 1.06685842\n",
      "Iteration 321, loss = 1.06558811\n",
      "Iteration 322, loss = 1.06436681\n",
      "Iteration 323, loss = 1.06311815\n",
      "Iteration 324, loss = 1.06190816\n",
      "Iteration 325, loss = 1.06092233\n",
      "Iteration 326, loss = 1.05959636\n",
      "Iteration 327, loss = 1.05838507\n",
      "Iteration 328, loss = 1.05714897\n",
      "Iteration 329, loss = 1.05585974\n",
      "Iteration 330, loss = 1.05486687\n",
      "Iteration 331, loss = 1.05361189\n",
      "Iteration 332, loss = 1.05241354\n",
      "Iteration 333, loss = 1.05127619\n",
      "Iteration 334, loss = 1.05009227\n",
      "Iteration 335, loss = 1.04890692\n",
      "Iteration 336, loss = 1.04787986\n",
      "Iteration 337, loss = 1.04663348\n",
      "Iteration 338, loss = 1.04558176\n",
      "Iteration 339, loss = 1.04421144\n",
      "Iteration 340, loss = 1.04341841\n",
      "Iteration 341, loss = 1.04220902\n",
      "Iteration 342, loss = 1.04105878\n",
      "Iteration 343, loss = 1.03986518\n",
      "Iteration 344, loss = 1.03888534\n",
      "Iteration 345, loss = 1.03761427\n",
      "Iteration 346, loss = 1.03642428\n",
      "Iteration 347, loss = 1.03525046\n",
      "Iteration 348, loss = 1.03433733\n",
      "Iteration 349, loss = 1.03341506\n",
      "Iteration 350, loss = 1.03251641\n",
      "Iteration 351, loss = 1.03152638\n",
      "Iteration 352, loss = 1.03033552\n",
      "Iteration 353, loss = 1.02911430\n",
      "Iteration 354, loss = 1.02794950\n",
      "Iteration 355, loss = 1.02684030\n",
      "Iteration 356, loss = 1.02583365\n",
      "Iteration 357, loss = 1.02476292\n",
      "Iteration 358, loss = 1.02381240\n",
      "Iteration 359, loss = 1.02289596\n",
      "Iteration 360, loss = 1.02196278\n",
      "Iteration 361, loss = 1.02084463\n",
      "Iteration 362, loss = 1.01992317\n",
      "Iteration 363, loss = 1.01887054\n",
      "Iteration 364, loss = 1.01785963\n",
      "Iteration 365, loss = 1.01666945\n",
      "Iteration 366, loss = 1.01548132\n",
      "Iteration 367, loss = 1.01435553\n",
      "Iteration 368, loss = 1.01325357\n",
      "Iteration 369, loss = 1.01205832\n",
      "Iteration 370, loss = 1.01121057\n",
      "Iteration 371, loss = 1.01014712\n",
      "Iteration 372, loss = 1.00915803\n",
      "Iteration 373, loss = 1.00829660\n",
      "Iteration 374, loss = 1.00723579\n",
      "Iteration 375, loss = 1.00635152\n",
      "Iteration 376, loss = 1.00529995\n",
      "Iteration 377, loss = 1.00429491\n",
      "Iteration 378, loss = 1.00323883\n",
      "Iteration 379, loss = 1.00226424\n",
      "Iteration 380, loss = 1.00149797\n",
      "Iteration 381, loss = 1.00030319\n",
      "Iteration 382, loss = 0.99938209\n",
      "Iteration 383, loss = 0.99835469\n",
      "Iteration 384, loss = 0.99754028\n",
      "Iteration 385, loss = 0.99670350\n",
      "Iteration 386, loss = 0.99578747\n",
      "Iteration 387, loss = 0.99500168\n",
      "Iteration 388, loss = 0.99382473\n",
      "Iteration 389, loss = 0.99283998\n",
      "Iteration 390, loss = 0.99186729\n",
      "Iteration 391, loss = 0.99062509\n",
      "Iteration 392, loss = 0.99008129\n",
      "Iteration 393, loss = 0.98932439\n",
      "Iteration 394, loss = 0.98833945\n",
      "Iteration 395, loss = 0.98729364\n",
      "Iteration 396, loss = 0.98626597\n",
      "Iteration 397, loss = 0.98548998\n",
      "Iteration 398, loss = 0.98434907\n",
      "Iteration 399, loss = 0.98351001\n",
      "Iteration 400, loss = 0.98280186\n",
      "Iteration 401, loss = 0.98191022\n",
      "Iteration 402, loss = 0.98070558\n",
      "Iteration 403, loss = 0.97983322\n",
      "Iteration 404, loss = 0.97871541\n",
      "Iteration 405, loss = 0.97798284\n",
      "Iteration 406, loss = 0.97736280\n",
      "Iteration 407, loss = 0.97666677\n",
      "Iteration 408, loss = 0.97575108\n",
      "Iteration 409, loss = 0.97482322\n",
      "Iteration 410, loss = 0.97388067\n",
      "Iteration 411, loss = 0.97331643\n",
      "Iteration 412, loss = 0.97221132\n",
      "Iteration 413, loss = 0.97131770\n",
      "Iteration 414, loss = 0.97049325\n",
      "Iteration 415, loss = 0.96964402\n",
      "Iteration 416, loss = 0.96889777\n",
      "Iteration 417, loss = 0.96782612\n",
      "Iteration 418, loss = 0.96703274\n",
      "Iteration 419, loss = 0.96625542\n",
      "Iteration 420, loss = 0.96569749\n",
      "Iteration 421, loss = 0.96547037\n",
      "Iteration 422, loss = 0.96488046\n",
      "Iteration 423, loss = 0.96419341\n",
      "Iteration 424, loss = 0.96306484\n",
      "Iteration 425, loss = 0.96186853\n",
      "Iteration 426, loss = 0.96079790\n",
      "Iteration 427, loss = 0.95953529\n",
      "Iteration 428, loss = 0.95899953\n",
      "Iteration 429, loss = 0.95794022\n",
      "Iteration 430, loss = 0.95730548\n",
      "Iteration 431, loss = 0.95652206\n",
      "Iteration 432, loss = 0.95566220\n",
      "Iteration 433, loss = 0.95494604\n",
      "Iteration 434, loss = 0.95403889\n",
      "Iteration 435, loss = 0.95302280\n",
      "Iteration 436, loss = 0.95202257\n",
      "Iteration 437, loss = 0.95141857\n",
      "Iteration 438, loss = 0.95035461\n",
      "Iteration 439, loss = 0.94967071\n",
      "Iteration 440, loss = 0.94880890\n",
      "Iteration 441, loss = 0.94795789\n",
      "Iteration 442, loss = 0.94727510\n",
      "Iteration 443, loss = 0.94630621\n",
      "Iteration 444, loss = 0.94555955\n",
      "Iteration 445, loss = 0.94475691\n",
      "Iteration 446, loss = 0.94398025\n",
      "Iteration 447, loss = 0.94306803\n",
      "Iteration 448, loss = 0.94240462\n",
      "Iteration 449, loss = 0.94131995\n",
      "Iteration 450, loss = 0.94049119\n",
      "Iteration 451, loss = 0.93979248\n",
      "Iteration 452, loss = 0.93899036\n",
      "Iteration 453, loss = 0.93821926\n",
      "Iteration 454, loss = 0.93740252\n",
      "Iteration 455, loss = 0.93651302\n",
      "Iteration 456, loss = 0.93578805\n",
      "Iteration 457, loss = 0.93470682\n",
      "Iteration 458, loss = 0.93400218\n",
      "Iteration 459, loss = 0.93329543\n",
      "Iteration 460, loss = 0.93256054\n",
      "Iteration 461, loss = 0.93165989\n",
      "Iteration 462, loss = 0.93088610\n",
      "Iteration 463, loss = 0.92987738\n",
      "Iteration 464, loss = 0.92899976\n",
      "Iteration 465, loss = 0.92803179\n",
      "Iteration 466, loss = 0.92746348\n",
      "Iteration 467, loss = 0.92655282\n",
      "Iteration 468, loss = 0.92552329\n",
      "Iteration 469, loss = 0.92505175\n",
      "Iteration 470, loss = 0.92406178\n",
      "Iteration 471, loss = 0.92326996\n",
      "Iteration 472, loss = 0.92243981\n",
      "Iteration 473, loss = 0.92156397\n",
      "Iteration 474, loss = 0.92079258\n",
      "Iteration 475, loss = 0.91985721\n",
      "Iteration 476, loss = 0.91911587\n",
      "Iteration 477, loss = 0.91836083\n",
      "Iteration 478, loss = 0.91781408\n",
      "Iteration 479, loss = 0.91718296\n",
      "Iteration 480, loss = 0.91663454\n",
      "Iteration 481, loss = 0.91585859\n",
      "Iteration 482, loss = 0.91502594\n",
      "Iteration 483, loss = 0.91411866\n",
      "Iteration 484, loss = 0.91323920\n",
      "Iteration 485, loss = 0.91221332\n",
      "Iteration 486, loss = 0.91172620\n",
      "Iteration 487, loss = 0.91081406\n",
      "Iteration 488, loss = 0.90997843\n",
      "Iteration 489, loss = 0.90947616\n",
      "Iteration 490, loss = 0.90877275\n",
      "Iteration 491, loss = 0.90811695\n",
      "Iteration 492, loss = 0.90728459\n",
      "Iteration 493, loss = 0.90644006\n",
      "Iteration 494, loss = 0.90565386\n",
      "Iteration 495, loss = 0.90484190\n",
      "Iteration 496, loss = 0.90399245\n",
      "Iteration 497, loss = 0.90319326\n",
      "Iteration 498, loss = 0.90240454\n",
      "Iteration 499, loss = 0.90190544\n",
      "Iteration 500, loss = 0.90082404\n",
      "Iteration 501, loss = 0.89998784\n",
      "Iteration 502, loss = 0.89916386\n",
      "Iteration 503, loss = 0.89831710\n",
      "Iteration 504, loss = 0.89767411\n",
      "Iteration 505, loss = 0.89713989\n",
      "Iteration 506, loss = 0.89638104\n",
      "Iteration 507, loss = 0.89594992\n",
      "Iteration 508, loss = 0.89529448\n",
      "Iteration 509, loss = 0.89439782\n",
      "Iteration 510, loss = 0.89351863\n",
      "Iteration 511, loss = 0.89234285\n",
      "Iteration 512, loss = 0.89163988\n",
      "Iteration 513, loss = 0.89075653\n",
      "Iteration 514, loss = 0.89013479\n",
      "Iteration 515, loss = 0.88922672\n",
      "Iteration 516, loss = 0.88845792\n",
      "Iteration 517, loss = 0.88825320\n",
      "Iteration 518, loss = 0.88730949\n",
      "Iteration 519, loss = 0.88674835\n",
      "Iteration 520, loss = 0.88646720\n",
      "Iteration 521, loss = 0.88590994\n",
      "Iteration 522, loss = 0.88457238\n",
      "Iteration 523, loss = 0.88392252\n",
      "Iteration 524, loss = 0.88326179\n",
      "Iteration 525, loss = 0.88264284\n",
      "Iteration 526, loss = 0.88224871\n",
      "Iteration 527, loss = 0.88162991\n",
      "Iteration 528, loss = 0.88085637\n",
      "Iteration 529, loss = 0.87990641\n",
      "Iteration 530, loss = 0.87918202\n",
      "Iteration 531, loss = 0.87829804\n",
      "Iteration 532, loss = 0.87746104\n",
      "Iteration 533, loss = 0.87685224\n",
      "Iteration 534, loss = 0.87593585\n",
      "Iteration 535, loss = 0.87524124\n",
      "Iteration 536, loss = 0.87451543\n",
      "Iteration 537, loss = 0.87368658\n",
      "Iteration 538, loss = 0.87332277\n",
      "Iteration 539, loss = 0.87295759\n",
      "Iteration 540, loss = 0.87248361\n",
      "Iteration 541, loss = 0.87173128\n",
      "Iteration 542, loss = 0.87070776\n",
      "Iteration 543, loss = 0.86976616\n",
      "Iteration 544, loss = 0.86870698\n",
      "Iteration 545, loss = 0.86808959\n",
      "Iteration 546, loss = 0.86748146\n",
      "Iteration 547, loss = 0.86684101\n",
      "Iteration 548, loss = 0.86604215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 549, loss = 0.86569001\n",
      "Iteration 550, loss = 0.86489417\n",
      "Iteration 551, loss = 0.86417705\n",
      "Iteration 552, loss = 0.86372499\n",
      "Iteration 553, loss = 0.86279968\n",
      "Iteration 554, loss = 0.86216280\n",
      "Iteration 555, loss = 0.86147728\n",
      "Iteration 556, loss = 0.86086838\n",
      "Iteration 557, loss = 0.86023074\n",
      "Iteration 558, loss = 0.85920150\n",
      "Iteration 559, loss = 0.85848305\n",
      "Iteration 560, loss = 0.85804273\n",
      "Iteration 561, loss = 0.85697999\n",
      "Iteration 562, loss = 0.85639703\n",
      "Iteration 563, loss = 0.85610664\n",
      "Iteration 564, loss = 0.85530857\n",
      "Iteration 565, loss = 0.85460811\n",
      "Iteration 566, loss = 0.85412170\n",
      "Iteration 567, loss = 0.85341863\n",
      "Iteration 568, loss = 0.85259655\n",
      "Iteration 569, loss = 0.85184392\n",
      "Iteration 570, loss = 0.85126176\n",
      "Iteration 571, loss = 0.85058664\n",
      "Iteration 572, loss = 0.84987582\n",
      "Iteration 573, loss = 0.84919429\n",
      "Iteration 574, loss = 0.84845567\n",
      "Iteration 575, loss = 0.84767840\n",
      "Iteration 576, loss = 0.84703011\n",
      "Iteration 577, loss = 0.84637264\n",
      "Iteration 578, loss = 0.84592958\n",
      "Iteration 579, loss = 0.84520470\n",
      "Iteration 580, loss = 0.84466166\n",
      "Iteration 581, loss = 0.84411124\n",
      "Iteration 582, loss = 0.84331192\n",
      "Iteration 583, loss = 0.84269800\n",
      "Iteration 584, loss = 0.84216861\n",
      "Iteration 585, loss = 0.84152785\n",
      "Iteration 586, loss = 0.84089062\n",
      "Iteration 587, loss = 0.84017241\n",
      "Iteration 588, loss = 0.84045582\n",
      "Iteration 589, loss = 0.83948243\n",
      "Iteration 590, loss = 0.83886173\n",
      "Iteration 591, loss = 0.83834918\n",
      "Iteration 592, loss = 0.83733620\n",
      "Iteration 593, loss = 0.83647688\n",
      "Iteration 594, loss = 0.83567199\n",
      "Iteration 595, loss = 0.83499093\n",
      "Iteration 596, loss = 0.83428552\n",
      "Iteration 597, loss = 0.83367320\n",
      "Iteration 598, loss = 0.83301534\n",
      "Iteration 599, loss = 0.83234022\n",
      "Iteration 600, loss = 0.83160899\n",
      "Iteration 601, loss = 0.83094880\n",
      "Iteration 602, loss = 0.83032824\n",
      "Iteration 603, loss = 0.82968603\n",
      "Iteration 604, loss = 0.82909787\n",
      "Iteration 605, loss = 0.82873134\n",
      "Iteration 606, loss = 0.82800013\n",
      "Iteration 607, loss = 0.82720400\n",
      "Iteration 608, loss = 0.82638216\n",
      "Iteration 609, loss = 0.82583065\n",
      "Iteration 610, loss = 0.82549318\n",
      "Iteration 611, loss = 0.82468155\n",
      "Iteration 612, loss = 0.82410387\n",
      "Iteration 613, loss = 0.82349501\n",
      "Iteration 614, loss = 0.82271573\n",
      "Iteration 615, loss = 0.82210165\n",
      "Iteration 616, loss = 0.82168118\n",
      "Iteration 617, loss = 0.82063605\n",
      "Iteration 618, loss = 0.82018422\n",
      "Iteration 619, loss = 0.81957277\n",
      "Iteration 620, loss = 0.81899183\n",
      "Iteration 621, loss = 0.81888560\n",
      "Iteration 622, loss = 0.81813766\n",
      "Iteration 623, loss = 0.81764441\n",
      "Iteration 624, loss = 0.81699315\n",
      "Iteration 625, loss = 0.81615770\n",
      "Iteration 626, loss = 0.81541910\n",
      "Iteration 627, loss = 0.81448510\n",
      "Iteration 628, loss = 0.81377273\n",
      "Iteration 629, loss = 0.81325320\n",
      "Iteration 630, loss = 0.81255397\n",
      "Iteration 631, loss = 0.81194348\n",
      "Iteration 632, loss = 0.81141481\n",
      "Iteration 633, loss = 0.81088174\n",
      "Iteration 634, loss = 0.81043279\n",
      "Iteration 635, loss = 0.80985807\n",
      "Iteration 636, loss = 0.80923695\n",
      "Iteration 637, loss = 0.80853833\n",
      "Iteration 638, loss = 0.80782230\n",
      "Iteration 639, loss = 0.80712538\n",
      "Iteration 640, loss = 0.80646778\n",
      "Iteration 641, loss = 0.80579219\n",
      "Iteration 642, loss = 0.80508670\n",
      "Iteration 643, loss = 0.80466283\n",
      "Iteration 644, loss = 0.80429377\n",
      "Iteration 645, loss = 0.80380575\n",
      "Iteration 646, loss = 0.80324323\n",
      "Iteration 647, loss = 0.80266518\n",
      "Iteration 648, loss = 0.80187873\n",
      "Iteration 649, loss = 0.80139488\n",
      "Iteration 650, loss = 0.80058242\n",
      "Iteration 651, loss = 0.80002686\n",
      "Iteration 652, loss = 0.79933644\n",
      "Iteration 653, loss = 0.79875087\n",
      "Iteration 654, loss = 0.79780994\n",
      "Iteration 655, loss = 0.79706440\n",
      "Iteration 656, loss = 0.79658538\n",
      "Iteration 657, loss = 0.79593053\n",
      "Iteration 658, loss = 0.79573279\n",
      "Iteration 659, loss = 0.79505044\n",
      "Iteration 660, loss = 0.79428458\n",
      "Iteration 661, loss = 0.79370785\n",
      "Iteration 662, loss = 0.79309743\n",
      "Iteration 663, loss = 0.79236011\n",
      "Iteration 664, loss = 0.79195374\n",
      "Iteration 665, loss = 0.79120376\n",
      "Iteration 666, loss = 0.79058711\n",
      "Iteration 667, loss = 0.78995271\n",
      "Iteration 668, loss = 0.78935379\n",
      "Iteration 669, loss = 0.78886996\n",
      "Iteration 670, loss = 0.78818801\n",
      "Iteration 671, loss = 0.78768930\n",
      "Iteration 672, loss = 0.78747120\n",
      "Iteration 673, loss = 0.78666381\n",
      "Iteration 674, loss = 0.78609187\n",
      "Iteration 675, loss = 0.78532623\n",
      "Iteration 676, loss = 0.78524859\n",
      "Iteration 677, loss = 0.78440481\n",
      "Iteration 678, loss = 0.78354299\n",
      "Iteration 679, loss = 0.78306843\n",
      "Iteration 680, loss = 0.78240727\n",
      "Iteration 681, loss = 0.78271879\n",
      "Iteration 682, loss = 0.78229316\n",
      "Iteration 683, loss = 0.78178426\n",
      "Iteration 684, loss = 0.78099762\n",
      "Iteration 685, loss = 0.78009102\n",
      "Iteration 686, loss = 0.77925081\n",
      "Iteration 687, loss = 0.77848113\n",
      "Iteration 688, loss = 0.77818695\n",
      "Iteration 689, loss = 0.77748270\n",
      "Iteration 690, loss = 0.77708320\n",
      "Iteration 691, loss = 0.77633896\n",
      "Iteration 692, loss = 0.77595295\n",
      "Iteration 693, loss = 0.77534213\n",
      "Iteration 694, loss = 0.77481734\n",
      "Iteration 695, loss = 0.77419104\n",
      "Iteration 696, loss = 0.77346515\n",
      "Iteration 697, loss = 0.77285482\n",
      "Iteration 698, loss = 0.77249637\n",
      "Iteration 699, loss = 0.77213396\n",
      "Iteration 700, loss = 0.77161383\n",
      "Iteration 701, loss = 0.77102860\n",
      "Iteration 702, loss = 0.77044512\n",
      "Iteration 703, loss = 0.76992287\n",
      "Iteration 704, loss = 0.76899404\n",
      "Iteration 705, loss = 0.76840888\n",
      "Iteration 706, loss = 0.76786758\n",
      "Iteration 707, loss = 0.76750217\n",
      "Iteration 708, loss = 0.76683078\n",
      "Iteration 709, loss = 0.76613641\n",
      "Iteration 710, loss = 0.76562339\n",
      "Iteration 711, loss = 0.76502495\n",
      "Iteration 712, loss = 0.76434509\n",
      "Iteration 713, loss = 0.76387339\n",
      "Iteration 714, loss = 0.76334059\n",
      "Iteration 715, loss = 0.76267515\n",
      "Iteration 716, loss = 0.76210868\n",
      "Iteration 717, loss = 0.76171279\n",
      "Iteration 718, loss = 0.76132485\n",
      "Iteration 719, loss = 0.76087944\n",
      "Iteration 720, loss = 0.76054364\n",
      "Iteration 721, loss = 0.76002198\n",
      "Iteration 722, loss = 0.75964028\n",
      "Iteration 723, loss = 0.75880358\n",
      "Iteration 724, loss = 0.75804265\n",
      "Iteration 725, loss = 0.75746345\n",
      "Iteration 726, loss = 0.75695500\n",
      "Iteration 727, loss = 0.75636939\n",
      "Iteration 728, loss = 0.75582368\n",
      "Iteration 729, loss = 0.75519507\n",
      "Iteration 730, loss = 0.75496380\n",
      "Iteration 731, loss = 0.75450366\n",
      "Iteration 732, loss = 0.75395794\n",
      "Iteration 733, loss = 0.75336963\n",
      "Iteration 734, loss = 0.75277462\n",
      "Iteration 735, loss = 0.75215365\n",
      "Iteration 736, loss = 0.75193275\n",
      "Iteration 737, loss = 0.75106409\n",
      "Iteration 738, loss = 0.75055125\n",
      "Iteration 739, loss = 0.74994098\n",
      "Iteration 740, loss = 0.74977257\n",
      "Iteration 741, loss = 0.74918610\n",
      "Iteration 742, loss = 0.74873365\n",
      "Iteration 743, loss = 0.74806892\n",
      "Iteration 744, loss = 0.74780768\n",
      "Iteration 745, loss = 0.74708104\n",
      "Iteration 746, loss = 0.74650805\n",
      "Iteration 747, loss = 0.74605062\n",
      "Iteration 748, loss = 0.74598847\n",
      "Iteration 749, loss = 0.74570156\n",
      "Iteration 750, loss = 0.74503464\n",
      "Iteration 751, loss = 0.74424532\n",
      "Iteration 752, loss = 0.74370694\n",
      "Iteration 753, loss = 0.74310447\n",
      "Iteration 754, loss = 0.74267074\n",
      "Iteration 755, loss = 0.74211229\n",
      "Iteration 756, loss = 0.74192717\n",
      "Iteration 757, loss = 0.74143789\n",
      "Iteration 758, loss = 0.74103123\n",
      "Iteration 759, loss = 0.74036121\n",
      "Iteration 760, loss = 0.73988174\n",
      "Iteration 761, loss = 0.73935025\n",
      "Iteration 762, loss = 0.73849631\n",
      "Iteration 763, loss = 0.73773920\n",
      "Iteration 764, loss = 0.73718781\n",
      "Iteration 765, loss = 0.73685287\n",
      "Iteration 766, loss = 0.73638245\n",
      "Iteration 767, loss = 0.73582439\n",
      "Iteration 768, loss = 0.73542929\n",
      "Iteration 769, loss = 0.73489123\n",
      "Iteration 770, loss = 0.73433064\n",
      "Iteration 771, loss = 0.73375345\n",
      "Iteration 772, loss = 0.73335318\n",
      "Iteration 773, loss = 0.73269639\n",
      "Iteration 774, loss = 0.73224941\n",
      "Iteration 775, loss = 0.73179025\n",
      "Iteration 776, loss = 0.73133788\n",
      "Iteration 777, loss = 0.73095584\n",
      "Iteration 778, loss = 0.73047159\n",
      "Iteration 779, loss = 0.72988051\n",
      "Iteration 780, loss = 0.72931865\n",
      "Iteration 781, loss = 0.72875463\n",
      "Iteration 782, loss = 0.72829873\n",
      "Iteration 783, loss = 0.72781962\n",
      "Iteration 784, loss = 0.72742921\n",
      "Iteration 785, loss = 0.72684680\n",
      "Iteration 786, loss = 0.72638358\n",
      "Iteration 787, loss = 0.72583791\n",
      "Iteration 788, loss = 0.72530259\n",
      "Iteration 789, loss = 0.72475563\n",
      "Iteration 790, loss = 0.72423891\n",
      "Iteration 791, loss = 0.72406550\n",
      "Iteration 792, loss = 0.72372038\n",
      "Iteration 793, loss = 0.72361897\n",
      "Iteration 794, loss = 0.72348225\n",
      "Iteration 795, loss = 0.72334508\n",
      "Iteration 796, loss = 0.72263899\n",
      "Iteration 797, loss = 0.72179227\n",
      "Iteration 798, loss = 0.72124690\n",
      "Iteration 799, loss = 0.72052900\n",
      "Iteration 800, loss = 0.72029153\n",
      "Iteration 801, loss = 0.71938124\n",
      "Iteration 802, loss = 0.71868521\n",
      "Iteration 803, loss = 0.71824155\n",
      "Iteration 804, loss = 0.71786136\n",
      "Iteration 805, loss = 0.71737344\n",
      "Iteration 806, loss = 0.71694834\n",
      "Iteration 807, loss = 0.71660407\n",
      "Iteration 808, loss = 0.71611098\n",
      "Iteration 809, loss = 0.71565041\n",
      "Iteration 810, loss = 0.71559326\n",
      "Iteration 811, loss = 0.71504286\n",
      "Iteration 812, loss = 0.71447182\n",
      "Iteration 813, loss = 0.71377318\n",
      "Iteration 814, loss = 0.71311234\n",
      "Iteration 815, loss = 0.71272091\n",
      "Iteration 816, loss = 0.71201825\n",
      "Iteration 817, loss = 0.71173448\n",
      "Iteration 818, loss = 0.71122755\n",
      "Iteration 819, loss = 0.71086987\n",
      "Iteration 820, loss = 0.71023624\n",
      "Iteration 821, loss = 0.70958512\n",
      "Iteration 822, loss = 0.70947541\n",
      "Iteration 823, loss = 0.70929723\n",
      "Iteration 824, loss = 0.70924081\n",
      "Iteration 825, loss = 0.70876963\n",
      "Iteration 826, loss = 0.70812457\n",
      "Iteration 827, loss = 0.70758375\n",
      "Iteration 828, loss = 0.70667632\n",
      "Iteration 829, loss = 0.70608738\n",
      "Iteration 830, loss = 0.70554013\n",
      "Iteration 831, loss = 0.70500801\n",
      "Iteration 832, loss = 0.70475834\n",
      "Iteration 833, loss = 0.70435608\n",
      "Iteration 834, loss = 0.70410707\n",
      "Iteration 835, loss = 0.70403967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 836, loss = 0.70353471\n",
      "Iteration 837, loss = 0.70292555\n",
      "Iteration 838, loss = 0.70233930\n",
      "Iteration 839, loss = 0.70169568\n",
      "Iteration 840, loss = 0.70108250\n",
      "Iteration 841, loss = 0.70093205\n",
      "Iteration 842, loss = 0.70045285\n",
      "Iteration 843, loss = 0.70017342\n",
      "Iteration 844, loss = 0.70018342\n",
      "Iteration 845, loss = 0.70006601\n",
      "Iteration 846, loss = 0.69994410\n",
      "Iteration 847, loss = 0.69958477\n",
      "Iteration 848, loss = 0.69877622\n",
      "Iteration 849, loss = 0.69780953\n",
      "Iteration 850, loss = 0.69713575\n",
      "Iteration 851, loss = 0.69619653\n",
      "Iteration 852, loss = 0.69577685\n",
      "Iteration 853, loss = 0.69538298\n",
      "Iteration 854, loss = 0.69460190\n",
      "Iteration 855, loss = 0.69412293\n",
      "Iteration 856, loss = 0.69372663\n",
      "Iteration 857, loss = 0.69334517\n",
      "Iteration 858, loss = 0.69297942\n",
      "Iteration 859, loss = 0.69252076\n",
      "Iteration 860, loss = 0.69225972\n",
      "Iteration 861, loss = 0.69183318\n",
      "Iteration 862, loss = 0.69149338\n",
      "Iteration 863, loss = 0.69107765\n",
      "Iteration 864, loss = 0.69061012\n",
      "Iteration 865, loss = 0.68999799\n",
      "Iteration 866, loss = 0.68961592\n",
      "Iteration 867, loss = 0.68889780\n",
      "Iteration 868, loss = 0.68857650\n",
      "Iteration 869, loss = 0.68800266\n",
      "Iteration 870, loss = 0.68781327\n",
      "Iteration 871, loss = 0.68759132\n",
      "Iteration 872, loss = 0.68722537\n",
      "Iteration 873, loss = 0.68674396\n",
      "Iteration 874, loss = 0.68619624\n",
      "Iteration 875, loss = 0.68556743\n",
      "Iteration 876, loss = 0.68517312\n",
      "Iteration 877, loss = 0.68454793\n",
      "Iteration 878, loss = 0.68399238\n",
      "Iteration 879, loss = 0.68361773\n",
      "Iteration 880, loss = 0.68344087\n",
      "Iteration 881, loss = 0.68306713\n",
      "Iteration 882, loss = 0.68266430\n",
      "Iteration 883, loss = 0.68224490\n",
      "Iteration 884, loss = 0.68176861\n",
      "Iteration 885, loss = 0.68138976\n",
      "Iteration 886, loss = 0.68093150\n",
      "Iteration 887, loss = 0.68040983\n",
      "Iteration 888, loss = 0.67994935\n",
      "Iteration 889, loss = 0.67957786\n",
      "Iteration 890, loss = 0.67916938\n",
      "Iteration 891, loss = 0.67865115\n",
      "Iteration 892, loss = 0.67810100\n",
      "Iteration 893, loss = 0.67763652\n",
      "Iteration 894, loss = 0.67744971\n",
      "Iteration 895, loss = 0.67709953\n",
      "Iteration 896, loss = 0.67666107\n",
      "Iteration 897, loss = 0.67651879\n",
      "Iteration 898, loss = 0.67568084\n",
      "Iteration 899, loss = 0.67534931\n",
      "Iteration 900, loss = 0.67502131\n",
      "Iteration 901, loss = 0.67511598\n",
      "Iteration 902, loss = 0.67509785\n",
      "Iteration 903, loss = 0.67453762\n",
      "Iteration 904, loss = 0.67395692\n",
      "Iteration 905, loss = 0.67324716\n",
      "Iteration 906, loss = 0.67249851\n",
      "Iteration 907, loss = 0.67190353\n",
      "Iteration 908, loss = 0.67139852\n",
      "Iteration 909, loss = 0.67127068\n",
      "Iteration 910, loss = 0.67092978\n",
      "Iteration 911, loss = 0.67059573\n",
      "Iteration 912, loss = 0.67018156\n",
      "Iteration 913, loss = 0.66941390\n",
      "Iteration 914, loss = 0.66910059\n",
      "Iteration 915, loss = 0.66860456\n",
      "Iteration 916, loss = 0.66822618\n",
      "Iteration 917, loss = 0.66793720\n",
      "Iteration 918, loss = 0.66742596\n",
      "Iteration 919, loss = 0.66703282\n",
      "Iteration 920, loss = 0.66671053\n",
      "Iteration 921, loss = 0.66630658\n",
      "Iteration 922, loss = 0.66615557\n",
      "Iteration 923, loss = 0.66572782\n",
      "Iteration 924, loss = 0.66535452\n",
      "Iteration 925, loss = 0.66510439\n",
      "Iteration 926, loss = 0.66463695\n",
      "Iteration 927, loss = 0.66384780\n",
      "Iteration 928, loss = 0.66323797\n",
      "Iteration 929, loss = 0.66283345\n",
      "Iteration 930, loss = 0.66239624\n",
      "Iteration 931, loss = 0.66199626\n",
      "Iteration 932, loss = 0.66240784\n",
      "Iteration 933, loss = 0.66185749\n",
      "Iteration 934, loss = 0.66127516\n",
      "Iteration 935, loss = 0.66077413\n",
      "Iteration 936, loss = 0.66039912\n",
      "Iteration 937, loss = 0.65955373\n",
      "Iteration 938, loss = 0.65913131\n",
      "Iteration 939, loss = 0.65881334\n",
      "Iteration 940, loss = 0.65854073\n",
      "Iteration 941, loss = 0.65823679\n",
      "Iteration 942, loss = 0.65799713\n",
      "Iteration 943, loss = 0.65793997\n",
      "Iteration 944, loss = 0.65800282\n",
      "Iteration 945, loss = 0.65778038\n",
      "Iteration 946, loss = 0.65714732\n",
      "Iteration 947, loss = 0.65673746\n",
      "Iteration 948, loss = 0.65602710\n",
      "Iteration 949, loss = 0.65557364\n",
      "Iteration 950, loss = 0.65506001\n",
      "Iteration 951, loss = 0.65428129\n",
      "Iteration 952, loss = 0.65373136\n",
      "Iteration 953, loss = 0.65372755\n",
      "Iteration 954, loss = 0.65337088\n",
      "Iteration 955, loss = 0.65310042\n",
      "Iteration 956, loss = 0.65266435\n",
      "Iteration 957, loss = 0.65224983\n",
      "Iteration 958, loss = 0.65172377\n",
      "Iteration 959, loss = 0.65122650\n",
      "Iteration 960, loss = 0.65105046\n",
      "Iteration 961, loss = 0.65030619\n",
      "Iteration 962, loss = 0.65007029\n",
      "Iteration 963, loss = 0.64962081\n",
      "Iteration 964, loss = 0.64935142\n",
      "Iteration 965, loss = 0.64895786\n",
      "Iteration 966, loss = 0.64858276\n",
      "Iteration 967, loss = 0.64815014\n",
      "Iteration 968, loss = 0.64768381\n",
      "Iteration 969, loss = 0.64728381\n",
      "Iteration 970, loss = 0.64730395\n",
      "Iteration 971, loss = 0.64706754\n",
      "Iteration 972, loss = 0.64652951\n",
      "Iteration 973, loss = 0.64598016\n",
      "Iteration 974, loss = 0.64553579\n",
      "Iteration 975, loss = 0.64543115\n",
      "Iteration 976, loss = 0.64471009\n",
      "Iteration 977, loss = 0.64469292\n",
      "Iteration 978, loss = 0.64429750\n",
      "Iteration 979, loss = 0.64419912\n",
      "Iteration 980, loss = 0.64374626\n",
      "Iteration 981, loss = 0.64322222\n",
      "Iteration 982, loss = 0.64274551\n",
      "Iteration 983, loss = 0.64232154\n",
      "Iteration 984, loss = 0.64200738\n",
      "Iteration 985, loss = 0.64149107\n",
      "Iteration 986, loss = 0.64141542\n",
      "Iteration 987, loss = 0.64087988\n",
      "Iteration 988, loss = 0.64037259\n",
      "Iteration 989, loss = 0.63995534\n",
      "Iteration 990, loss = 0.64012274\n",
      "Iteration 991, loss = 0.63993666\n",
      "Iteration 992, loss = 0.63941698\n",
      "Iteration 993, loss = 0.63890880\n",
      "Iteration 994, loss = 0.63888955\n",
      "Iteration 995, loss = 0.63822748\n",
      "Iteration 996, loss = 0.63791973\n",
      "Iteration 997, loss = 0.63746103\n",
      "Iteration 998, loss = 0.63704514\n",
      "Iteration 999, loss = 0.63651637\n",
      "Iteration 1000, loss = 0.63606841\n",
      "Iteration 1001, loss = 0.63556956\n",
      "Iteration 1002, loss = 0.63548355\n",
      "Iteration 1003, loss = 0.63526061\n",
      "Iteration 1004, loss = 0.63490729\n",
      "Iteration 1005, loss = 0.63457614\n",
      "Iteration 1006, loss = 0.63436131\n",
      "Iteration 1007, loss = 0.63389003\n",
      "Iteration 1008, loss = 0.63329700\n",
      "Iteration 1009, loss = 0.63297662\n",
      "Iteration 1010, loss = 0.63246483\n",
      "Iteration 1011, loss = 0.63237658\n",
      "Iteration 1012, loss = 0.63191160\n",
      "Iteration 1013, loss = 0.63201644\n",
      "Iteration 1014, loss = 0.63150199\n",
      "Iteration 1015, loss = 0.63119095\n",
      "Iteration 1016, loss = 0.63071789\n",
      "Iteration 1017, loss = 0.63064472\n",
      "Iteration 1018, loss = 0.63016981\n",
      "Iteration 1019, loss = 0.62970006\n",
      "Iteration 1020, loss = 0.62921983\n",
      "Iteration 1021, loss = 0.62879726\n",
      "Iteration 1022, loss = 0.62846928\n",
      "Iteration 1023, loss = 0.62793612\n",
      "Iteration 1024, loss = 0.62775356\n",
      "Iteration 1025, loss = 0.62749077\n",
      "Iteration 1026, loss = 0.62724313\n",
      "Iteration 1027, loss = 0.62707227\n",
      "Iteration 1028, loss = 0.62653282\n",
      "Iteration 1029, loss = 0.62619442\n",
      "Iteration 1030, loss = 0.62593741\n",
      "Iteration 1031, loss = 0.62557154\n",
      "Iteration 1032, loss = 0.62517312\n",
      "Iteration 1033, loss = 0.62510135\n",
      "Iteration 1034, loss = 0.62474828\n",
      "Iteration 1035, loss = 0.62429507\n",
      "Iteration 1036, loss = 0.62391903\n",
      "Iteration 1037, loss = 0.62350906\n",
      "Iteration 1038, loss = 0.62310284\n",
      "Iteration 1039, loss = 0.62278378\n",
      "Iteration 1040, loss = 0.62234518\n",
      "Iteration 1041, loss = 0.62218343\n",
      "Iteration 1042, loss = 0.62167349\n",
      "Iteration 1043, loss = 0.62160564\n",
      "Iteration 1044, loss = 0.62128357\n",
      "Iteration 1045, loss = 0.62106048\n",
      "Iteration 1046, loss = 0.62070744\n",
      "Iteration 1047, loss = 0.62022133\n",
      "Iteration 1048, loss = 0.62001209\n",
      "Iteration 1049, loss = 0.61909588\n",
      "Iteration 1050, loss = 0.61898965\n",
      "Iteration 1051, loss = 0.61876242\n",
      "Iteration 1052, loss = 0.61844352\n",
      "Iteration 1053, loss = 0.61837761\n",
      "Iteration 1054, loss = 0.61796388\n",
      "Iteration 1055, loss = 0.61755884\n",
      "Iteration 1056, loss = 0.61709291\n",
      "Iteration 1057, loss = 0.61667744\n",
      "Iteration 1058, loss = 0.61656748\n",
      "Iteration 1059, loss = 0.61610972\n",
      "Iteration 1060, loss = 0.61602757\n",
      "Iteration 1061, loss = 0.61560227\n",
      "Iteration 1062, loss = 0.61554534\n",
      "Iteration 1063, loss = 0.61510616\n",
      "Iteration 1064, loss = 0.61520709\n",
      "Iteration 1065, loss = 0.61484358\n",
      "Iteration 1066, loss = 0.61458321\n",
      "Iteration 1067, loss = 0.61415207\n",
      "Iteration 1068, loss = 0.61351547\n",
      "Iteration 1069, loss = 0.61309601\n",
      "Iteration 1070, loss = 0.61275484\n",
      "Iteration 1071, loss = 0.61243456\n",
      "Iteration 1072, loss = 0.61210045\n",
      "Iteration 1073, loss = 0.61178048\n",
      "Iteration 1074, loss = 0.61140735\n",
      "Iteration 1075, loss = 0.61088341\n",
      "Iteration 1076, loss = 0.61072868\n",
      "Iteration 1077, loss = 0.61006681\n",
      "Iteration 1078, loss = 0.60997814\n",
      "Iteration 1079, loss = 0.60998150\n",
      "Iteration 1080, loss = 0.60978112\n",
      "Iteration 1081, loss = 0.60953074\n",
      "Iteration 1082, loss = 0.60904937\n",
      "Iteration 1083, loss = 0.60867789\n",
      "Iteration 1084, loss = 0.60838081\n",
      "Iteration 1085, loss = 0.60796913\n",
      "Iteration 1086, loss = 0.60750358\n",
      "Iteration 1087, loss = 0.60726590\n",
      "Iteration 1088, loss = 0.60680903\n",
      "Iteration 1089, loss = 0.60696198\n",
      "Iteration 1090, loss = 0.60655261\n",
      "Iteration 1091, loss = 0.60611962\n",
      "Iteration 1092, loss = 0.60568439\n",
      "Iteration 1093, loss = 0.60536283\n",
      "Iteration 1094, loss = 0.60480937\n",
      "Iteration 1095, loss = 0.60440593\n",
      "Iteration 1096, loss = 0.60423888\n",
      "Iteration 1097, loss = 0.60401637\n",
      "Iteration 1098, loss = 0.60358115\n",
      "Iteration 1099, loss = 0.60329142\n",
      "Iteration 1100, loss = 0.60299003\n",
      "Iteration 1101, loss = 0.60243601\n",
      "Iteration 1102, loss = 0.60231854\n",
      "Iteration 1103, loss = 0.60206375\n",
      "Iteration 1104, loss = 0.60177606\n",
      "Iteration 1105, loss = 0.60142282\n",
      "Iteration 1106, loss = 0.60122143\n",
      "Iteration 1107, loss = 0.60104807\n",
      "Iteration 1108, loss = 0.60045108\n",
      "Iteration 1109, loss = 0.60021859\n",
      "Iteration 1110, loss = 0.59998888\n",
      "Iteration 1111, loss = 0.60012820\n",
      "Iteration 1112, loss = 0.59999837\n",
      "Iteration 1113, loss = 0.59966099\n",
      "Iteration 1114, loss = 0.59938679\n",
      "Iteration 1115, loss = 0.59859874\n",
      "Iteration 1116, loss = 0.59826710\n",
      "Iteration 1117, loss = 0.59778467\n",
      "Iteration 1118, loss = 0.59727430\n",
      "Iteration 1119, loss = 0.59694383\n",
      "Iteration 1120, loss = 0.59700702\n",
      "Iteration 1121, loss = 0.59725415\n",
      "Iteration 1122, loss = 0.59688844\n",
      "Iteration 1123, loss = 0.59647886\n",
      "Iteration 1124, loss = 0.59596734\n",
      "Iteration 1125, loss = 0.59557393\n",
      "Iteration 1126, loss = 0.59518597\n",
      "Iteration 1127, loss = 0.59467706\n",
      "Iteration 1128, loss = 0.59461347\n",
      "Iteration 1129, loss = 0.59406175\n",
      "Iteration 1130, loss = 0.59374683\n",
      "Iteration 1131, loss = 0.59341096\n",
      "Iteration 1132, loss = 0.59325771\n",
      "Iteration 1133, loss = 0.59297106\n",
      "Iteration 1134, loss = 0.59277953\n",
      "Iteration 1135, loss = 0.59269881\n",
      "Iteration 1136, loss = 0.59244137\n",
      "Iteration 1137, loss = 0.59194207\n",
      "Iteration 1138, loss = 0.59201592\n",
      "Iteration 1139, loss = 0.59171001\n",
      "Iteration 1140, loss = 0.59136694\n",
      "Iteration 1141, loss = 0.59099822\n",
      "Iteration 1142, loss = 0.59035291\n",
      "Iteration 1143, loss = 0.59024161\n",
      "Iteration 1144, loss = 0.58974635\n",
      "Iteration 1145, loss = 0.58947720\n",
      "Iteration 1146, loss = 0.58913077\n",
      "Iteration 1147, loss = 0.58872683\n",
      "Iteration 1148, loss = 0.58838799\n",
      "Iteration 1149, loss = 0.58837499\n",
      "Iteration 1150, loss = 0.58797645\n",
      "Iteration 1151, loss = 0.58772374\n",
      "Iteration 1152, loss = 0.58725389\n",
      "Iteration 1153, loss = 0.58692874\n",
      "Iteration 1154, loss = 0.58669103\n",
      "Iteration 1155, loss = 0.58641407\n",
      "Iteration 1156, loss = 0.58611892\n",
      "Iteration 1157, loss = 0.58571262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1158, loss = 0.58544786\n",
      "Iteration 1159, loss = 0.58520335\n",
      "Iteration 1160, loss = 0.58495688\n",
      "Iteration 1161, loss = 0.58485702\n",
      "Iteration 1162, loss = 0.58456765\n",
      "Iteration 1163, loss = 0.58427342\n",
      "Iteration 1164, loss = 0.58376447\n",
      "Iteration 1165, loss = 0.58373675\n",
      "Iteration 1166, loss = 0.58360323\n",
      "Iteration 1167, loss = 0.58345498\n",
      "Iteration 1168, loss = 0.58318359\n",
      "Iteration 1169, loss = 0.58294045\n",
      "Iteration 1170, loss = 0.58268622\n",
      "Iteration 1171, loss = 0.58209817\n",
      "Iteration 1172, loss = 0.58177691\n",
      "Iteration 1173, loss = 0.58163678\n",
      "Iteration 1174, loss = 0.58147032\n",
      "Iteration 1175, loss = 0.58110170\n",
      "Iteration 1176, loss = 0.58069435\n",
      "Iteration 1177, loss = 0.58022765\n",
      "Iteration 1178, loss = 0.57980109\n",
      "Iteration 1179, loss = 0.57943906\n",
      "Iteration 1180, loss = 0.57952863\n",
      "Iteration 1181, loss = 0.57914960\n",
      "Iteration 1182, loss = 0.57880369\n",
      "Iteration 1183, loss = 0.57838330\n",
      "Iteration 1184, loss = 0.57793955\n",
      "Iteration 1185, loss = 0.57750880\n",
      "Iteration 1186, loss = 0.57771376\n",
      "Iteration 1187, loss = 0.57761050\n",
      "Iteration 1188, loss = 0.57724876\n",
      "Iteration 1189, loss = 0.57706361\n",
      "Iteration 1190, loss = 0.57713568\n",
      "Iteration 1191, loss = 0.57706426\n",
      "Iteration 1192, loss = 0.57682612\n",
      "Iteration 1193, loss = 0.57606380\n",
      "Iteration 1194, loss = 0.57568801\n",
      "Iteration 1195, loss = 0.57490741\n",
      "Iteration 1196, loss = 0.57442719\n",
      "Iteration 1197, loss = 0.57447778\n",
      "Iteration 1198, loss = 0.57430299\n",
      "Iteration 1199, loss = 0.57407082\n",
      "Iteration 1200, loss = 0.57373123\n",
      "Iteration 1201, loss = 0.57344494\n",
      "Iteration 1202, loss = 0.57295516\n",
      "Iteration 1203, loss = 0.57294061\n",
      "Iteration 1204, loss = 0.57231252\n",
      "Iteration 1205, loss = 0.57225649\n",
      "Iteration 1206, loss = 0.57177436\n",
      "Iteration 1207, loss = 0.57165773\n",
      "Iteration 1208, loss = 0.57123027\n",
      "Iteration 1209, loss = 0.57099820\n",
      "Iteration 1210, loss = 0.57066013\n",
      "Iteration 1211, loss = 0.57037131\n",
      "Iteration 1212, loss = 0.57004790\n",
      "Iteration 1213, loss = 0.56981552\n",
      "Iteration 1214, loss = 0.56979830\n",
      "Iteration 1215, loss = 0.56951852\n",
      "Iteration 1216, loss = 0.56930597\n",
      "Iteration 1217, loss = 0.56905224\n",
      "Iteration 1218, loss = 0.56885053\n",
      "Iteration 1219, loss = 0.56847675\n",
      "Iteration 1220, loss = 0.56807242\n",
      "Iteration 1221, loss = 0.56755366\n",
      "Iteration 1222, loss = 0.56783019\n",
      "Iteration 1223, loss = 0.56732213\n",
      "Iteration 1224, loss = 0.56717297\n",
      "Iteration 1225, loss = 0.56647151\n",
      "Iteration 1226, loss = 0.56644550\n",
      "Iteration 1227, loss = 0.56704229\n",
      "Iteration 1228, loss = 0.56699030\n",
      "Iteration 1229, loss = 0.56631272\n",
      "Iteration 1230, loss = 0.56574937\n",
      "Iteration 1231, loss = 0.56500139\n",
      "Iteration 1232, loss = 0.56471129\n",
      "Iteration 1233, loss = 0.56465740\n",
      "Iteration 1234, loss = 0.56461618\n",
      "Iteration 1235, loss = 0.56472572\n",
      "Iteration 1236, loss = 0.56412094\n",
      "Iteration 1237, loss = 0.56342034\n",
      "Iteration 1238, loss = 0.56302833\n",
      "Iteration 1239, loss = 0.56277354\n",
      "Iteration 1240, loss = 0.56273169\n",
      "Iteration 1241, loss = 0.56249998\n",
      "Iteration 1242, loss = 0.56218372\n",
      "Iteration 1243, loss = 0.56177778\n",
      "Iteration 1244, loss = 0.56141126\n",
      "Iteration 1245, loss = 0.56150001\n",
      "Iteration 1246, loss = 0.56094275\n",
      "Iteration 1247, loss = 0.56075177\n",
      "Iteration 1248, loss = 0.56050067\n",
      "Iteration 1249, loss = 0.56042320\n",
      "Iteration 1250, loss = 0.55996382\n",
      "Iteration 1251, loss = 0.55958801\n",
      "Iteration 1252, loss = 0.55943766\n",
      "Iteration 1253, loss = 0.55909717\n",
      "Iteration 1254, loss = 0.55877613\n",
      "Iteration 1255, loss = 0.55851978\n",
      "Iteration 1256, loss = 0.55814637\n",
      "Iteration 1257, loss = 0.55847465\n",
      "Iteration 1258, loss = 0.55800567\n",
      "Iteration 1259, loss = 0.55758904\n",
      "Iteration 1260, loss = 0.55707735\n",
      "Iteration 1261, loss = 0.55679168\n",
      "Iteration 1262, loss = 0.55663716\n",
      "Iteration 1263, loss = 0.55646180\n",
      "Iteration 1264, loss = 0.55638013\n",
      "Iteration 1265, loss = 0.55632897\n",
      "Iteration 1266, loss = 0.55593861\n",
      "Iteration 1267, loss = 0.55536131\n",
      "Iteration 1268, loss = 0.55499209\n",
      "Iteration 1269, loss = 0.55472560\n",
      "Iteration 1270, loss = 0.55471919\n",
      "Iteration 1271, loss = 0.55462976\n",
      "Iteration 1272, loss = 0.55481864\n",
      "Iteration 1273, loss = 0.55434918\n",
      "Iteration 1274, loss = 0.55381792\n",
      "Iteration 1275, loss = 0.55339965\n",
      "Iteration 1276, loss = 0.55298903\n",
      "Iteration 1277, loss = 0.55301485\n",
      "Iteration 1278, loss = 0.55260924\n",
      "Iteration 1279, loss = 0.55256304\n",
      "Iteration 1280, loss = 0.55213407\n",
      "Iteration 1281, loss = 0.55182273\n",
      "Iteration 1282, loss = 0.55137308\n",
      "Iteration 1283, loss = 0.55106836\n",
      "Iteration 1284, loss = 0.55110688\n",
      "Iteration 1285, loss = 0.55106980\n",
      "Iteration 1286, loss = 0.55074694\n",
      "Iteration 1287, loss = 0.55037936\n",
      "Iteration 1288, loss = 0.55021441\n",
      "Iteration 1289, loss = 0.54972825\n",
      "Iteration 1290, loss = 0.54999875\n",
      "Iteration 1291, loss = 0.54956988\n",
      "Iteration 1292, loss = 0.54935807\n",
      "Iteration 1293, loss = 0.54911581\n",
      "Iteration 1294, loss = 0.54857714\n",
      "Iteration 1295, loss = 0.54825186\n",
      "Iteration 1296, loss = 0.54796453\n",
      "Iteration 1297, loss = 0.54762920\n",
      "Iteration 1298, loss = 0.54734994\n",
      "Iteration 1299, loss = 0.54706935\n",
      "Iteration 1300, loss = 0.54717580\n",
      "Iteration 1301, loss = 0.54694065\n",
      "Iteration 1302, loss = 0.54664780\n",
      "Iteration 1303, loss = 0.54647646\n",
      "Iteration 1304, loss = 0.54622357\n",
      "Iteration 1305, loss = 0.54591713\n",
      "Iteration 1306, loss = 0.54542671\n",
      "Iteration 1307, loss = 0.54506636\n",
      "Iteration 1308, loss = 0.54490889\n",
      "Iteration 1309, loss = 0.54484973\n",
      "Iteration 1310, loss = 0.54455258\n",
      "Iteration 1311, loss = 0.54425135\n",
      "Iteration 1312, loss = 0.54386058\n",
      "Iteration 1313, loss = 0.54368650\n",
      "Iteration 1314, loss = 0.54345558\n",
      "Iteration 1315, loss = 0.54352812\n",
      "Iteration 1316, loss = 0.54377900\n",
      "Iteration 1317, loss = 0.54369099\n",
      "Iteration 1318, loss = 0.54279500\n",
      "Iteration 1319, loss = 0.54281123\n",
      "Iteration 1320, loss = 0.54175918\n",
      "Iteration 1321, loss = 0.54144301\n",
      "Iteration 1322, loss = 0.54113977\n",
      "Iteration 1323, loss = 0.54091827\n",
      "Iteration 1324, loss = 0.54065092\n",
      "Iteration 1325, loss = 0.54051859\n",
      "Iteration 1326, loss = 0.54018223\n",
      "Iteration 1327, loss = 0.54001206\n",
      "Iteration 1328, loss = 0.53975616\n",
      "Iteration 1329, loss = 0.53945590\n",
      "Iteration 1330, loss = 0.53913860\n",
      "Iteration 1331, loss = 0.53905029\n",
      "Iteration 1332, loss = 0.53890122\n",
      "Iteration 1333, loss = 0.53880697\n",
      "Iteration 1334, loss = 0.53855928\n",
      "Iteration 1335, loss = 0.53804788\n",
      "Iteration 1336, loss = 0.53828962\n",
      "Iteration 1337, loss = 0.53751986\n",
      "Iteration 1338, loss = 0.53744115\n",
      "Iteration 1339, loss = 0.53706827\n",
      "Iteration 1340, loss = 0.53683745\n",
      "Iteration 1341, loss = 0.53637213\n",
      "Iteration 1342, loss = 0.53643594\n",
      "Iteration 1343, loss = 0.53601173\n",
      "Iteration 1344, loss = 0.53598246\n",
      "Iteration 1345, loss = 0.53582789\n",
      "Iteration 1346, loss = 0.53559516\n",
      "Iteration 1347, loss = 0.53516576\n",
      "Iteration 1348, loss = 0.53486221\n",
      "Iteration 1349, loss = 0.53455811\n",
      "Iteration 1350, loss = 0.53422949\n",
      "Iteration 1351, loss = 0.53400219\n",
      "Iteration 1352, loss = 0.53407916\n",
      "Iteration 1353, loss = 0.53379306\n",
      "Iteration 1354, loss = 0.53344993\n",
      "Iteration 1355, loss = 0.53312206\n",
      "Iteration 1356, loss = 0.53276231\n",
      "Iteration 1357, loss = 0.53254886\n",
      "Iteration 1358, loss = 0.53232719\n",
      "Iteration 1359, loss = 0.53208009\n",
      "Iteration 1360, loss = 0.53186080\n",
      "Iteration 1361, loss = 0.53174114\n",
      "Iteration 1362, loss = 0.53141901\n",
      "Iteration 1363, loss = 0.53141547\n",
      "Iteration 1364, loss = 0.53096604\n",
      "Iteration 1365, loss = 0.53078554\n",
      "Iteration 1366, loss = 0.53050851\n",
      "Iteration 1367, loss = 0.53040563\n",
      "Iteration 1368, loss = 0.53014321\n",
      "Iteration 1369, loss = 0.52991388\n",
      "Iteration 1370, loss = 0.52958694\n",
      "Iteration 1371, loss = 0.52918571\n",
      "Iteration 1372, loss = 0.52905163\n",
      "Iteration 1373, loss = 0.52889249\n",
      "Iteration 1374, loss = 0.52871110\n",
      "Iteration 1375, loss = 0.52856862\n",
      "Iteration 1376, loss = 0.52820868\n",
      "Iteration 1377, loss = 0.52762028\n",
      "Iteration 1378, loss = 0.52709321\n",
      "Iteration 1379, loss = 0.52719900\n",
      "Iteration 1380, loss = 0.52719124\n",
      "Iteration 1381, loss = 0.52721081\n",
      "Iteration 1382, loss = 0.52704185\n",
      "Iteration 1383, loss = 0.52662184\n",
      "Iteration 1384, loss = 0.52601199\n",
      "Iteration 1385, loss = 0.52566390\n",
      "Iteration 1386, loss = 0.52542191\n",
      "Iteration 1387, loss = 0.52524544\n",
      "Iteration 1388, loss = 0.52489126\n",
      "Iteration 1389, loss = 0.52476953\n",
      "Iteration 1390, loss = 0.52487396\n",
      "Iteration 1391, loss = 0.52440072\n",
      "Iteration 1392, loss = 0.52423710\n",
      "Iteration 1393, loss = 0.52370596\n",
      "Iteration 1394, loss = 0.52333086\n",
      "Iteration 1395, loss = 0.52331350\n",
      "Iteration 1396, loss = 0.52300337\n",
      "Iteration 1397, loss = 0.52283285\n",
      "Iteration 1398, loss = 0.52248743\n",
      "Iteration 1399, loss = 0.52220722\n",
      "Iteration 1400, loss = 0.52189797\n",
      "Iteration 1401, loss = 0.52163235\n",
      "Iteration 1402, loss = 0.52149744\n",
      "Iteration 1403, loss = 0.52113993\n",
      "Iteration 1404, loss = 0.52108209\n",
      "Iteration 1405, loss = 0.52073121\n",
      "Iteration 1406, loss = 0.52047939\n",
      "Iteration 1407, loss = 0.52018364\n",
      "Iteration 1408, loss = 0.51999917\n",
      "Iteration 1409, loss = 0.51965916\n",
      "Iteration 1410, loss = 0.51941324\n",
      "Iteration 1411, loss = 0.51922215\n",
      "Iteration 1412, loss = 0.51905951\n",
      "Iteration 1413, loss = 0.51882030\n",
      "Iteration 1414, loss = 0.51867222\n",
      "Iteration 1415, loss = 0.51843832\n",
      "Iteration 1416, loss = 0.51821162\n",
      "Iteration 1417, loss = 0.51792908\n",
      "Iteration 1418, loss = 0.51761169\n",
      "Iteration 1419, loss = 0.51740382\n",
      "Iteration 1420, loss = 0.51718166\n",
      "Iteration 1421, loss = 0.51714928\n",
      "Iteration 1422, loss = 0.51682225\n",
      "Iteration 1423, loss = 0.51652266\n",
      "Iteration 1424, loss = 0.51626794\n",
      "Iteration 1425, loss = 0.51594092\n",
      "Iteration 1426, loss = 0.51553746\n",
      "Iteration 1427, loss = 0.51553811\n",
      "Iteration 1428, loss = 0.51565220\n",
      "Iteration 1429, loss = 0.51548877\n",
      "Iteration 1430, loss = 0.51518759\n",
      "Iteration 1431, loss = 0.51499771\n",
      "Iteration 1432, loss = 0.51480915\n",
      "Iteration 1433, loss = 0.51426397\n",
      "Iteration 1434, loss = 0.51417118\n",
      "Iteration 1435, loss = 0.51404775\n",
      "Iteration 1436, loss = 0.51396801\n",
      "Iteration 1437, loss = 0.51398945\n",
      "Iteration 1438, loss = 0.51400487\n",
      "Iteration 1439, loss = 0.51404773\n",
      "Iteration 1440, loss = 0.51383326\n",
      "Iteration 1441, loss = 0.51304435\n",
      "Iteration 1442, loss = 0.51235933\n",
      "Iteration 1443, loss = 0.51150513\n",
      "Iteration 1444, loss = 0.51188493\n",
      "Iteration 1445, loss = 0.51199884\n",
      "Iteration 1446, loss = 0.51252058\n",
      "Iteration 1447, loss = 0.51239756\n",
      "Iteration 1448, loss = 0.51199717\n",
      "Iteration 1449, loss = 0.51107184\n",
      "Iteration 1450, loss = 0.51064489\n",
      "Iteration 1451, loss = 0.51075583\n",
      "Iteration 1452, loss = 0.50961217\n",
      "Iteration 1453, loss = 0.50971410\n",
      "Iteration 1454, loss = 0.50941894\n",
      "Iteration 1455, loss = 0.50927766\n",
      "Iteration 1456, loss = 0.50931399\n",
      "Iteration 1457, loss = 0.50915781\n",
      "Iteration 1458, loss = 0.50893529\n",
      "Iteration 1459, loss = 0.50856251\n",
      "Iteration 1460, loss = 0.50823494\n",
      "Iteration 1461, loss = 0.50831165\n",
      "Iteration 1462, loss = 0.50777582\n",
      "Iteration 1463, loss = 0.50739487\n",
      "Iteration 1464, loss = 0.50691690\n",
      "Iteration 1465, loss = 0.50667836\n",
      "Iteration 1466, loss = 0.50642053\n",
      "Iteration 1467, loss = 0.50649680\n",
      "Iteration 1468, loss = 0.50668223\n",
      "Iteration 1469, loss = 0.50646773\n",
      "Iteration 1470, loss = 0.50621652\n",
      "Iteration 1471, loss = 0.50584104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1472, loss = 0.50561848\n",
      "Iteration 1473, loss = 0.50508400\n",
      "Iteration 1474, loss = 0.50478448\n",
      "Iteration 1475, loss = 0.50447090\n",
      "Iteration 1476, loss = 0.50435033\n",
      "Iteration 1477, loss = 0.50463458\n",
      "Iteration 1478, loss = 0.50516707\n",
      "Iteration 1479, loss = 0.50491201\n",
      "Iteration 1480, loss = 0.50437202\n",
      "Iteration 1481, loss = 0.50348922\n",
      "Iteration 1482, loss = 0.50313143\n",
      "Iteration 1483, loss = 0.50344014\n",
      "Iteration 1484, loss = 0.50294009\n",
      "Iteration 1485, loss = 0.50262315\n",
      "Iteration 1486, loss = 0.50230513\n",
      "Iteration 1487, loss = 0.50181308\n",
      "Iteration 1488, loss = 0.50187946\n",
      "Iteration 1489, loss = 0.50134551\n",
      "Iteration 1490, loss = 0.50121032\n",
      "Iteration 1491, loss = 0.50114075\n",
      "Iteration 1492, loss = 0.50108794\n",
      "Iteration 1493, loss = 0.50146738\n",
      "Iteration 1494, loss = 0.50136601\n",
      "Iteration 1495, loss = 0.50087372\n",
      "Iteration 1496, loss = 0.50032824\n",
      "Iteration 1497, loss = 0.49960023\n",
      "Iteration 1498, loss = 0.49924611\n",
      "Iteration 1499, loss = 0.49937324\n",
      "Iteration 1500, loss = 0.49902250\n",
      "Iteration 1501, loss = 0.49901529\n",
      "Iteration 1502, loss = 0.49887223\n",
      "Iteration 1503, loss = 0.49877962\n",
      "Iteration 1504, loss = 0.49827622\n",
      "Iteration 1505, loss = 0.49787129\n",
      "Iteration 1506, loss = 0.49736773\n",
      "Iteration 1507, loss = 0.49764957\n",
      "Iteration 1508, loss = 0.49694884\n",
      "Iteration 1509, loss = 0.49669605\n",
      "Iteration 1510, loss = 0.49630985\n",
      "Iteration 1511, loss = 0.49713471\n",
      "Iteration 1512, loss = 0.49732954\n",
      "Iteration 1513, loss = 0.49702038\n",
      "Iteration 1514, loss = 0.49655685\n",
      "Iteration 1515, loss = 0.49598813\n",
      "Iteration 1516, loss = 0.49561396\n",
      "Iteration 1517, loss = 0.49541676\n",
      "Iteration 1518, loss = 0.49488334\n",
      "Iteration 1519, loss = 0.49488641\n",
      "Iteration 1520, loss = 0.49466346\n",
      "Iteration 1521, loss = 0.49466711\n",
      "Iteration 1522, loss = 0.49407045\n",
      "Iteration 1523, loss = 0.49400211\n",
      "Iteration 1524, loss = 0.49381457\n",
      "Iteration 1525, loss = 0.49359311\n",
      "Iteration 1526, loss = 0.49310864\n",
      "Iteration 1527, loss = 0.49283001\n",
      "Iteration 1528, loss = 0.49279479\n",
      "Iteration 1529, loss = 0.49229655\n",
      "Iteration 1530, loss = 0.49210301\n",
      "Iteration 1531, loss = 0.49204042\n",
      "Iteration 1532, loss = 0.49207324\n",
      "Iteration 1533, loss = 0.49165567\n",
      "Iteration 1534, loss = 0.49155846\n",
      "Iteration 1535, loss = 0.49121355\n",
      "Iteration 1536, loss = 0.49072986\n",
      "Iteration 1537, loss = 0.49051658\n",
      "Iteration 1538, loss = 0.49074918\n",
      "Iteration 1539, loss = 0.49165003\n",
      "Iteration 1540, loss = 0.49167750\n",
      "Iteration 1541, loss = 0.49103010\n",
      "Iteration 1542, loss = 0.48990910\n",
      "Iteration 1543, loss = 0.48903188\n",
      "Iteration 1544, loss = 0.48888013\n",
      "Iteration 1545, loss = 0.48987310\n",
      "Iteration 1546, loss = 0.49024158\n",
      "Iteration 1547, loss = 0.48992919\n",
      "Iteration 1548, loss = 0.48930901\n",
      "Iteration 1549, loss = 0.48854959\n",
      "Iteration 1550, loss = 0.48822438\n",
      "Iteration 1551, loss = 0.48781171\n",
      "Iteration 1552, loss = 0.48761811\n",
      "Iteration 1553, loss = 0.48724295\n",
      "Iteration 1554, loss = 0.48694554\n",
      "Iteration 1555, loss = 0.48681802\n",
      "Iteration 1556, loss = 0.48652765\n",
      "Iteration 1557, loss = 0.48635942\n",
      "Iteration 1558, loss = 0.48609192\n",
      "Iteration 1559, loss = 0.48594478\n",
      "Iteration 1560, loss = 0.48568652\n",
      "Iteration 1561, loss = 0.48553743\n",
      "Iteration 1562, loss = 0.48521526\n",
      "Iteration 1563, loss = 0.48519680\n",
      "Iteration 1564, loss = 0.48505188\n",
      "Iteration 1565, loss = 0.48485100\n",
      "Iteration 1566, loss = 0.48452496\n",
      "Iteration 1567, loss = 0.48435475\n",
      "Iteration 1568, loss = 0.48376176\n",
      "Iteration 1569, loss = 0.48395992\n",
      "Iteration 1570, loss = 0.48409788\n",
      "Iteration 1571, loss = 0.48382835\n",
      "Iteration 1572, loss = 0.48328752\n",
      "Iteration 1573, loss = 0.48296508\n",
      "Iteration 1574, loss = 0.48327427\n",
      "Iteration 1575, loss = 0.48294564\n",
      "Iteration 1576, loss = 0.48275936\n",
      "Iteration 1577, loss = 0.48237877\n",
      "Iteration 1578, loss = 0.48204040\n",
      "Iteration 1579, loss = 0.48179034\n",
      "Iteration 1580, loss = 0.48195965\n",
      "Iteration 1581, loss = 0.48175580\n",
      "Iteration 1582, loss = 0.48146557\n",
      "Iteration 1583, loss = 0.48111327\n",
      "Iteration 1584, loss = 0.48074210\n",
      "Iteration 1585, loss = 0.48054112\n",
      "Iteration 1586, loss = 0.48019478\n",
      "Iteration 1587, loss = 0.47984816\n",
      "Iteration 1588, loss = 0.47979166\n",
      "Iteration 1589, loss = 0.47960759\n",
      "Iteration 1590, loss = 0.47942022\n",
      "Iteration 1591, loss = 0.47937728\n",
      "Iteration 1592, loss = 0.47928632\n",
      "Iteration 1593, loss = 0.47920153\n",
      "Iteration 1594, loss = 0.47906236\n",
      "Iteration 1595, loss = 0.47964850\n",
      "Iteration 1596, loss = 0.47925474\n",
      "Iteration 1597, loss = 0.47878216\n",
      "Iteration 1598, loss = 0.47828684\n",
      "Iteration 1599, loss = 0.47844350\n",
      "Iteration 1600, loss = 0.47794181\n",
      "Iteration 1601, loss = 0.47746689\n",
      "Iteration 1602, loss = 0.47724667\n",
      "Iteration 1603, loss = 0.47671477\n",
      "Iteration 1604, loss = 0.47666334\n",
      "Iteration 1605, loss = 0.47632705\n",
      "Iteration 1606, loss = 0.47625463\n",
      "Iteration 1607, loss = 0.47616549\n",
      "Iteration 1608, loss = 0.47644372\n",
      "Iteration 1609, loss = 0.47711362\n",
      "Iteration 1610, loss = 0.47713534\n",
      "Iteration 1611, loss = 0.47650836\n",
      "Iteration 1612, loss = 0.47579734\n",
      "Iteration 1613, loss = 0.47524656\n",
      "Iteration 1614, loss = 0.47422583\n",
      "Iteration 1615, loss = 0.47425232\n",
      "Iteration 1616, loss = 0.47442484\n",
      "Iteration 1617, loss = 0.47452476\n",
      "Iteration 1618, loss = 0.47419061\n",
      "Iteration 1619, loss = 0.47386314\n",
      "Iteration 1620, loss = 0.47323581\n",
      "Iteration 1621, loss = 0.47294848\n",
      "Iteration 1622, loss = 0.47269736\n",
      "Iteration 1623, loss = 0.47249671\n",
      "Iteration 1624, loss = 0.47228924\n",
      "Iteration 1625, loss = 0.47219694\n",
      "Iteration 1626, loss = 0.47204358\n",
      "Iteration 1627, loss = 0.47164851\n",
      "Iteration 1628, loss = 0.47144269\n",
      "Iteration 1629, loss = 0.47148590\n",
      "Iteration 1630, loss = 0.47160381\n",
      "Iteration 1631, loss = 0.47137408\n",
      "Iteration 1632, loss = 0.47106776\n",
      "Iteration 1633, loss = 0.47078102\n",
      "Iteration 1634, loss = 0.47043417\n",
      "Iteration 1635, loss = 0.47025652\n",
      "Iteration 1636, loss = 0.46989269\n",
      "Iteration 1637, loss = 0.46968719\n",
      "Iteration 1638, loss = 0.46967015\n",
      "Iteration 1639, loss = 0.46919454\n",
      "Iteration 1640, loss = 0.46900155\n",
      "Iteration 1641, loss = 0.46937554\n",
      "Iteration 1642, loss = 0.46914519\n",
      "Iteration 1643, loss = 0.46900697\n",
      "Iteration 1644, loss = 0.46865675\n",
      "Iteration 1645, loss = 0.46820569\n",
      "Iteration 1646, loss = 0.46795051\n",
      "Iteration 1647, loss = 0.46769839\n",
      "Iteration 1648, loss = 0.46796223\n",
      "Iteration 1649, loss = 0.46769513\n",
      "Iteration 1650, loss = 0.46743891\n",
      "Iteration 1651, loss = 0.46712796\n",
      "Iteration 1652, loss = 0.46673959\n",
      "Iteration 1653, loss = 0.46654983\n",
      "Iteration 1654, loss = 0.46627707\n",
      "Iteration 1655, loss = 0.46605426\n",
      "Iteration 1656, loss = 0.46621117\n",
      "Iteration 1657, loss = 0.46592633\n",
      "Iteration 1658, loss = 0.46594040\n",
      "Iteration 1659, loss = 0.46531071\n",
      "Iteration 1660, loss = 0.46521753\n",
      "Iteration 1661, loss = 0.46507739\n",
      "Iteration 1662, loss = 0.46469207\n",
      "Iteration 1663, loss = 0.46460191\n",
      "Iteration 1664, loss = 0.46423617\n",
      "Iteration 1665, loss = 0.46400842\n",
      "Iteration 1666, loss = 0.46383874\n",
      "Iteration 1667, loss = 0.46366874\n",
      "Iteration 1668, loss = 0.46356054\n",
      "Iteration 1669, loss = 0.46332536\n",
      "Iteration 1670, loss = 0.46306195\n",
      "Iteration 1671, loss = 0.46300298\n",
      "Iteration 1672, loss = 0.46264808\n",
      "Iteration 1673, loss = 0.46245694\n",
      "Iteration 1674, loss = 0.46239262\n",
      "Iteration 1675, loss = 0.46212030\n",
      "Iteration 1676, loss = 0.46185961\n",
      "Iteration 1677, loss = 0.46205395\n",
      "Iteration 1678, loss = 0.46159750\n",
      "Iteration 1679, loss = 0.46157613\n",
      "Iteration 1680, loss = 0.46110385\n",
      "Iteration 1681, loss = 0.46088836\n",
      "Iteration 1682, loss = 0.46063604\n",
      "Iteration 1683, loss = 0.46043874\n",
      "Iteration 1684, loss = 0.46031916\n",
      "Iteration 1685, loss = 0.46053900\n",
      "Iteration 1686, loss = 0.46095185\n",
      "Iteration 1687, loss = 0.46122767\n",
      "Iteration 1688, loss = 0.46132619\n",
      "Iteration 1689, loss = 0.46112415\n",
      "Iteration 1690, loss = 0.46043907\n",
      "Iteration 1691, loss = 0.45957657\n",
      "Iteration 1692, loss = 0.45926100\n",
      "Iteration 1693, loss = 0.45881279\n",
      "Iteration 1694, loss = 0.45884935\n",
      "Iteration 1695, loss = 0.45847452\n",
      "Iteration 1696, loss = 0.45782001\n",
      "Iteration 1697, loss = 0.45797037\n",
      "Iteration 1698, loss = 0.45797093\n",
      "Iteration 1699, loss = 0.45844639\n",
      "Iteration 1700, loss = 0.45814260\n",
      "Iteration 1701, loss = 0.45773757\n",
      "Iteration 1702, loss = 0.45718772\n",
      "Iteration 1703, loss = 0.45726577\n",
      "Iteration 1704, loss = 0.45650994\n",
      "Iteration 1705, loss = 0.45646237\n",
      "Iteration 1706, loss = 0.45584397\n",
      "Iteration 1707, loss = 0.45574897\n",
      "Iteration 1708, loss = 0.45552420\n",
      "Iteration 1709, loss = 0.45537445\n",
      "Iteration 1710, loss = 0.45540550\n",
      "Iteration 1711, loss = 0.45512473\n",
      "Iteration 1712, loss = 0.45487499\n",
      "Iteration 1713, loss = 0.45474149\n",
      "Iteration 1714, loss = 0.45501123\n",
      "Iteration 1715, loss = 0.45493313\n",
      "Iteration 1716, loss = 0.45469468\n",
      "Iteration 1717, loss = 0.45413484\n",
      "Iteration 1718, loss = 0.45369242\n",
      "Iteration 1719, loss = 0.45398290\n",
      "Iteration 1720, loss = 0.45387602\n",
      "Iteration 1721, loss = 0.45412073\n",
      "Iteration 1722, loss = 0.45410445\n",
      "Iteration 1723, loss = 0.45398614\n",
      "Iteration 1724, loss = 0.45359858\n",
      "Iteration 1725, loss = 0.45312513\n",
      "Iteration 1726, loss = 0.45269417\n",
      "Iteration 1727, loss = 0.45260953\n",
      "Iteration 1728, loss = 0.45209036\n",
      "Iteration 1729, loss = 0.45169438\n",
      "Iteration 1730, loss = 0.45132783\n",
      "Iteration 1731, loss = 0.45115412\n",
      "Iteration 1732, loss = 0.45095196\n",
      "Iteration 1733, loss = 0.45095373\n",
      "Iteration 1734, loss = 0.45134963\n",
      "Iteration 1735, loss = 0.45176710\n",
      "Iteration 1736, loss = 0.45211870\n",
      "Iteration 1737, loss = 0.45200083\n",
      "Iteration 1738, loss = 0.45127138\n",
      "Iteration 1739, loss = 0.45068555\n",
      "Iteration 1740, loss = 0.44998804\n",
      "Iteration 1741, loss = 0.44951589\n",
      "Iteration 1742, loss = 0.44922081\n",
      "Iteration 1743, loss = 0.44893899\n",
      "Iteration 1744, loss = 0.44863913\n",
      "Iteration 1745, loss = 0.44866775\n",
      "Iteration 1746, loss = 0.44859374\n",
      "Iteration 1747, loss = 0.44846117\n",
      "Iteration 1748, loss = 0.44825984\n",
      "Iteration 1749, loss = 0.44798254\n",
      "Iteration 1750, loss = 0.44765958\n",
      "Iteration 1751, loss = 0.44717722\n",
      "Iteration 1752, loss = 0.44706896\n",
      "Iteration 1753, loss = 0.44734037\n",
      "Iteration 1754, loss = 0.44726516\n",
      "Iteration 1755, loss = 0.44698509\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1756, loss = 0.44673038\n",
      "Iteration 1757, loss = 0.44663125\n",
      "Iteration 1758, loss = 0.44648438\n",
      "Iteration 1759, loss = 0.44644792\n",
      "Iteration 1760, loss = 0.44649088\n",
      "Iteration 1761, loss = 0.44674970\n",
      "Iteration 1762, loss = 0.44636579\n",
      "Iteration 1763, loss = 0.44575994\n",
      "Iteration 1764, loss = 0.44528772\n",
      "Iteration 1765, loss = 0.44525700\n",
      "Iteration 1766, loss = 0.44456367\n",
      "Iteration 1767, loss = 0.44454838\n",
      "Iteration 1768, loss = 0.44446318\n",
      "Iteration 1769, loss = 0.44427352\n",
      "Iteration 1770, loss = 0.44393236\n",
      "Iteration 1771, loss = 0.44381953\n",
      "Iteration 1772, loss = 0.44366350\n",
      "Iteration 1773, loss = 0.44350870\n",
      "Iteration 1774, loss = 0.44306200\n",
      "Iteration 1775, loss = 0.44283925\n",
      "Iteration 1776, loss = 0.44281398\n",
      "Iteration 1777, loss = 0.44276613\n",
      "Iteration 1778, loss = 0.44295532\n",
      "Iteration 1779, loss = 0.44301334\n",
      "Iteration 1780, loss = 0.44261189\n",
      "Iteration 1781, loss = 0.44226110\n",
      "Iteration 1782, loss = 0.44212430\n",
      "Iteration 1783, loss = 0.44151510\n",
      "Iteration 1784, loss = 0.44112176\n",
      "Iteration 1785, loss = 0.44089765\n",
      "Iteration 1786, loss = 0.44074448\n",
      "Iteration 1787, loss = 0.44054013\n",
      "Iteration 1788, loss = 0.44033323\n",
      "Iteration 1789, loss = 0.44028068\n",
      "Iteration 1790, loss = 0.44011113\n",
      "Iteration 1791, loss = 0.44020622\n",
      "Iteration 1792, loss = 0.44013545\n",
      "Iteration 1793, loss = 0.43988682\n",
      "Iteration 1794, loss = 0.43952054\n",
      "Iteration 1795, loss = 0.43916610\n",
      "Iteration 1796, loss = 0.43911454\n",
      "Iteration 1797, loss = 0.43897770\n",
      "Iteration 1798, loss = 0.43878786\n",
      "Iteration 1799, loss = 0.43838498\n",
      "Iteration 1800, loss = 0.43833091\n",
      "Iteration 1801, loss = 0.43806741\n",
      "Iteration 1802, loss = 0.43785111\n",
      "Iteration 1803, loss = 0.43785973\n",
      "Iteration 1804, loss = 0.43820967\n",
      "Iteration 1805, loss = 0.43814481\n",
      "Iteration 1806, loss = 0.43748043\n",
      "Iteration 1807, loss = 0.43723942\n",
      "Iteration 1808, loss = 0.43690224\n",
      "Iteration 1809, loss = 0.43647326\n",
      "Iteration 1810, loss = 0.43653306\n",
      "Iteration 1811, loss = 0.43601347\n",
      "Iteration 1812, loss = 0.43589729\n",
      "Iteration 1813, loss = 0.43568818\n",
      "Iteration 1814, loss = 0.43585664\n",
      "Iteration 1815, loss = 0.43548783\n",
      "Iteration 1816, loss = 0.43521486\n",
      "Iteration 1817, loss = 0.43525035\n",
      "Iteration 1818, loss = 0.43518162\n",
      "Iteration 1819, loss = 0.43484378\n",
      "Iteration 1820, loss = 0.43449205\n",
      "Iteration 1821, loss = 0.43433948\n",
      "Iteration 1822, loss = 0.43444825\n",
      "Iteration 1823, loss = 0.43475862\n",
      "Iteration 1824, loss = 0.43402116\n",
      "Iteration 1825, loss = 0.43407291\n",
      "Iteration 1826, loss = 0.43370371\n",
      "Iteration 1827, loss = 0.43339047\n",
      "Iteration 1828, loss = 0.43317214\n",
      "Iteration 1829, loss = 0.43297487\n",
      "Iteration 1830, loss = 0.43289928\n",
      "Iteration 1831, loss = 0.43276858\n",
      "Iteration 1832, loss = 0.43279088\n",
      "Iteration 1833, loss = 0.43238916\n",
      "Iteration 1834, loss = 0.43205753\n",
      "Iteration 1835, loss = 0.43214281\n",
      "Iteration 1836, loss = 0.43163394\n",
      "Iteration 1837, loss = 0.43152223\n",
      "Iteration 1838, loss = 0.43138178\n",
      "Iteration 1839, loss = 0.43116103\n",
      "Iteration 1840, loss = 0.43087052\n",
      "Iteration 1841, loss = 0.43061582\n",
      "Iteration 1842, loss = 0.43067323\n",
      "Iteration 1843, loss = 0.43054708\n",
      "Iteration 1844, loss = 0.43036560\n",
      "Iteration 1845, loss = 0.43007701\n",
      "Iteration 1846, loss = 0.43004058\n",
      "Iteration 1847, loss = 0.42973890\n",
      "Iteration 1848, loss = 0.42955780\n",
      "Iteration 1849, loss = 0.42922927\n",
      "Iteration 1850, loss = 0.42936525\n",
      "Iteration 1851, loss = 0.42902935\n",
      "Iteration 1852, loss = 0.42884112\n",
      "Iteration 1853, loss = 0.42867486\n",
      "Iteration 1854, loss = 0.42844323\n",
      "Iteration 1855, loss = 0.42829347\n",
      "Iteration 1856, loss = 0.42811492\n",
      "Iteration 1857, loss = 0.42790522\n",
      "Iteration 1858, loss = 0.42764388\n",
      "Iteration 1859, loss = 0.42751812\n",
      "Iteration 1860, loss = 0.42748057\n",
      "Iteration 1861, loss = 0.42726764\n",
      "Iteration 1862, loss = 0.42717450\n",
      "Iteration 1863, loss = 0.42693200\n",
      "Iteration 1864, loss = 0.42678657\n",
      "Iteration 1865, loss = 0.42658262\n",
      "Iteration 1866, loss = 0.42635450\n",
      "Iteration 1867, loss = 0.42605895\n",
      "Iteration 1868, loss = 0.42615045\n",
      "Iteration 1869, loss = 0.42591064\n",
      "Iteration 1870, loss = 0.42575002\n",
      "Iteration 1871, loss = 0.42556361\n",
      "Iteration 1872, loss = 0.42552995\n",
      "Iteration 1873, loss = 0.42541537\n",
      "Iteration 1874, loss = 0.42530041\n",
      "Iteration 1875, loss = 0.42494167\n",
      "Iteration 1876, loss = 0.42468399\n",
      "Iteration 1877, loss = 0.42454455\n",
      "Iteration 1878, loss = 0.42438932\n",
      "Iteration 1879, loss = 0.42434049\n",
      "Iteration 1880, loss = 0.42396157\n",
      "Iteration 1881, loss = 0.42367345\n",
      "Iteration 1882, loss = 0.42345891\n",
      "Iteration 1883, loss = 0.42332123\n",
      "Iteration 1884, loss = 0.42334901\n",
      "Iteration 1885, loss = 0.42321988\n",
      "Iteration 1886, loss = 0.42294005\n",
      "Iteration 1887, loss = 0.42303301\n",
      "Iteration 1888, loss = 0.42271995\n",
      "Iteration 1889, loss = 0.42223936\n",
      "Iteration 1890, loss = 0.42229751\n",
      "Iteration 1891, loss = 0.42178822\n",
      "Iteration 1892, loss = 0.42180243\n",
      "Iteration 1893, loss = 0.42186012\n",
      "Iteration 1894, loss = 0.42174460\n",
      "Iteration 1895, loss = 0.42199700\n",
      "Iteration 1896, loss = 0.42203594\n",
      "Iteration 1897, loss = 0.42151960\n",
      "Iteration 1898, loss = 0.42104512\n",
      "Iteration 1899, loss = 0.42057123\n",
      "Iteration 1900, loss = 0.42051021\n",
      "Iteration 1901, loss = 0.42030963\n",
      "Iteration 1902, loss = 0.41994582\n",
      "Iteration 1903, loss = 0.41988647\n",
      "Iteration 1904, loss = 0.42036403\n",
      "Iteration 1905, loss = 0.41966706\n",
      "Iteration 1906, loss = 0.41955180\n",
      "Iteration 1907, loss = 0.41913448\n",
      "Iteration 1908, loss = 0.41890263\n",
      "Iteration 1909, loss = 0.41891508\n",
      "Iteration 1910, loss = 0.41872115\n",
      "Iteration 1911, loss = 0.41845416\n",
      "Iteration 1912, loss = 0.41842484\n",
      "Iteration 1913, loss = 0.41805091\n",
      "Iteration 1914, loss = 0.41781057\n",
      "Iteration 1915, loss = 0.41786676\n",
      "Iteration 1916, loss = 0.41805200\n",
      "Iteration 1917, loss = 0.41802901\n",
      "Iteration 1918, loss = 0.41775715\n",
      "Iteration 1919, loss = 0.41740374\n",
      "Iteration 1920, loss = 0.41705489\n",
      "Iteration 1921, loss = 0.41674427\n",
      "Iteration 1922, loss = 0.41693575\n",
      "Iteration 1923, loss = 0.41668604\n",
      "Iteration 1924, loss = 0.41653981\n",
      "Iteration 1925, loss = 0.41614188\n",
      "Iteration 1926, loss = 0.41600670\n",
      "Iteration 1927, loss = 0.41577701\n",
      "Iteration 1928, loss = 0.41564206\n",
      "Iteration 1929, loss = 0.41548847\n",
      "Iteration 1930, loss = 0.41542333\n",
      "Iteration 1931, loss = 0.41537714\n",
      "Iteration 1932, loss = 0.41491231\n",
      "Iteration 1933, loss = 0.41519997\n",
      "Iteration 1934, loss = 0.41515593\n",
      "Iteration 1935, loss = 0.41502355\n",
      "Iteration 1936, loss = 0.41498204\n",
      "Iteration 1937, loss = 0.41481238\n",
      "Iteration 1938, loss = 0.41418881\n",
      "Iteration 1939, loss = 0.41363772\n",
      "Iteration 1940, loss = 0.41342733\n",
      "Iteration 1941, loss = 0.41406089\n",
      "Iteration 1942, loss = 0.41428653\n",
      "Iteration 1943, loss = 0.41457274\n",
      "Iteration 1944, loss = 0.41447355\n",
      "Iteration 1945, loss = 0.41478948\n",
      "Iteration 1946, loss = 0.41428314\n",
      "Iteration 1947, loss = 0.41363638\n",
      "Iteration 1948, loss = 0.41272525\n",
      "Iteration 1949, loss = 0.41228701\n",
      "Iteration 1950, loss = 0.41197854\n",
      "Iteration 1951, loss = 0.41175733\n",
      "Iteration 1952, loss = 0.41145822\n",
      "Iteration 1953, loss = 0.41128037\n",
      "Iteration 1954, loss = 0.41117369\n",
      "Iteration 1955, loss = 0.41094051\n",
      "Iteration 1956, loss = 0.41080871\n",
      "Iteration 1957, loss = 0.41065751\n",
      "Iteration 1958, loss = 0.41053960\n",
      "Iteration 1959, loss = 0.41042670\n",
      "Iteration 1960, loss = 0.41026106\n",
      "Iteration 1961, loss = 0.41002585\n",
      "Iteration 1962, loss = 0.40988544\n",
      "Iteration 1963, loss = 0.40971477\n",
      "Iteration 1964, loss = 0.40975237\n",
      "Iteration 1965, loss = 0.40951154\n",
      "Iteration 1966, loss = 0.40926012\n",
      "Iteration 1967, loss = 0.40918199\n",
      "Iteration 1968, loss = 0.40888188\n",
      "Iteration 1969, loss = 0.40868991\n",
      "Iteration 1970, loss = 0.40859904\n",
      "Iteration 1971, loss = 0.40848745\n",
      "Iteration 1972, loss = 0.40810929\n",
      "Iteration 1973, loss = 0.40876962\n",
      "Iteration 1974, loss = 0.40812412\n",
      "Iteration 1975, loss = 0.40801785\n",
      "Iteration 1976, loss = 0.40787355\n",
      "Iteration 1977, loss = 0.40783042\n",
      "Iteration 1978, loss = 0.40790352\n",
      "Iteration 1979, loss = 0.40778650\n",
      "Iteration 1980, loss = 0.40740581\n",
      "Iteration 1981, loss = 0.40701935\n",
      "Iteration 1982, loss = 0.40683547\n",
      "Iteration 1983, loss = 0.40669476\n",
      "Iteration 1984, loss = 0.40661699\n",
      "Iteration 1985, loss = 0.40671927\n",
      "Iteration 1986, loss = 0.40646680\n",
      "Iteration 1987, loss = 0.40601020\n",
      "Iteration 1988, loss = 0.40573349\n",
      "Iteration 1989, loss = 0.40554398\n",
      "Iteration 1990, loss = 0.40531535\n",
      "Iteration 1991, loss = 0.40523804\n",
      "Iteration 1992, loss = 0.40511435\n",
      "Iteration 1993, loss = 0.40466343\n",
      "Iteration 1994, loss = 0.40462991\n",
      "Iteration 1995, loss = 0.40476913\n",
      "Iteration 1996, loss = 0.40471298\n",
      "Iteration 1997, loss = 0.40458911\n",
      "Iteration 1998, loss = 0.40451849\n",
      "Iteration 1999, loss = 0.40424198\n",
      "Iteration 2000, loss = 0.40394102\n",
      "Iteration 2001, loss = 0.40348963\n",
      "Iteration 2002, loss = 0.40340752\n",
      "Iteration 2003, loss = 0.40323517\n",
      "Iteration 2004, loss = 0.40314015\n",
      "Iteration 2005, loss = 0.40316698\n",
      "Iteration 2006, loss = 0.40327086\n",
      "Iteration 2007, loss = 0.40312823\n",
      "Iteration 2008, loss = 0.40291259\n",
      "Iteration 2009, loss = 0.40269281\n",
      "Iteration 2010, loss = 0.40234502\n",
      "Iteration 2011, loss = 0.40194838\n",
      "Iteration 2012, loss = 0.40173724\n",
      "Iteration 2013, loss = 0.40158092\n",
      "Iteration 2014, loss = 0.40140311\n",
      "Iteration 2015, loss = 0.40116829\n",
      "Iteration 2016, loss = 0.40094720\n",
      "Iteration 2017, loss = 0.40109681\n",
      "Iteration 2018, loss = 0.40088822\n",
      "Iteration 2019, loss = 0.40104584\n",
      "Iteration 2020, loss = 0.40095117\n",
      "Iteration 2021, loss = 0.40070314\n",
      "Iteration 2022, loss = 0.40044077\n",
      "Iteration 2023, loss = 0.40019101\n",
      "Iteration 2024, loss = 0.39984901\n",
      "Iteration 2025, loss = 0.39954291\n",
      "Iteration 2026, loss = 0.39947786\n",
      "Iteration 2027, loss = 0.39942107\n",
      "Iteration 2028, loss = 0.39887768\n",
      "Iteration 2029, loss = 0.39922454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2030, loss = 0.39920569\n",
      "Iteration 2031, loss = 0.39929337\n",
      "Iteration 2032, loss = 0.39956097\n",
      "Iteration 2033, loss = 0.39962676\n",
      "Iteration 2034, loss = 0.39947412\n",
      "Iteration 2035, loss = 0.39877086\n",
      "Iteration 2036, loss = 0.39830045\n",
      "Iteration 2037, loss = 0.39794165\n",
      "Iteration 2038, loss = 0.39771706\n",
      "Iteration 2039, loss = 0.39734916\n",
      "Iteration 2040, loss = 0.39732706\n",
      "Iteration 2041, loss = 0.39757851\n",
      "Iteration 2042, loss = 0.39789632\n",
      "Iteration 2043, loss = 0.39759995\n",
      "Iteration 2044, loss = 0.39718402\n",
      "Iteration 2045, loss = 0.39679504\n",
      "Iteration 2046, loss = 0.39623432\n",
      "Iteration 2047, loss = 0.39622239\n",
      "Iteration 2048, loss = 0.39582654\n",
      "Iteration 2049, loss = 0.39616036\n",
      "Iteration 2050, loss = 0.39597984\n",
      "Iteration 2051, loss = 0.39545381\n",
      "Iteration 2052, loss = 0.39494357\n",
      "Iteration 2053, loss = 0.39472159\n",
      "Iteration 2054, loss = 0.39555897\n",
      "Iteration 2055, loss = 0.39572818\n",
      "Iteration 2056, loss = 0.39582956\n",
      "Iteration 2057, loss = 0.39563527\n",
      "Iteration 2058, loss = 0.39535087\n",
      "Iteration 2059, loss = 0.39468539\n",
      "Iteration 2060, loss = 0.39428563\n",
      "Iteration 2061, loss = 0.39389806\n",
      "Iteration 2062, loss = 0.39361562\n",
      "Iteration 2063, loss = 0.39345193\n",
      "Iteration 2064, loss = 0.39336600\n",
      "Iteration 2065, loss = 0.39345935\n",
      "Iteration 2066, loss = 0.39344496\n",
      "Iteration 2067, loss = 0.39309388\n",
      "Iteration 2068, loss = 0.39288004\n",
      "Iteration 2069, loss = 0.39263776\n",
      "Iteration 2070, loss = 0.39248799\n",
      "Iteration 2071, loss = 0.39233163\n",
      "Iteration 2072, loss = 0.39204735\n",
      "Iteration 2073, loss = 0.39199408\n",
      "Iteration 2074, loss = 0.39193623\n",
      "Iteration 2075, loss = 0.39197156\n",
      "Iteration 2076, loss = 0.39169940\n",
      "Iteration 2077, loss = 0.39158390\n",
      "Iteration 2078, loss = 0.39124557\n",
      "Iteration 2079, loss = 0.39118578\n",
      "Iteration 2080, loss = 0.39105735\n",
      "Iteration 2081, loss = 0.39082004\n",
      "Iteration 2082, loss = 0.39083979\n",
      "Iteration 2083, loss = 0.39062784\n",
      "Iteration 2084, loss = 0.39026749\n",
      "Iteration 2085, loss = 0.39014837\n",
      "Iteration 2086, loss = 0.39000369\n",
      "Iteration 2087, loss = 0.38992938\n",
      "Iteration 2088, loss = 0.38982078\n",
      "Iteration 2089, loss = 0.38983257\n",
      "Iteration 2090, loss = 0.38968061\n",
      "Iteration 2091, loss = 0.38951335\n",
      "Iteration 2092, loss = 0.38920714\n",
      "Iteration 2093, loss = 0.38887370\n",
      "Iteration 2094, loss = 0.38899993\n",
      "Iteration 2095, loss = 0.38862866\n",
      "Iteration 2096, loss = 0.38850454\n",
      "Iteration 2097, loss = 0.38846125\n",
      "Iteration 2098, loss = 0.38826632\n",
      "Iteration 2099, loss = 0.38807565\n",
      "Iteration 2100, loss = 0.38779627\n",
      "Iteration 2101, loss = 0.38757421\n",
      "Iteration 2102, loss = 0.38742230\n",
      "Iteration 2103, loss = 0.38739548\n",
      "Iteration 2104, loss = 0.38706971\n",
      "Iteration 2105, loss = 0.38715345\n",
      "Iteration 2106, loss = 0.38704982\n",
      "Iteration 2107, loss = 0.38677525\n",
      "Iteration 2108, loss = 0.38643579\n",
      "Iteration 2109, loss = 0.38633938\n",
      "Iteration 2110, loss = 0.38614919\n",
      "Iteration 2111, loss = 0.38619544\n",
      "Iteration 2112, loss = 0.38606303\n",
      "Iteration 2113, loss = 0.38605958\n",
      "Iteration 2114, loss = 0.38564145\n",
      "Iteration 2115, loss = 0.38567970\n",
      "Iteration 2116, loss = 0.38552568\n",
      "Iteration 2117, loss = 0.38533970\n",
      "Iteration 2118, loss = 0.38502523\n",
      "Iteration 2119, loss = 0.38499324\n",
      "Iteration 2120, loss = 0.38456262\n",
      "Iteration 2121, loss = 0.38452137\n",
      "Iteration 2122, loss = 0.38416726\n",
      "Iteration 2123, loss = 0.38401873\n",
      "Iteration 2124, loss = 0.38374987\n",
      "Iteration 2125, loss = 0.38363282\n",
      "Iteration 2126, loss = 0.38355947\n",
      "Iteration 2127, loss = 0.38340949\n",
      "Iteration 2128, loss = 0.38337865\n",
      "Iteration 2129, loss = 0.38317127\n",
      "Iteration 2130, loss = 0.38293254\n",
      "Iteration 2131, loss = 0.38292326\n",
      "Iteration 2132, loss = 0.38267787\n",
      "Iteration 2133, loss = 0.38255968\n",
      "Iteration 2134, loss = 0.38242449\n",
      "Iteration 2135, loss = 0.38215413\n",
      "Iteration 2136, loss = 0.38204355\n",
      "Iteration 2137, loss = 0.38189898\n",
      "Iteration 2138, loss = 0.38183981\n",
      "Iteration 2139, loss = 0.38165036\n",
      "Iteration 2140, loss = 0.38140108\n",
      "Iteration 2141, loss = 0.38129565\n",
      "Iteration 2142, loss = 0.38113435\n",
      "Iteration 2143, loss = 0.38135908\n",
      "Iteration 2144, loss = 0.38117692\n",
      "Iteration 2145, loss = 0.38073875\n",
      "Iteration 2146, loss = 0.38046816\n",
      "Iteration 2147, loss = 0.38043268\n",
      "Iteration 2148, loss = 0.38049223\n",
      "Iteration 2149, loss = 0.38080687\n",
      "Iteration 2150, loss = 0.38084037\n",
      "Iteration 2151, loss = 0.38048997\n",
      "Iteration 2152, loss = 0.38016860\n",
      "Iteration 2153, loss = 0.37958633\n",
      "Iteration 2154, loss = 0.37941957\n",
      "Iteration 2155, loss = 0.37946914\n",
      "Iteration 2156, loss = 0.37939284\n",
      "Iteration 2157, loss = 0.37938463\n",
      "Iteration 2158, loss = 0.37906308\n",
      "Iteration 2159, loss = 0.37883730\n",
      "Iteration 2160, loss = 0.37845512\n",
      "Iteration 2161, loss = 0.37830895\n",
      "Iteration 2162, loss = 0.37801249\n",
      "Iteration 2163, loss = 0.37806626\n",
      "Iteration 2164, loss = 0.37782162\n",
      "Iteration 2165, loss = 0.37768979\n",
      "Iteration 2166, loss = 0.37734580\n",
      "Iteration 2167, loss = 0.37709940\n",
      "Iteration 2168, loss = 0.37737395\n",
      "Iteration 2169, loss = 0.37716434\n",
      "Iteration 2170, loss = 0.37694228\n",
      "Iteration 2171, loss = 0.37665712\n",
      "Iteration 2172, loss = 0.37642091\n",
      "Iteration 2173, loss = 0.37653627\n",
      "Iteration 2174, loss = 0.37637929\n",
      "Iteration 2175, loss = 0.37609586\n",
      "Iteration 2176, loss = 0.37575744\n",
      "Iteration 2177, loss = 0.37568413\n",
      "Iteration 2178, loss = 0.37579662\n",
      "Iteration 2179, loss = 0.37566161\n",
      "Iteration 2180, loss = 0.37552560\n",
      "Iteration 2181, loss = 0.37543175\n",
      "Iteration 2182, loss = 0.37512052\n",
      "Iteration 2183, loss = 0.37488874\n",
      "Iteration 2184, loss = 0.37487670\n",
      "Iteration 2185, loss = 0.37437913\n",
      "Iteration 2186, loss = 0.37423488\n",
      "Iteration 2187, loss = 0.37408341\n",
      "Iteration 2188, loss = 0.37389122\n",
      "Iteration 2189, loss = 0.37367580\n",
      "Iteration 2190, loss = 0.37363272\n",
      "Iteration 2191, loss = 0.37341970\n",
      "Iteration 2192, loss = 0.37329324\n",
      "Iteration 2193, loss = 0.37306798\n",
      "Iteration 2194, loss = 0.37297432\n",
      "Iteration 2195, loss = 0.37268760\n",
      "Iteration 2196, loss = 0.37262422\n",
      "Iteration 2197, loss = 0.37245860\n",
      "Iteration 2198, loss = 0.37238502\n",
      "Iteration 2199, loss = 0.37255016\n",
      "Iteration 2200, loss = 0.37240493\n",
      "Iteration 2201, loss = 0.37212482\n",
      "Iteration 2202, loss = 0.37200298\n",
      "Iteration 2203, loss = 0.37206919\n",
      "Iteration 2204, loss = 0.37171561\n",
      "Iteration 2205, loss = 0.37152868\n",
      "Iteration 2206, loss = 0.37142982\n",
      "Iteration 2207, loss = 0.37124691\n",
      "Iteration 2208, loss = 0.37117181\n",
      "Iteration 2209, loss = 0.37094475\n",
      "Iteration 2210, loss = 0.37103049\n",
      "Iteration 2211, loss = 0.37060261\n",
      "Iteration 2212, loss = 0.37052539\n",
      "Iteration 2213, loss = 0.37031267\n",
      "Iteration 2214, loss = 0.37001520\n",
      "Iteration 2215, loss = 0.36989462\n",
      "Iteration 2216, loss = 0.36967450\n",
      "Iteration 2217, loss = 0.36953825\n",
      "Iteration 2218, loss = 0.36925266\n",
      "Iteration 2219, loss = 0.36910286\n",
      "Iteration 2220, loss = 0.36908841\n",
      "Iteration 2221, loss = 0.36913024\n",
      "Iteration 2222, loss = 0.36903077\n",
      "Iteration 2223, loss = 0.36883408\n",
      "Iteration 2224, loss = 0.36847642\n",
      "Iteration 2225, loss = 0.36827528\n",
      "Iteration 2226, loss = 0.36803982\n",
      "Iteration 2227, loss = 0.36846842\n",
      "Iteration 2228, loss = 0.36833237\n",
      "Iteration 2229, loss = 0.36826819\n",
      "Iteration 2230, loss = 0.36812246\n",
      "Iteration 2231, loss = 0.36768092\n",
      "Iteration 2232, loss = 0.36734502\n",
      "Iteration 2233, loss = 0.36733180\n",
      "Iteration 2234, loss = 0.36708355\n",
      "Iteration 2235, loss = 0.36698107\n",
      "Iteration 2236, loss = 0.36668016\n",
      "Iteration 2237, loss = 0.36667631\n",
      "Iteration 2238, loss = 0.36627470\n",
      "Iteration 2239, loss = 0.36610011\n",
      "Iteration 2240, loss = 0.36596134\n",
      "Iteration 2241, loss = 0.36591958\n",
      "Iteration 2242, loss = 0.36575802\n",
      "Iteration 2243, loss = 0.36566432\n",
      "Iteration 2244, loss = 0.36543031\n",
      "Iteration 2245, loss = 0.36523636\n",
      "Iteration 2246, loss = 0.36519349\n",
      "Iteration 2247, loss = 0.36506426\n",
      "Iteration 2248, loss = 0.36503125\n",
      "Iteration 2249, loss = 0.36489968\n",
      "Iteration 2250, loss = 0.36452351\n",
      "Iteration 2251, loss = 0.36440129\n",
      "Iteration 2252, loss = 0.36430101\n",
      "Iteration 2253, loss = 0.36447671\n",
      "Iteration 2254, loss = 0.36450401\n",
      "Iteration 2255, loss = 0.36435673\n",
      "Iteration 2256, loss = 0.36431456\n",
      "Iteration 2257, loss = 0.36364658\n",
      "Iteration 2258, loss = 0.36344970\n",
      "Iteration 2259, loss = 0.36326973\n",
      "Iteration 2260, loss = 0.36326140\n",
      "Iteration 2261, loss = 0.36353595\n",
      "Iteration 2262, loss = 0.36336279\n",
      "Iteration 2263, loss = 0.36297795\n",
      "Iteration 2264, loss = 0.36255664\n",
      "Iteration 2265, loss = 0.36226659\n",
      "Iteration 2266, loss = 0.36219753\n",
      "Iteration 2267, loss = 0.36206791\n",
      "Iteration 2268, loss = 0.36205445\n",
      "Iteration 2269, loss = 0.36194045\n",
      "Iteration 2270, loss = 0.36189114\n",
      "Iteration 2271, loss = 0.36189425\n",
      "Iteration 2272, loss = 0.36163081\n",
      "Iteration 2273, loss = 0.36145038\n",
      "Iteration 2274, loss = 0.36127874\n",
      "Iteration 2275, loss = 0.36105403\n",
      "Iteration 2276, loss = 0.36096055\n",
      "Iteration 2277, loss = 0.36049629\n",
      "Iteration 2278, loss = 0.36027858\n",
      "Iteration 2279, loss = 0.36081119\n",
      "Iteration 2280, loss = 0.36093873\n",
      "Iteration 2281, loss = 0.36093170\n",
      "Iteration 2282, loss = 0.36090676\n",
      "Iteration 2283, loss = 0.36069563\n",
      "Iteration 2284, loss = 0.36041707\n",
      "Iteration 2285, loss = 0.36024895\n",
      "Iteration 2286, loss = 0.35971837\n",
      "Iteration 2287, loss = 0.35969947\n",
      "Iteration 2288, loss = 0.35923443\n",
      "Iteration 2289, loss = 0.35899757\n",
      "Iteration 2290, loss = 0.35896671\n",
      "Iteration 2291, loss = 0.35861429\n",
      "Iteration 2292, loss = 0.35843187\n",
      "Iteration 2293, loss = 0.35848798\n",
      "Iteration 2294, loss = 0.35817981\n",
      "Iteration 2295, loss = 0.35804838\n",
      "Iteration 2296, loss = 0.35836349\n",
      "Iteration 2297, loss = 0.35880179\n",
      "Iteration 2298, loss = 0.35861589\n",
      "Iteration 2299, loss = 0.35833238\n",
      "Iteration 2300, loss = 0.35771214\n",
      "Iteration 2301, loss = 0.35751044\n",
      "Iteration 2302, loss = 0.35746021\n",
      "Iteration 2303, loss = 0.35712291\n",
      "Iteration 2304, loss = 0.35693887\n",
      "Iteration 2305, loss = 0.35669442\n",
      "Iteration 2306, loss = 0.35641629\n",
      "Iteration 2307, loss = 0.35626300\n",
      "Iteration 2308, loss = 0.35613075\n",
      "Iteration 2309, loss = 0.35588563\n",
      "Iteration 2310, loss = 0.35582635\n",
      "Iteration 2311, loss = 0.35560683\n",
      "Iteration 2312, loss = 0.35582532\n",
      "Iteration 2313, loss = 0.35586286\n",
      "Iteration 2314, loss = 0.35557774\n",
      "Iteration 2315, loss = 0.35505179\n",
      "Iteration 2316, loss = 0.35501683\n",
      "Iteration 2317, loss = 0.35508374\n",
      "Iteration 2318, loss = 0.35471148\n",
      "Iteration 2319, loss = 0.35453100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2320, loss = 0.35459834\n",
      "Iteration 2321, loss = 0.35416989\n",
      "Iteration 2322, loss = 0.35430801\n",
      "Iteration 2323, loss = 0.35396316\n",
      "Iteration 2324, loss = 0.35384906\n",
      "Iteration 2325, loss = 0.35398216\n",
      "Iteration 2326, loss = 0.35372647\n",
      "Iteration 2327, loss = 0.35374200\n",
      "Iteration 2328, loss = 0.35340649\n",
      "Iteration 2329, loss = 0.35317065\n",
      "Iteration 2330, loss = 0.35305251\n",
      "Iteration 2331, loss = 0.35276375\n",
      "Iteration 2332, loss = 0.35251923\n",
      "Iteration 2333, loss = 0.35241094\n",
      "Iteration 2334, loss = 0.35237948\n",
      "Iteration 2335, loss = 0.35224438\n",
      "Iteration 2336, loss = 0.35210747\n",
      "Iteration 2337, loss = 0.35202735\n",
      "Iteration 2338, loss = 0.35204711\n",
      "Iteration 2339, loss = 0.35216225\n",
      "Iteration 2340, loss = 0.35189776\n",
      "Iteration 2341, loss = 0.35138839\n",
      "Iteration 2342, loss = 0.35103805\n",
      "Iteration 2343, loss = 0.35167554\n",
      "Iteration 2344, loss = 0.35187495\n",
      "Iteration 2345, loss = 0.35178013\n",
      "Iteration 2346, loss = 0.35166391\n",
      "Iteration 2347, loss = 0.35092978\n",
      "Iteration 2348, loss = 0.35052452\n",
      "Iteration 2349, loss = 0.35009102\n",
      "Iteration 2350, loss = 0.34997074\n",
      "Iteration 2351, loss = 0.35020338\n",
      "Iteration 2352, loss = 0.35016977\n",
      "Iteration 2353, loss = 0.35011148\n",
      "Iteration 2354, loss = 0.34997864\n",
      "Iteration 2355, loss = 0.35002520\n",
      "Iteration 2356, loss = 0.34934923\n",
      "Iteration 2357, loss = 0.34909907\n",
      "Iteration 2358, loss = 0.34885092\n",
      "Iteration 2359, loss = 0.34863974\n",
      "Iteration 2360, loss = 0.34861708\n",
      "Iteration 2361, loss = 0.34831985\n",
      "Iteration 2362, loss = 0.34822861\n",
      "Iteration 2363, loss = 0.34801194\n",
      "Iteration 2364, loss = 0.34779854\n",
      "Iteration 2365, loss = 0.34776919\n",
      "Iteration 2366, loss = 0.34799926\n",
      "Iteration 2367, loss = 0.34771864\n",
      "Iteration 2368, loss = 0.34766242\n",
      "Iteration 2369, loss = 0.34743717\n",
      "Iteration 2370, loss = 0.34725162\n",
      "Iteration 2371, loss = 0.34705579\n",
      "Iteration 2372, loss = 0.34692745\n",
      "Iteration 2373, loss = 0.34657976\n",
      "Iteration 2374, loss = 0.34649407\n",
      "Iteration 2375, loss = 0.34633963\n",
      "Iteration 2376, loss = 0.34661157\n",
      "Iteration 2377, loss = 0.34631478\n",
      "Iteration 2378, loss = 0.34619989\n",
      "Iteration 2379, loss = 0.34579379\n",
      "Iteration 2380, loss = 0.34567293\n",
      "Iteration 2381, loss = 0.34573367\n",
      "Iteration 2382, loss = 0.34567968\n",
      "Iteration 2383, loss = 0.34553044\n",
      "Iteration 2384, loss = 0.34520541\n",
      "Iteration 2385, loss = 0.34511530\n",
      "Iteration 2386, loss = 0.34538947\n",
      "Iteration 2387, loss = 0.34501551\n",
      "Iteration 2388, loss = 0.34484322\n",
      "Iteration 2389, loss = 0.34483397\n",
      "Iteration 2390, loss = 0.34418150\n",
      "Iteration 2391, loss = 0.34448477\n",
      "Iteration 2392, loss = 0.34442124\n",
      "Iteration 2393, loss = 0.34420459\n",
      "Iteration 2394, loss = 0.34375343\n",
      "Iteration 2395, loss = 0.34328142\n",
      "Iteration 2396, loss = 0.34335992\n",
      "Iteration 2397, loss = 0.34378789\n",
      "Iteration 2398, loss = 0.34423626\n",
      "Iteration 2399, loss = 0.34403269\n",
      "Iteration 2400, loss = 0.34341716\n",
      "Iteration 2401, loss = 0.34277069\n",
      "Iteration 2402, loss = 0.34262412\n",
      "Iteration 2403, loss = 0.34234169\n",
      "Iteration 2404, loss = 0.34234118\n",
      "Iteration 2405, loss = 0.34234490\n",
      "Iteration 2406, loss = 0.34224063\n",
      "Iteration 2407, loss = 0.34208155\n",
      "Iteration 2408, loss = 0.34196596\n",
      "Iteration 2409, loss = 0.34164229\n",
      "Iteration 2410, loss = 0.34210592\n",
      "Iteration 2411, loss = 0.34152623\n",
      "Iteration 2412, loss = 0.34148640\n",
      "Iteration 2413, loss = 0.34143281\n",
      "Iteration 2414, loss = 0.34133809\n",
      "Iteration 2415, loss = 0.34101562\n",
      "Iteration 2416, loss = 0.34082257\n",
      "Iteration 2417, loss = 0.34066777\n",
      "Iteration 2418, loss = 0.34067667\n",
      "Iteration 2419, loss = 0.34050255\n",
      "Iteration 2420, loss = 0.34052213\n",
      "Iteration 2421, loss = 0.34037204\n",
      "Iteration 2422, loss = 0.34013109\n",
      "Iteration 2423, loss = 0.33985445\n",
      "Iteration 2424, loss = 0.33957008\n",
      "Iteration 2425, loss = 0.33940272\n",
      "Iteration 2426, loss = 0.33948108\n",
      "Iteration 2427, loss = 0.33912848\n",
      "Iteration 2428, loss = 0.33907535\n",
      "Iteration 2429, loss = 0.33881130\n",
      "Iteration 2430, loss = 0.33868242\n",
      "Iteration 2431, loss = 0.33861294\n",
      "Iteration 2432, loss = 0.33830158\n",
      "Iteration 2433, loss = 0.33848637\n",
      "Iteration 2434, loss = 0.33831246\n",
      "Iteration 2435, loss = 0.33809973\n",
      "Iteration 2436, loss = 0.33789566\n",
      "Iteration 2437, loss = 0.33779501\n",
      "Iteration 2438, loss = 0.33796178\n",
      "Iteration 2439, loss = 0.33810841\n",
      "Iteration 2440, loss = 0.33769005\n",
      "Iteration 2441, loss = 0.33704002\n",
      "Iteration 2442, loss = 0.33729260\n",
      "Iteration 2443, loss = 0.33674142\n",
      "Iteration 2444, loss = 0.33665015\n",
      "Iteration 2445, loss = 0.33660086\n",
      "Iteration 2446, loss = 0.33667598\n",
      "Iteration 2447, loss = 0.33645027\n",
      "Iteration 2448, loss = 0.33617225\n",
      "Iteration 2449, loss = 0.33587554\n",
      "Iteration 2450, loss = 0.33575672\n",
      "Iteration 2451, loss = 0.33551173\n",
      "Iteration 2452, loss = 0.33550364\n",
      "Iteration 2453, loss = 0.33517402\n",
      "Iteration 2454, loss = 0.33516867\n",
      "Iteration 2455, loss = 0.33513275\n",
      "Iteration 2456, loss = 0.33504406\n",
      "Iteration 2457, loss = 0.33485357\n",
      "Iteration 2458, loss = 0.33497049\n",
      "Iteration 2459, loss = 0.33540031\n",
      "Iteration 2460, loss = 0.33518453\n",
      "Iteration 2461, loss = 0.33495504\n",
      "Iteration 2462, loss = 0.33468012\n",
      "Iteration 2463, loss = 0.33435116\n",
      "Iteration 2464, loss = 0.33425855\n",
      "Iteration 2465, loss = 0.33376833\n",
      "Iteration 2466, loss = 0.33351760\n",
      "Iteration 2467, loss = 0.33375219\n",
      "Iteration 2468, loss = 0.33380896\n",
      "Iteration 2469, loss = 0.33340716\n",
      "Iteration 2470, loss = 0.33301826\n",
      "Iteration 2471, loss = 0.33305293\n",
      "Iteration 2472, loss = 0.33267022\n",
      "Iteration 2473, loss = 0.33241221\n",
      "Iteration 2474, loss = 0.33243338\n",
      "Iteration 2475, loss = 0.33252553\n",
      "Iteration 2476, loss = 0.33300396\n",
      "Iteration 2477, loss = 0.33272126\n",
      "Iteration 2478, loss = 0.33229604\n",
      "Iteration 2479, loss = 0.33173611\n",
      "Iteration 2480, loss = 0.33135784\n",
      "Iteration 2481, loss = 0.33117452\n",
      "Iteration 2482, loss = 0.33120147\n",
      "Iteration 2483, loss = 0.33107054\n",
      "Iteration 2484, loss = 0.33103555\n",
      "Iteration 2485, loss = 0.33123871\n",
      "Iteration 2486, loss = 0.33116868\n",
      "Iteration 2487, loss = 0.33086083\n",
      "Iteration 2488, loss = 0.33058250\n",
      "Iteration 2489, loss = 0.33022294\n",
      "Iteration 2490, loss = 0.33018203\n",
      "Iteration 2491, loss = 0.33046564\n",
      "Iteration 2492, loss = 0.33063089\n",
      "Iteration 2493, loss = 0.33072067\n",
      "Iteration 2494, loss = 0.33030909\n",
      "Iteration 2495, loss = 0.32987150\n",
      "Iteration 2496, loss = 0.32944686\n",
      "Iteration 2497, loss = 0.32904358\n",
      "Iteration 2498, loss = 0.32884482\n",
      "Iteration 2499, loss = 0.32897397\n",
      "Iteration 2500, loss = 0.32863905\n",
      "Iteration 2501, loss = 0.32861164\n",
      "Iteration 2502, loss = 0.32830979\n",
      "Iteration 2503, loss = 0.32822801\n",
      "Iteration 2504, loss = 0.32823360\n",
      "Iteration 2505, loss = 0.32823242\n",
      "Iteration 2506, loss = 0.32816731\n",
      "Iteration 2507, loss = 0.32835697\n",
      "Iteration 2508, loss = 0.32856273\n",
      "Iteration 2509, loss = 0.32875246\n",
      "Iteration 2510, loss = 0.32836251\n",
      "Iteration 2511, loss = 0.32759880\n",
      "Iteration 2512, loss = 0.32720827\n",
      "Iteration 2513, loss = 0.32705776\n",
      "Iteration 2514, loss = 0.32704801\n",
      "Iteration 2515, loss = 0.32727452\n",
      "Iteration 2516, loss = 0.32728240\n",
      "Iteration 2517, loss = 0.32723175\n",
      "Iteration 2518, loss = 0.32708258\n",
      "Iteration 2519, loss = 0.32673240\n",
      "Iteration 2520, loss = 0.32620705\n",
      "Iteration 2521, loss = 0.32599414\n",
      "Iteration 2522, loss = 0.32576483\n",
      "Iteration 2523, loss = 0.32570846\n",
      "Iteration 2524, loss = 0.32575286\n",
      "Iteration 2525, loss = 0.32553568\n",
      "Iteration 2526, loss = 0.32547456\n",
      "Iteration 2527, loss = 0.32502008\n",
      "Iteration 2528, loss = 0.32505890\n",
      "Iteration 2529, loss = 0.32459613\n",
      "Iteration 2530, loss = 0.32466080\n",
      "Iteration 2531, loss = 0.32467561\n",
      "Iteration 2532, loss = 0.32457814\n",
      "Iteration 2533, loss = 0.32429832\n",
      "Iteration 2534, loss = 0.32412518\n",
      "Iteration 2535, loss = 0.32376060\n",
      "Iteration 2536, loss = 0.32379685\n",
      "Iteration 2537, loss = 0.32352165\n",
      "Iteration 2538, loss = 0.32355367\n",
      "Iteration 2539, loss = 0.32351055\n",
      "Iteration 2540, loss = 0.32310894\n",
      "Iteration 2541, loss = 0.32311938\n",
      "Iteration 2542, loss = 0.32292472\n",
      "Iteration 2543, loss = 0.32279488\n",
      "Iteration 2544, loss = 0.32286614\n",
      "Iteration 2545, loss = 0.32270896\n",
      "Iteration 2546, loss = 0.32276306\n",
      "Iteration 2547, loss = 0.32210176\n",
      "Iteration 2548, loss = 0.32218715\n",
      "Iteration 2549, loss = 0.32212673\n",
      "Iteration 2550, loss = 0.32205278\n",
      "Iteration 2551, loss = 0.32182901\n",
      "Iteration 2552, loss = 0.32167022\n",
      "Iteration 2553, loss = 0.32160233\n",
      "Iteration 2554, loss = 0.32146314\n",
      "Iteration 2555, loss = 0.32130647\n",
      "Iteration 2556, loss = 0.32124289\n",
      "Iteration 2557, loss = 0.32091055\n",
      "Iteration 2558, loss = 0.32081635\n",
      "Iteration 2559, loss = 0.32081035\n",
      "Iteration 2560, loss = 0.32047891\n",
      "Iteration 2561, loss = 0.32028452\n",
      "Iteration 2562, loss = 0.32021599\n",
      "Iteration 2563, loss = 0.31998109\n",
      "Iteration 2564, loss = 0.32027003\n",
      "Iteration 2565, loss = 0.32042529\n",
      "Iteration 2566, loss = 0.32032794\n",
      "Iteration 2567, loss = 0.32019740\n",
      "Iteration 2568, loss = 0.31973900\n",
      "Iteration 2569, loss = 0.31944237\n",
      "Iteration 2570, loss = 0.31924783\n",
      "Iteration 2571, loss = 0.31896469\n",
      "Iteration 2572, loss = 0.31904527\n",
      "Iteration 2573, loss = 0.31881484\n",
      "Iteration 2574, loss = 0.31863206\n",
      "Iteration 2575, loss = 0.31851289\n",
      "Iteration 2576, loss = 0.31819541\n",
      "Iteration 2577, loss = 0.31812235\n",
      "Iteration 2578, loss = 0.31822251\n",
      "Iteration 2579, loss = 0.31800342\n",
      "Iteration 2580, loss = 0.31790833\n",
      "Iteration 2581, loss = 0.31767143\n",
      "Iteration 2582, loss = 0.31758206\n",
      "Iteration 2583, loss = 0.31741383\n",
      "Iteration 2584, loss = 0.31733580\n",
      "Iteration 2585, loss = 0.31724284\n",
      "Iteration 2586, loss = 0.31725887\n",
      "Iteration 2587, loss = 0.31729288\n",
      "Iteration 2588, loss = 0.31767511\n",
      "Iteration 2589, loss = 0.31762412\n",
      "Iteration 2590, loss = 0.31711530\n",
      "Iteration 2591, loss = 0.31667836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2592, loss = 0.31637243\n",
      "Iteration 2593, loss = 0.31584240\n",
      "Iteration 2594, loss = 0.31603282\n",
      "Iteration 2595, loss = 0.31601659\n",
      "Iteration 2596, loss = 0.31586564\n",
      "Iteration 2597, loss = 0.31579535\n",
      "Iteration 2598, loss = 0.31561567\n",
      "Iteration 2599, loss = 0.31521057\n",
      "Iteration 2600, loss = 0.31507088\n",
      "Iteration 2601, loss = 0.31504688\n",
      "Iteration 2602, loss = 0.31483187\n",
      "Iteration 2603, loss = 0.31469989\n",
      "Iteration 2604, loss = 0.31484497\n",
      "Iteration 2605, loss = 0.31508175\n",
      "Iteration 2606, loss = 0.31487920\n",
      "Iteration 2607, loss = 0.31453742\n",
      "Iteration 2608, loss = 0.31414976\n",
      "Iteration 2609, loss = 0.31399650\n",
      "Iteration 2610, loss = 0.31371857\n",
      "Iteration 2611, loss = 0.31350833\n",
      "Iteration 2612, loss = 0.31339995\n",
      "Iteration 2613, loss = 0.31322239\n",
      "Iteration 2614, loss = 0.31314482\n",
      "Iteration 2615, loss = 0.31307399\n",
      "Iteration 2616, loss = 0.31314621\n",
      "Iteration 2617, loss = 0.31285614\n",
      "Iteration 2618, loss = 0.31268689\n",
      "Iteration 2619, loss = 0.31251908\n",
      "Iteration 2620, loss = 0.31233025\n",
      "Iteration 2621, loss = 0.31211527\n",
      "Iteration 2622, loss = 0.31198084\n",
      "Iteration 2623, loss = 0.31192781\n",
      "Iteration 2624, loss = 0.31179321\n",
      "Iteration 2625, loss = 0.31154400\n",
      "Iteration 2626, loss = 0.31164965\n",
      "Iteration 2627, loss = 0.31140769\n",
      "Iteration 2628, loss = 0.31130777\n",
      "Iteration 2629, loss = 0.31112091\n",
      "Iteration 2630, loss = 0.31113103\n",
      "Iteration 2631, loss = 0.31091961\n",
      "Iteration 2632, loss = 0.31068021\n",
      "Iteration 2633, loss = 0.31069186\n",
      "Iteration 2634, loss = 0.31048900\n",
      "Iteration 2635, loss = 0.31031746\n",
      "Iteration 2636, loss = 0.31019085\n",
      "Iteration 2637, loss = 0.31010121\n",
      "Iteration 2638, loss = 0.30989465\n",
      "Iteration 2639, loss = 0.30973374\n",
      "Iteration 2640, loss = 0.30946915\n",
      "Iteration 2641, loss = 0.30949805\n",
      "Iteration 2642, loss = 0.30975047\n",
      "Iteration 2643, loss = 0.30977771\n",
      "Iteration 2644, loss = 0.30969862\n",
      "Iteration 2645, loss = 0.30897717\n",
      "Iteration 2646, loss = 0.30879559\n",
      "Iteration 2647, loss = 0.30894233\n",
      "Iteration 2648, loss = 0.30851963\n",
      "Iteration 2649, loss = 0.30835536\n",
      "Iteration 2650, loss = 0.30836114\n",
      "Iteration 2651, loss = 0.30867003\n",
      "Iteration 2652, loss = 0.30880168\n",
      "Iteration 2653, loss = 0.30866583\n",
      "Iteration 2654, loss = 0.30834473\n",
      "Iteration 2655, loss = 0.30787868\n",
      "Iteration 2656, loss = 0.30749658\n",
      "Iteration 2657, loss = 0.30739232\n",
      "Iteration 2658, loss = 0.30740887\n",
      "Iteration 2659, loss = 0.30749114\n",
      "Iteration 2660, loss = 0.30760950\n",
      "Iteration 2661, loss = 0.30737187\n",
      "Iteration 2662, loss = 0.30723908\n",
      "Iteration 2663, loss = 0.30695806\n",
      "Iteration 2664, loss = 0.30668155\n",
      "Iteration 2665, loss = 0.30655433\n",
      "Iteration 2666, loss = 0.30628693\n",
      "Iteration 2667, loss = 0.30610227\n",
      "Iteration 2668, loss = 0.30603059\n",
      "Iteration 2669, loss = 0.30579317\n",
      "Iteration 2670, loss = 0.30584358\n",
      "Iteration 2671, loss = 0.30578968\n",
      "Iteration 2672, loss = 0.30551164\n",
      "Iteration 2673, loss = 0.30539308\n",
      "Iteration 2674, loss = 0.30512250\n",
      "Iteration 2675, loss = 0.30500370\n",
      "Iteration 2676, loss = 0.30507261\n",
      "Iteration 2677, loss = 0.30492525\n",
      "Iteration 2678, loss = 0.30469402\n",
      "Iteration 2679, loss = 0.30454255\n",
      "Iteration 2680, loss = 0.30450036\n",
      "Iteration 2681, loss = 0.30424144\n",
      "Iteration 2682, loss = 0.30405768\n",
      "Iteration 2683, loss = 0.30395478\n",
      "Iteration 2684, loss = 0.30381735\n",
      "Iteration 2685, loss = 0.30365293\n",
      "Iteration 2686, loss = 0.30353833\n",
      "Iteration 2687, loss = 0.30336362\n",
      "Iteration 2688, loss = 0.30326873\n",
      "Iteration 2689, loss = 0.30320116\n",
      "Iteration 2690, loss = 0.30304969\n",
      "Iteration 2691, loss = 0.30309526\n",
      "Iteration 2692, loss = 0.30299108\n",
      "Iteration 2693, loss = 0.30274453\n",
      "Iteration 2694, loss = 0.30259169\n",
      "Iteration 2695, loss = 0.30235599\n",
      "Iteration 2696, loss = 0.30306673\n",
      "Iteration 2697, loss = 0.30260381\n",
      "Iteration 2698, loss = 0.30216889\n",
      "Iteration 2699, loss = 0.30170625\n",
      "Iteration 2700, loss = 0.30161228\n",
      "Iteration 2701, loss = 0.30263653\n",
      "Iteration 2702, loss = 0.30221636\n",
      "Iteration 2703, loss = 0.30203601\n",
      "Iteration 2704, loss = 0.30178008\n",
      "Iteration 2705, loss = 0.30158557\n",
      "Iteration 2706, loss = 0.30143236\n",
      "Iteration 2707, loss = 0.30117006\n",
      "Iteration 2708, loss = 0.30074245\n",
      "Iteration 2709, loss = 0.30051250\n",
      "Iteration 2710, loss = 0.30039643\n",
      "Iteration 2711, loss = 0.30060417\n",
      "Iteration 2712, loss = 0.30023610\n",
      "Iteration 2713, loss = 0.30002192\n",
      "Iteration 2714, loss = 0.29985053\n",
      "Iteration 2715, loss = 0.29991453\n",
      "Iteration 2716, loss = 0.30044020\n",
      "Iteration 2717, loss = 0.30072784\n",
      "Iteration 2718, loss = 0.30059331\n",
      "Iteration 2719, loss = 0.29992666\n",
      "Iteration 2720, loss = 0.29982414\n",
      "Iteration 2721, loss = 0.29949363\n",
      "Iteration 2722, loss = 0.29939444\n",
      "Iteration 2723, loss = 0.29901142\n",
      "Iteration 2724, loss = 0.29879547\n",
      "Iteration 2725, loss = 0.29860890\n",
      "Iteration 2726, loss = 0.29856754\n",
      "Iteration 2727, loss = 0.29837931\n",
      "Iteration 2728, loss = 0.29829336\n",
      "Iteration 2729, loss = 0.29863515\n",
      "Iteration 2730, loss = 0.29816877\n",
      "Iteration 2731, loss = 0.29786736\n",
      "Iteration 2732, loss = 0.29767226\n",
      "Iteration 2733, loss = 0.29754545\n",
      "Iteration 2734, loss = 0.29747611\n",
      "Iteration 2735, loss = 0.29738253\n",
      "Iteration 2736, loss = 0.29720902\n",
      "Iteration 2737, loss = 0.29693428\n",
      "Iteration 2738, loss = 0.29671838\n",
      "Iteration 2739, loss = 0.29643092\n",
      "Iteration 2740, loss = 0.29657069\n",
      "Iteration 2741, loss = 0.29767759\n",
      "Iteration 2742, loss = 0.29760304\n",
      "Iteration 2743, loss = 0.29711361\n",
      "Iteration 2744, loss = 0.29671935\n",
      "Iteration 2745, loss = 0.29616572\n",
      "Iteration 2746, loss = 0.29564041\n",
      "Iteration 2747, loss = 0.29560878\n",
      "Iteration 2748, loss = 0.29584275\n",
      "Iteration 2749, loss = 0.29589122\n",
      "Iteration 2750, loss = 0.29559088\n",
      "Iteration 2751, loss = 0.29516071\n",
      "Iteration 2752, loss = 0.29492661\n",
      "Iteration 2753, loss = 0.29529418\n",
      "Iteration 2754, loss = 0.29525148\n",
      "Iteration 2755, loss = 0.29509435\n",
      "Iteration 2756, loss = 0.29473244\n",
      "Iteration 2757, loss = 0.29452511\n",
      "Iteration 2758, loss = 0.29421182\n",
      "Iteration 2759, loss = 0.29427950\n",
      "Iteration 2760, loss = 0.29436355\n",
      "Iteration 2761, loss = 0.29433142\n",
      "Iteration 2762, loss = 0.29436268\n",
      "Iteration 2763, loss = 0.29370887\n",
      "Iteration 2764, loss = 0.29356465\n",
      "Iteration 2765, loss = 0.29350950\n",
      "Iteration 2766, loss = 0.29327892\n",
      "Iteration 2767, loss = 0.29317317\n",
      "Iteration 2768, loss = 0.29310978\n",
      "Iteration 2769, loss = 0.29289871\n",
      "Iteration 2770, loss = 0.29273647\n",
      "Iteration 2771, loss = 0.29262466\n",
      "Iteration 2772, loss = 0.29241619\n",
      "Iteration 2773, loss = 0.29233055\n",
      "Iteration 2774, loss = 0.29214174\n",
      "Iteration 2775, loss = 0.29229360\n",
      "Iteration 2776, loss = 0.29197025\n",
      "Iteration 2777, loss = 0.29178988\n",
      "Iteration 2778, loss = 0.29172447\n",
      "Iteration 2779, loss = 0.29154763\n",
      "Iteration 2780, loss = 0.29149348\n",
      "Iteration 2781, loss = 0.29169453\n",
      "Iteration 2782, loss = 0.29164272\n",
      "Iteration 2783, loss = 0.29157184\n",
      "Iteration 2784, loss = 0.29105759\n",
      "Iteration 2785, loss = 0.29061245\n",
      "Iteration 2786, loss = 0.29082919\n",
      "Iteration 2787, loss = 0.29127778\n",
      "Iteration 2788, loss = 0.29184592\n",
      "Iteration 2789, loss = 0.29167278\n",
      "Iteration 2790, loss = 0.29110478\n",
      "Iteration 2791, loss = 0.29057483\n",
      "Iteration 2792, loss = 0.29008685\n",
      "Iteration 2793, loss = 0.28983777\n",
      "Iteration 2794, loss = 0.28978182\n",
      "Iteration 2795, loss = 0.28984802\n",
      "Iteration 2796, loss = 0.28970793\n",
      "Iteration 2797, loss = 0.28949966\n",
      "Iteration 2798, loss = 0.28921581\n",
      "Iteration 2799, loss = 0.28910913\n",
      "Iteration 2800, loss = 0.28939112\n",
      "Iteration 2801, loss = 0.28926545\n",
      "Iteration 2802, loss = 0.28901690\n",
      "Iteration 2803, loss = 0.28885205\n",
      "Iteration 2804, loss = 0.28845093\n",
      "Iteration 2805, loss = 0.28826991\n",
      "Iteration 2806, loss = 0.28831357\n",
      "Iteration 2807, loss = 0.28820826\n",
      "Iteration 2808, loss = 0.28803571\n",
      "Iteration 2809, loss = 0.28779843\n",
      "Iteration 2810, loss = 0.28797122\n",
      "Iteration 2811, loss = 0.28767145\n",
      "Iteration 2812, loss = 0.28742715\n",
      "Iteration 2813, loss = 0.28718230\n",
      "Iteration 2814, loss = 0.28717151\n",
      "Iteration 2815, loss = 0.28724020\n",
      "Iteration 2816, loss = 0.28701913\n",
      "Iteration 2817, loss = 0.28680104\n",
      "Iteration 2818, loss = 0.28671941\n",
      "Iteration 2819, loss = 0.28675382\n",
      "Iteration 2820, loss = 0.28675732\n",
      "Iteration 2821, loss = 0.28658996\n",
      "Iteration 2822, loss = 0.28623251\n",
      "Iteration 2823, loss = 0.28611422\n",
      "Iteration 2824, loss = 0.28598468\n",
      "Iteration 2825, loss = 0.28606397\n",
      "Iteration 2826, loss = 0.28596510\n",
      "Iteration 2827, loss = 0.28606000\n",
      "Iteration 2828, loss = 0.28594116\n",
      "Iteration 2829, loss = 0.28563885\n",
      "Iteration 2830, loss = 0.28532788\n",
      "Iteration 2831, loss = 0.28503603\n",
      "Iteration 2832, loss = 0.28519509\n",
      "Iteration 2833, loss = 0.28499505\n",
      "Iteration 2834, loss = 0.28474348\n",
      "Iteration 2835, loss = 0.28471302\n",
      "Iteration 2836, loss = 0.28451831\n",
      "Iteration 2837, loss = 0.28444578\n",
      "Iteration 2838, loss = 0.28426576\n",
      "Iteration 2839, loss = 0.28423040\n",
      "Iteration 2840, loss = 0.28414229\n",
      "Iteration 2841, loss = 0.28396587\n",
      "Iteration 2842, loss = 0.28371915\n",
      "Iteration 2843, loss = 0.28352113\n",
      "Iteration 2844, loss = 0.28353723\n",
      "Iteration 2845, loss = 0.28363468\n",
      "Iteration 2846, loss = 0.28348442\n",
      "Iteration 2847, loss = 0.28327089\n",
      "Iteration 2848, loss = 0.28312973\n",
      "Iteration 2849, loss = 0.28311117\n",
      "Iteration 2850, loss = 0.28304175\n",
      "Iteration 2851, loss = 0.28299874\n",
      "Iteration 2852, loss = 0.28310448\n",
      "Iteration 2853, loss = 0.28280091\n",
      "Iteration 2854, loss = 0.28253687\n",
      "Iteration 2855, loss = 0.28225005\n",
      "Iteration 2856, loss = 0.28199449\n",
      "Iteration 2857, loss = 0.28214553\n",
      "Iteration 2858, loss = 0.28221944\n",
      "Iteration 2859, loss = 0.28200046\n",
      "Iteration 2860, loss = 0.28188330\n",
      "Iteration 2861, loss = 0.28174106\n",
      "Iteration 2862, loss = 0.28153854\n",
      "Iteration 2863, loss = 0.28135497\n",
      "Iteration 2864, loss = 0.28103102\n",
      "Iteration 2865, loss = 0.28111911\n",
      "Iteration 2866, loss = 0.28112321\n",
      "Iteration 2867, loss = 0.28105190\n",
      "Iteration 2868, loss = 0.28103077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2869, loss = 0.28095973\n",
      "Iteration 2870, loss = 0.28065380\n",
      "Iteration 2871, loss = 0.28062614\n",
      "Iteration 2872, loss = 0.28045013\n",
      "Iteration 2873, loss = 0.28011412\n",
      "Iteration 2874, loss = 0.28005449\n",
      "Iteration 2875, loss = 0.27986296\n",
      "Iteration 2876, loss = 0.27971993\n",
      "Iteration 2877, loss = 0.27944806\n",
      "Iteration 2878, loss = 0.27923905\n",
      "Iteration 2879, loss = 0.27931203\n",
      "Iteration 2880, loss = 0.27933123\n",
      "Iteration 2881, loss = 0.27902803\n",
      "Iteration 2882, loss = 0.27882631\n",
      "Iteration 2883, loss = 0.27858948\n",
      "Iteration 2884, loss = 0.27863392\n",
      "Iteration 2885, loss = 0.27850154\n",
      "Iteration 2886, loss = 0.27869063\n",
      "Iteration 2887, loss = 0.27845261\n",
      "Iteration 2888, loss = 0.27812570\n",
      "Iteration 2889, loss = 0.27802553\n",
      "Iteration 2890, loss = 0.27790167\n",
      "Iteration 2891, loss = 0.27797539\n",
      "Iteration 2892, loss = 0.27792749\n",
      "Iteration 2893, loss = 0.27776837\n",
      "Iteration 2894, loss = 0.27758998\n",
      "Iteration 2895, loss = 0.27746915\n",
      "Iteration 2896, loss = 0.27717606\n",
      "Iteration 2897, loss = 0.27711821\n",
      "Iteration 2898, loss = 0.27689429\n",
      "Iteration 2899, loss = 0.27660711\n",
      "Iteration 2900, loss = 0.27683716\n",
      "Iteration 2901, loss = 0.27657206\n",
      "Iteration 2902, loss = 0.27635271\n",
      "Iteration 2903, loss = 0.27626663\n",
      "Iteration 2904, loss = 0.27644600\n",
      "Iteration 2905, loss = 0.27648132\n",
      "Iteration 2906, loss = 0.27605105\n",
      "Iteration 2907, loss = 0.27557473\n",
      "Iteration 2908, loss = 0.27565938\n",
      "Iteration 2909, loss = 0.27586040\n",
      "Iteration 2910, loss = 0.27613850\n",
      "Iteration 2911, loss = 0.27595057\n",
      "Iteration 2912, loss = 0.27573698\n",
      "Iteration 2913, loss = 0.27526804\n",
      "Iteration 2914, loss = 0.27511494\n",
      "Iteration 2915, loss = 0.27483462\n",
      "Iteration 2916, loss = 0.27456839\n",
      "Iteration 2917, loss = 0.27546961\n",
      "Iteration 2918, loss = 0.27503589\n",
      "Iteration 2919, loss = 0.27465758\n",
      "Iteration 2920, loss = 0.27468880\n",
      "Iteration 2921, loss = 0.27433274\n",
      "Iteration 2922, loss = 0.27429961\n",
      "Iteration 2923, loss = 0.27412247\n",
      "Iteration 2924, loss = 0.27379790\n",
      "Iteration 2925, loss = 0.27350203\n",
      "Iteration 2926, loss = 0.27342614\n",
      "Iteration 2927, loss = 0.27351285\n",
      "Iteration 2928, loss = 0.27340459\n",
      "Iteration 2929, loss = 0.27326359\n",
      "Iteration 2930, loss = 0.27312635\n",
      "Iteration 2931, loss = 0.27293316\n",
      "Iteration 2932, loss = 0.27283716\n",
      "Iteration 2933, loss = 0.27283171\n",
      "Iteration 2934, loss = 0.27273225\n",
      "Iteration 2935, loss = 0.27250942\n",
      "Iteration 2936, loss = 0.27211692\n",
      "Iteration 2937, loss = 0.27232392\n",
      "Iteration 2938, loss = 0.27226923\n",
      "Iteration 2939, loss = 0.27245990\n",
      "Iteration 2940, loss = 0.27240559\n",
      "Iteration 2941, loss = 0.27222255\n",
      "Iteration 2942, loss = 0.27223272\n",
      "Iteration 2943, loss = 0.27165134\n",
      "Iteration 2944, loss = 0.27145720\n",
      "Iteration 2945, loss = 0.27134022\n",
      "Iteration 2946, loss = 0.27121658\n",
      "Iteration 2947, loss = 0.27101699\n",
      "Iteration 2948, loss = 0.27084394\n",
      "Iteration 2949, loss = 0.27077922\n",
      "Iteration 2950, loss = 0.27075182\n",
      "Iteration 2951, loss = 0.27074443\n",
      "Iteration 2952, loss = 0.27069081\n",
      "Iteration 2953, loss = 0.27024195\n",
      "Iteration 2954, loss = 0.27004367\n",
      "Iteration 2955, loss = 0.27038026\n",
      "Iteration 2956, loss = 0.27063944\n",
      "Iteration 2957, loss = 0.27049842\n",
      "Iteration 2958, loss = 0.27001533\n",
      "Iteration 2959, loss = 0.26968871\n",
      "Iteration 2960, loss = 0.26948754\n",
      "Iteration 2961, loss = 0.26953730\n",
      "Iteration 2962, loss = 0.26945046\n",
      "Iteration 2963, loss = 0.26979052\n",
      "Iteration 2964, loss = 0.27009439\n",
      "Iteration 2965, loss = 0.26996056\n",
      "Iteration 2966, loss = 0.26942352\n",
      "Iteration 2967, loss = 0.26862601\n",
      "Iteration 2968, loss = 0.26841984\n",
      "Iteration 2969, loss = 0.26894029\n",
      "Iteration 2970, loss = 0.26906768\n",
      "Iteration 2971, loss = 0.26926422\n",
      "Iteration 2972, loss = 0.26946089\n",
      "Iteration 2973, loss = 0.26943398\n",
      "Iteration 2974, loss = 0.26943139\n",
      "Iteration 2975, loss = 0.26896304\n",
      "Iteration 2976, loss = 0.26839065\n",
      "Iteration 2977, loss = 0.26803521\n",
      "Iteration 2978, loss = 0.26749099\n",
      "Iteration 2979, loss = 0.26741519\n",
      "Iteration 2980, loss = 0.26718432\n",
      "Iteration 2981, loss = 0.26704803\n",
      "Iteration 2982, loss = 0.26683194\n",
      "Iteration 2983, loss = 0.26672274\n",
      "Iteration 2984, loss = 0.26653577\n",
      "Iteration 2985, loss = 0.26662090\n",
      "Iteration 2986, loss = 0.26667303\n",
      "Iteration 2987, loss = 0.26641607\n",
      "Iteration 2988, loss = 0.26630065\n",
      "Iteration 2989, loss = 0.26614303\n",
      "Iteration 2990, loss = 0.26627504\n",
      "Iteration 2991, loss = 0.26609322\n",
      "Iteration 2992, loss = 0.26594680\n",
      "Iteration 2993, loss = 0.26585146\n",
      "Iteration 2994, loss = 0.26592019\n",
      "Iteration 2995, loss = 0.26542123\n",
      "Iteration 2996, loss = 0.26527727\n",
      "Iteration 2997, loss = 0.26524176\n",
      "Iteration 2998, loss = 0.26504403\n",
      "Iteration 2999, loss = 0.26491392\n",
      "Iteration 3000, loss = 0.26481467\n",
      "Iteration 3001, loss = 0.26471049\n",
      "Iteration 3002, loss = 0.26445385\n",
      "Iteration 3003, loss = 0.26453298\n",
      "Iteration 3004, loss = 0.26473519\n",
      "Iteration 3005, loss = 0.26476837\n",
      "Iteration 3006, loss = 0.26451226\n",
      "Iteration 3007, loss = 0.26431618\n",
      "Iteration 3008, loss = 0.26396253\n",
      "Iteration 3009, loss = 0.26383801\n",
      "Iteration 3010, loss = 0.26358225\n",
      "Iteration 3011, loss = 0.26341887\n",
      "Iteration 3012, loss = 0.26339104\n",
      "Iteration 3013, loss = 0.26335196\n",
      "Iteration 3014, loss = 0.26325016\n",
      "Iteration 3015, loss = 0.26296745\n",
      "Iteration 3016, loss = 0.26300583\n",
      "Iteration 3017, loss = 0.26300356\n",
      "Iteration 3018, loss = 0.26294744\n",
      "Iteration 3019, loss = 0.26294189\n",
      "Iteration 3020, loss = 0.26295053\n",
      "Iteration 3021, loss = 0.26261292\n",
      "Iteration 3022, loss = 0.26208829\n",
      "Iteration 3023, loss = 0.26204474\n",
      "Iteration 3024, loss = 0.26208955\n",
      "Iteration 3025, loss = 0.26201496\n",
      "Iteration 3026, loss = 0.26208301\n",
      "Iteration 3027, loss = 0.26193822\n",
      "Iteration 3028, loss = 0.26162521\n",
      "Iteration 3029, loss = 0.26157371\n",
      "Iteration 3030, loss = 0.26138163\n",
      "Iteration 3031, loss = 0.26111907\n",
      "Iteration 3032, loss = 0.26097842\n",
      "Iteration 3033, loss = 0.26088067\n",
      "Iteration 3034, loss = 0.26087147\n",
      "Iteration 3035, loss = 0.26084346\n",
      "Iteration 3036, loss = 0.26070729\n",
      "Iteration 3037, loss = 0.26051051\n",
      "Iteration 3038, loss = 0.26030460\n",
      "Iteration 3039, loss = 0.26029908\n",
      "Iteration 3040, loss = 0.26004343\n",
      "Iteration 3041, loss = 0.26000451\n",
      "Iteration 3042, loss = 0.25993648\n",
      "Iteration 3043, loss = 0.25981863\n",
      "Iteration 3044, loss = 0.25986274\n",
      "Iteration 3045, loss = 0.25981769\n",
      "Iteration 3046, loss = 0.25944804\n",
      "Iteration 3047, loss = 0.25927484\n",
      "Iteration 3048, loss = 0.25913253\n",
      "Iteration 3049, loss = 0.25915445\n",
      "Iteration 3050, loss = 0.25907952\n",
      "Iteration 3051, loss = 0.25906955\n",
      "Iteration 3052, loss = 0.25896411\n",
      "Iteration 3053, loss = 0.25867824\n",
      "Iteration 3054, loss = 0.25865237\n",
      "Iteration 3055, loss = 0.25839362\n",
      "Iteration 3056, loss = 0.25835319\n",
      "Iteration 3057, loss = 0.25800330\n",
      "Iteration 3058, loss = 0.25791047\n",
      "Iteration 3059, loss = 0.25851388\n",
      "Iteration 3060, loss = 0.25883187\n",
      "Iteration 3061, loss = 0.25869757\n",
      "Iteration 3062, loss = 0.25825963\n",
      "Iteration 3063, loss = 0.25747305\n",
      "Iteration 3064, loss = 0.25727264\n",
      "Iteration 3065, loss = 0.25731644\n",
      "Iteration 3066, loss = 0.25784148\n",
      "Iteration 3067, loss = 0.25805503\n",
      "Iteration 3068, loss = 0.25792372\n",
      "Iteration 3069, loss = 0.25698691\n",
      "Iteration 3070, loss = 0.25673524\n",
      "Iteration 3071, loss = 0.25646850\n",
      "Iteration 3072, loss = 0.25632138\n",
      "Iteration 3073, loss = 0.25618808\n",
      "Iteration 3074, loss = 0.25605337\n",
      "Iteration 3075, loss = 0.25599469\n",
      "Iteration 3076, loss = 0.25606749\n",
      "Iteration 3077, loss = 0.25570375\n",
      "Iteration 3078, loss = 0.25574972\n",
      "Iteration 3079, loss = 0.25555833\n",
      "Iteration 3080, loss = 0.25563442\n",
      "Iteration 3081, loss = 0.25554057\n",
      "Iteration 3082, loss = 0.25548217\n",
      "Iteration 3083, loss = 0.25545645\n",
      "Iteration 3084, loss = 0.25525966\n",
      "Iteration 3085, loss = 0.25511601\n",
      "Iteration 3086, loss = 0.25502797\n",
      "Iteration 3087, loss = 0.25460527\n",
      "Iteration 3088, loss = 0.25449958\n",
      "Iteration 3089, loss = 0.25433201\n",
      "Iteration 3090, loss = 0.25448754\n",
      "Iteration 3091, loss = 0.25414553\n",
      "Iteration 3092, loss = 0.25416675\n",
      "Iteration 3093, loss = 0.25399194\n",
      "Iteration 3094, loss = 0.25362390\n",
      "Iteration 3095, loss = 0.25362118\n",
      "Iteration 3096, loss = 0.25429358\n",
      "Iteration 3097, loss = 0.25492939\n",
      "Iteration 3098, loss = 0.25545619\n",
      "Iteration 3099, loss = 0.25511754\n",
      "Iteration 3100, loss = 0.25458953\n",
      "Iteration 3101, loss = 0.25385781\n",
      "Iteration 3102, loss = 0.25344417\n",
      "Iteration 3103, loss = 0.25301418\n",
      "Iteration 3104, loss = 0.25268999\n",
      "Iteration 3105, loss = 0.25266419\n",
      "Iteration 3106, loss = 0.25247619\n",
      "Iteration 3107, loss = 0.25261075\n",
      "Iteration 3108, loss = 0.25220678\n",
      "Iteration 3109, loss = 0.25193737\n",
      "Iteration 3110, loss = 0.25233858\n",
      "Iteration 3111, loss = 0.25206407\n",
      "Iteration 3112, loss = 0.25193982\n",
      "Iteration 3113, loss = 0.25169972\n",
      "Iteration 3114, loss = 0.25134388\n",
      "Iteration 3115, loss = 0.25170568\n",
      "Iteration 3116, loss = 0.25149977\n",
      "Iteration 3117, loss = 0.25166273\n",
      "Iteration 3118, loss = 0.25133784\n",
      "Iteration 3119, loss = 0.25112362\n",
      "Iteration 3120, loss = 0.25082561\n",
      "Iteration 3121, loss = 0.25080474\n",
      "Iteration 3122, loss = 0.25084017\n",
      "Iteration 3123, loss = 0.25084646\n",
      "Iteration 3124, loss = 0.25072750\n",
      "Iteration 3125, loss = 0.25049869\n",
      "Iteration 3126, loss = 0.25013534\n",
      "Iteration 3127, loss = 0.25001685\n",
      "Iteration 3128, loss = 0.24991139\n",
      "Iteration 3129, loss = 0.24970708\n",
      "Iteration 3130, loss = 0.24968394\n",
      "Iteration 3131, loss = 0.24950328\n",
      "Iteration 3132, loss = 0.24950090\n",
      "Iteration 3133, loss = 0.24944378\n",
      "Iteration 3134, loss = 0.24932890\n",
      "Iteration 3135, loss = 0.24916836\n",
      "Iteration 3136, loss = 0.24909935\n",
      "Iteration 3137, loss = 0.24901742\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3138, loss = 0.24879524\n",
      "Iteration 3139, loss = 0.24854237\n",
      "Iteration 3140, loss = 0.24870950\n",
      "Iteration 3141, loss = 0.24845803\n",
      "Iteration 3142, loss = 0.24824196\n",
      "Iteration 3143, loss = 0.24803300\n",
      "Iteration 3144, loss = 0.24795157\n",
      "Iteration 3145, loss = 0.24778799\n",
      "Iteration 3146, loss = 0.24774600\n",
      "Iteration 3147, loss = 0.24775056\n",
      "Iteration 3148, loss = 0.24764230\n",
      "Iteration 3149, loss = 0.24755595\n",
      "Iteration 3150, loss = 0.24733261\n",
      "Iteration 3151, loss = 0.24748116\n",
      "Iteration 3152, loss = 0.24745173\n",
      "Iteration 3153, loss = 0.24741117\n",
      "Iteration 3154, loss = 0.24721102\n",
      "Iteration 3155, loss = 0.24698456\n",
      "Iteration 3156, loss = 0.24719727\n",
      "Iteration 3157, loss = 0.24678942\n",
      "Iteration 3158, loss = 0.24653209\n",
      "Iteration 3159, loss = 0.24633023\n",
      "Iteration 3160, loss = 0.24612701\n",
      "Iteration 3161, loss = 0.24602272\n",
      "Iteration 3162, loss = 0.24597851\n",
      "Iteration 3163, loss = 0.24584867\n",
      "Iteration 3164, loss = 0.24578963\n",
      "Iteration 3165, loss = 0.24546031\n",
      "Iteration 3166, loss = 0.24557108\n",
      "Iteration 3167, loss = 0.24597203\n",
      "Iteration 3168, loss = 0.24656196\n",
      "Iteration 3169, loss = 0.24718743\n",
      "Iteration 3170, loss = 0.24716725\n",
      "Iteration 3171, loss = 0.24655822\n",
      "Iteration 3172, loss = 0.24543675\n",
      "Iteration 3173, loss = 0.24521927\n",
      "Iteration 3174, loss = 0.24492907\n",
      "Iteration 3175, loss = 0.24526745\n",
      "Iteration 3176, loss = 0.24542092\n",
      "Iteration 3177, loss = 0.24552908\n",
      "Iteration 3178, loss = 0.24485598\n",
      "Iteration 3179, loss = 0.24431941\n",
      "Iteration 3180, loss = 0.24377295\n",
      "Iteration 3181, loss = 0.24350978\n",
      "Iteration 3182, loss = 0.24419720\n",
      "Iteration 3183, loss = 0.24544571\n",
      "Iteration 3184, loss = 0.24612646\n",
      "Iteration 3185, loss = 0.24598863\n",
      "Iteration 3186, loss = 0.24512608\n",
      "Iteration 3187, loss = 0.24438409\n",
      "Iteration 3188, loss = 0.24342018\n",
      "Iteration 3189, loss = 0.24290087\n",
      "Iteration 3190, loss = 0.24280845\n",
      "Iteration 3191, loss = 0.24320294\n",
      "Iteration 3192, loss = 0.24320949\n",
      "Iteration 3193, loss = 0.24325909\n",
      "Iteration 3194, loss = 0.24349379\n",
      "Iteration 3195, loss = 0.24370847\n",
      "Iteration 3196, loss = 0.24333707\n",
      "Iteration 3197, loss = 0.24302260\n",
      "Iteration 3198, loss = 0.24194253\n",
      "Iteration 3199, loss = 0.24217012\n",
      "Iteration 3200, loss = 0.24219496\n",
      "Iteration 3201, loss = 0.24213337\n",
      "Iteration 3202, loss = 0.24208726\n",
      "Iteration 3203, loss = 0.24174926\n",
      "Iteration 3204, loss = 0.24139078\n",
      "Iteration 3205, loss = 0.24130784\n",
      "Iteration 3206, loss = 0.24162754\n",
      "Iteration 3207, loss = 0.24121366\n",
      "Iteration 3208, loss = 0.24103526\n",
      "Iteration 3209, loss = 0.24091102\n",
      "Iteration 3210, loss = 0.24066409\n",
      "Iteration 3211, loss = 0.24061525\n",
      "Iteration 3212, loss = 0.24048136\n",
      "Iteration 3213, loss = 0.24035313\n",
      "Iteration 3214, loss = 0.24029898\n",
      "Iteration 3215, loss = 0.24031010\n",
      "Iteration 3216, loss = 0.24006185\n",
      "Iteration 3217, loss = 0.24009462\n",
      "Iteration 3218, loss = 0.24002898\n",
      "Iteration 3219, loss = 0.23982026\n",
      "Iteration 3220, loss = 0.23979444\n",
      "Iteration 3221, loss = 0.23964279\n",
      "Iteration 3222, loss = 0.23958016\n",
      "Iteration 3223, loss = 0.23943982\n",
      "Iteration 3224, loss = 0.23925546\n",
      "Iteration 3225, loss = 0.23901312\n",
      "Iteration 3226, loss = 0.23891354\n",
      "Iteration 3227, loss = 0.23888991\n",
      "Iteration 3228, loss = 0.23897757\n",
      "Iteration 3229, loss = 0.23897023\n",
      "Iteration 3230, loss = 0.23849656\n",
      "Iteration 3231, loss = 0.23860814\n",
      "Iteration 3232, loss = 0.23833400\n",
      "Iteration 3233, loss = 0.23846616\n",
      "Iteration 3234, loss = 0.23888081\n",
      "Iteration 3235, loss = 0.23791801\n",
      "Iteration 3236, loss = 0.23813770\n",
      "Iteration 3237, loss = 0.23826190\n",
      "Iteration 3238, loss = 0.23840347\n",
      "Iteration 3239, loss = 0.23867838\n",
      "Iteration 3240, loss = 0.23871829\n",
      "Iteration 3241, loss = 0.23831841\n",
      "Iteration 3242, loss = 0.23759301\n",
      "Iteration 3243, loss = 0.23724014\n",
      "Iteration 3244, loss = 0.23693469\n",
      "Iteration 3245, loss = 0.23708249\n",
      "Iteration 3246, loss = 0.23778791\n",
      "Iteration 3247, loss = 0.23889182\n",
      "Iteration 3248, loss = 0.23876227\n",
      "Iteration 3249, loss = 0.23805188\n",
      "Iteration 3250, loss = 0.23712860\n",
      "Iteration 3251, loss = 0.23675402\n",
      "Iteration 3252, loss = 0.23647347\n",
      "Iteration 3253, loss = 0.23644331\n",
      "Iteration 3254, loss = 0.23652719\n",
      "Iteration 3255, loss = 0.23641641\n",
      "Iteration 3256, loss = 0.23620000\n",
      "Iteration 3257, loss = 0.23590115\n",
      "Iteration 3258, loss = 0.23581369\n",
      "Iteration 3259, loss = 0.23583521\n",
      "Iteration 3260, loss = 0.23606653\n",
      "Iteration 3261, loss = 0.23573986\n",
      "Iteration 3262, loss = 0.23525187\n",
      "Iteration 3263, loss = 0.23531200\n",
      "Iteration 3264, loss = 0.23477688\n",
      "Iteration 3265, loss = 0.23488287\n",
      "Iteration 3266, loss = 0.23484716\n",
      "Iteration 3267, loss = 0.23456926\n",
      "Iteration 3268, loss = 0.23453261\n",
      "Iteration 3269, loss = 0.23451855\n",
      "Iteration 3270, loss = 0.23488987\n",
      "Iteration 3271, loss = 0.23447365\n",
      "Iteration 3272, loss = 0.23408709\n",
      "Iteration 3273, loss = 0.23392188\n",
      "Iteration 3274, loss = 0.23399688\n",
      "Iteration 3275, loss = 0.23461834\n",
      "Iteration 3276, loss = 0.23451012\n",
      "Iteration 3277, loss = 0.23405805\n",
      "Iteration 3278, loss = 0.23362497\n",
      "Iteration 3279, loss = 0.23320575\n",
      "Iteration 3280, loss = 0.23342485\n",
      "Iteration 3281, loss = 0.23357713\n",
      "Iteration 3282, loss = 0.23408748\n",
      "Iteration 3283, loss = 0.23423231\n",
      "Iteration 3284, loss = 0.23394554\n",
      "Iteration 3285, loss = 0.23356527\n",
      "Iteration 3286, loss = 0.23358196\n",
      "Iteration 3287, loss = 0.23287815\n",
      "Iteration 3288, loss = 0.23303281\n",
      "Iteration 3289, loss = 0.23265031\n",
      "Iteration 3290, loss = 0.23239558\n",
      "Iteration 3291, loss = 0.23234849\n",
      "Iteration 3292, loss = 0.23243291\n",
      "Iteration 3293, loss = 0.23220110\n",
      "Iteration 3294, loss = 0.23186359\n",
      "Iteration 3295, loss = 0.23164530\n",
      "Iteration 3296, loss = 0.23175633\n",
      "Iteration 3297, loss = 0.23145221\n",
      "Iteration 3298, loss = 0.23127179\n",
      "Iteration 3299, loss = 0.23139735\n",
      "Iteration 3300, loss = 0.23114062\n",
      "Iteration 3301, loss = 0.23099768\n",
      "Iteration 3302, loss = 0.23104519\n",
      "Iteration 3303, loss = 0.23110110\n",
      "Iteration 3304, loss = 0.23109919\n",
      "Iteration 3305, loss = 0.23082592\n",
      "Iteration 3306, loss = 0.23085815\n",
      "Iteration 3307, loss = 0.23061116\n",
      "Iteration 3308, loss = 0.23045486\n",
      "Iteration 3309, loss = 0.23015956\n",
      "Iteration 3310, loss = 0.23006744\n",
      "Iteration 3311, loss = 0.22995239\n",
      "Iteration 3312, loss = 0.22988927\n",
      "Iteration 3313, loss = 0.22977265\n",
      "Iteration 3314, loss = 0.22980661\n",
      "Iteration 3315, loss = 0.22965535\n",
      "Iteration 3316, loss = 0.22952213\n",
      "Iteration 3317, loss = 0.22945190\n",
      "Iteration 3318, loss = 0.22953114\n",
      "Iteration 3319, loss = 0.22962308\n",
      "Iteration 3320, loss = 0.22952759\n",
      "Iteration 3321, loss = 0.22944858\n",
      "Iteration 3322, loss = 0.22929760\n",
      "Iteration 3323, loss = 0.22909196\n",
      "Iteration 3324, loss = 0.22893524\n",
      "Iteration 3325, loss = 0.22862334\n",
      "Iteration 3326, loss = 0.22853488\n",
      "Iteration 3327, loss = 0.22839983\n",
      "Iteration 3328, loss = 0.22830739\n",
      "Iteration 3329, loss = 0.22816855\n",
      "Iteration 3330, loss = 0.22795673\n",
      "Iteration 3331, loss = 0.22787366\n",
      "Iteration 3332, loss = 0.22780908\n",
      "Iteration 3333, loss = 0.22795112\n",
      "Iteration 3334, loss = 0.22809957\n",
      "Iteration 3335, loss = 0.22779011\n",
      "Iteration 3336, loss = 0.22747105\n",
      "Iteration 3337, loss = 0.22726733\n",
      "Iteration 3338, loss = 0.22730240\n",
      "Iteration 3339, loss = 0.22758145\n",
      "Iteration 3340, loss = 0.22780770\n",
      "Iteration 3341, loss = 0.22780395\n",
      "Iteration 3342, loss = 0.22736465\n",
      "Iteration 3343, loss = 0.22693276\n",
      "Iteration 3344, loss = 0.22647989\n",
      "Iteration 3345, loss = 0.22661223\n",
      "Iteration 3346, loss = 0.22683582\n",
      "Iteration 3347, loss = 0.22701914\n",
      "Iteration 3348, loss = 0.22687283\n",
      "Iteration 3349, loss = 0.22635331\n",
      "Iteration 3350, loss = 0.22610649\n",
      "Iteration 3351, loss = 0.22606299\n",
      "Iteration 3352, loss = 0.22606362\n",
      "Iteration 3353, loss = 0.22631336\n",
      "Iteration 3354, loss = 0.22634051\n",
      "Iteration 3355, loss = 0.22600890\n",
      "Iteration 3356, loss = 0.22578989\n",
      "Iteration 3357, loss = 0.22532487\n",
      "Iteration 3358, loss = 0.22545351\n",
      "Iteration 3359, loss = 0.22544034\n",
      "Iteration 3360, loss = 0.22546313\n",
      "Iteration 3361, loss = 0.22506265\n",
      "Iteration 3362, loss = 0.22466496\n",
      "Iteration 3363, loss = 0.22478555\n",
      "Iteration 3364, loss = 0.22486402\n",
      "Iteration 3365, loss = 0.22477932\n",
      "Iteration 3366, loss = 0.22458264\n",
      "Iteration 3367, loss = 0.22438856\n",
      "Iteration 3368, loss = 0.22442800\n",
      "Iteration 3369, loss = 0.22411355\n",
      "Iteration 3370, loss = 0.22400916\n",
      "Iteration 3371, loss = 0.22388745\n",
      "Iteration 3372, loss = 0.22396178\n",
      "Iteration 3373, loss = 0.22379907\n",
      "Iteration 3374, loss = 0.22359689\n",
      "Iteration 3375, loss = 0.22348104\n",
      "Iteration 3376, loss = 0.22331275\n",
      "Iteration 3377, loss = 0.22348788\n",
      "Iteration 3378, loss = 0.22344397\n",
      "Iteration 3379, loss = 0.22318749\n",
      "Iteration 3380, loss = 0.22301295\n",
      "Iteration 3381, loss = 0.22285514\n",
      "Iteration 3382, loss = 0.22282028\n",
      "Iteration 3383, loss = 0.22258179\n",
      "Iteration 3384, loss = 0.22238457\n",
      "Iteration 3385, loss = 0.22247797\n",
      "Iteration 3386, loss = 0.22267772\n",
      "Iteration 3387, loss = 0.22265047\n",
      "Iteration 3388, loss = 0.22253577\n",
      "Iteration 3389, loss = 0.22229310\n",
      "Iteration 3390, loss = 0.22214776\n",
      "Iteration 3391, loss = 0.22199985\n",
      "Iteration 3392, loss = 0.22209938\n",
      "Iteration 3393, loss = 0.22200269\n",
      "Iteration 3394, loss = 0.22193699\n",
      "Iteration 3395, loss = 0.22158332\n",
      "Iteration 3396, loss = 0.22145067\n",
      "Iteration 3397, loss = 0.22141815\n",
      "Iteration 3398, loss = 0.22123887\n",
      "Iteration 3399, loss = 0.22112583\n",
      "Iteration 3400, loss = 0.22103327\n",
      "Iteration 3401, loss = 0.22101583\n",
      "Iteration 3402, loss = 0.22112177\n",
      "Iteration 3403, loss = 0.22108046\n",
      "Iteration 3404, loss = 0.22074400\n",
      "Iteration 3405, loss = 0.22053574\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3406, loss = 0.22050648\n",
      "Iteration 3407, loss = 0.22055589\n",
      "Iteration 3408, loss = 0.22022905\n",
      "Iteration 3409, loss = 0.21987584\n",
      "Iteration 3410, loss = 0.22004737\n",
      "Iteration 3411, loss = 0.22001067\n",
      "Iteration 3412, loss = 0.22020845\n",
      "Iteration 3413, loss = 0.21994254\n",
      "Iteration 3414, loss = 0.21973896\n",
      "Iteration 3415, loss = 0.21955650\n",
      "Iteration 3416, loss = 0.21931876\n",
      "Iteration 3417, loss = 0.21930033\n",
      "Iteration 3418, loss = 0.21943334\n",
      "Iteration 3419, loss = 0.21937202\n",
      "Iteration 3420, loss = 0.21944515\n",
      "Iteration 3421, loss = 0.21940661\n",
      "Iteration 3422, loss = 0.21938084\n",
      "Iteration 3423, loss = 0.21941352\n",
      "Iteration 3424, loss = 0.21924293\n",
      "Iteration 3425, loss = 0.21895136\n",
      "Iteration 3426, loss = 0.21816124\n",
      "Iteration 3427, loss = 0.21820795\n",
      "Iteration 3428, loss = 0.21938928\n",
      "Iteration 3429, loss = 0.21976221\n",
      "Iteration 3430, loss = 0.21968344\n",
      "Iteration 3431, loss = 0.21909987\n",
      "Iteration 3432, loss = 0.21851630\n",
      "Iteration 3433, loss = 0.21823573\n",
      "Iteration 3434, loss = 0.21822755\n",
      "Iteration 3435, loss = 0.21799642\n",
      "Iteration 3436, loss = 0.21781877\n",
      "Iteration 3437, loss = 0.21743782\n",
      "Iteration 3438, loss = 0.21728587\n",
      "Iteration 3439, loss = 0.21722432\n",
      "Iteration 3440, loss = 0.21724709\n",
      "Iteration 3441, loss = 0.21735631\n",
      "Iteration 3442, loss = 0.21715630\n",
      "Iteration 3443, loss = 0.21682165\n",
      "Iteration 3444, loss = 0.21682288\n",
      "Iteration 3445, loss = 0.21670101\n",
      "Iteration 3446, loss = 0.21657202\n",
      "Iteration 3447, loss = 0.21663622\n",
      "Iteration 3448, loss = 0.21625824\n",
      "Iteration 3449, loss = 0.21619513\n",
      "Iteration 3450, loss = 0.21585259\n",
      "Iteration 3451, loss = 0.21613658\n",
      "Iteration 3452, loss = 0.21629657\n",
      "Iteration 3453, loss = 0.21621709\n",
      "Iteration 3454, loss = 0.21547307\n",
      "Iteration 3455, loss = 0.21525319\n",
      "Iteration 3456, loss = 0.21622076\n",
      "Iteration 3457, loss = 0.21645278\n",
      "Iteration 3458, loss = 0.21633349\n",
      "Iteration 3459, loss = 0.21590090\n",
      "Iteration 3460, loss = 0.21542479\n",
      "Iteration 3461, loss = 0.21513377\n",
      "Iteration 3462, loss = 0.21500011\n",
      "Iteration 3463, loss = 0.21499053\n",
      "Iteration 3464, loss = 0.21458939\n",
      "Iteration 3465, loss = 0.21470035\n",
      "Iteration 3466, loss = 0.21474971\n",
      "Iteration 3467, loss = 0.21462901\n",
      "Iteration 3468, loss = 0.21455759\n",
      "Iteration 3469, loss = 0.21453212\n",
      "Iteration 3470, loss = 0.21420510\n",
      "Iteration 3471, loss = 0.21408180\n",
      "Iteration 3472, loss = 0.21411469\n",
      "Iteration 3473, loss = 0.21414664\n",
      "Iteration 3474, loss = 0.21392599\n",
      "Iteration 3475, loss = 0.21373579\n",
      "Iteration 3476, loss = 0.21362995\n",
      "Iteration 3477, loss = 0.21332886\n",
      "Iteration 3478, loss = 0.21322698\n",
      "Iteration 3479, loss = 0.21369609\n",
      "Iteration 3480, loss = 0.21398315\n",
      "Iteration 3481, loss = 0.21420519\n",
      "Iteration 3482, loss = 0.21412727\n",
      "Iteration 3483, loss = 0.21368413\n",
      "Iteration 3484, loss = 0.21303010\n",
      "Iteration 3485, loss = 0.21304599\n",
      "Iteration 3486, loss = 0.21265462\n",
      "Iteration 3487, loss = 0.21243293\n",
      "Iteration 3488, loss = 0.21236901\n",
      "Iteration 3489, loss = 0.21238571\n",
      "Iteration 3490, loss = 0.21231962\n",
      "Iteration 3491, loss = 0.21209131\n",
      "Iteration 3492, loss = 0.21200389\n",
      "Iteration 3493, loss = 0.21203838\n",
      "Iteration 3494, loss = 0.21187441\n",
      "Iteration 3495, loss = 0.21178633\n",
      "Iteration 3496, loss = 0.21176309\n",
      "Iteration 3497, loss = 0.21178191\n",
      "Iteration 3498, loss = 0.21158725\n",
      "Iteration 3499, loss = 0.21149481\n",
      "Iteration 3500, loss = 0.21112536\n",
      "Iteration 3501, loss = 0.21145060\n",
      "Iteration 3502, loss = 0.21157295\n",
      "Iteration 3503, loss = 0.21152052\n",
      "Iteration 3504, loss = 0.21140673\n",
      "Iteration 3505, loss = 0.21122442\n",
      "Iteration 3506, loss = 0.21098784\n",
      "Iteration 3507, loss = 0.21122897\n",
      "Iteration 3508, loss = 0.21110387\n",
      "Iteration 3509, loss = 0.21088623\n",
      "Iteration 3510, loss = 0.21059140\n",
      "Iteration 3511, loss = 0.21032657\n",
      "Iteration 3512, loss = 0.21041800\n",
      "Iteration 3513, loss = 0.21041202\n",
      "Iteration 3514, loss = 0.21052160\n",
      "Iteration 3515, loss = 0.21051297\n",
      "Iteration 3516, loss = 0.21031790\n",
      "Iteration 3517, loss = 0.20995797\n",
      "Iteration 3518, loss = 0.21010790\n",
      "Iteration 3519, loss = 0.20978601\n",
      "Iteration 3520, loss = 0.20941573\n",
      "Iteration 3521, loss = 0.20940628\n",
      "Iteration 3522, loss = 0.20926636\n",
      "Iteration 3523, loss = 0.20906270\n",
      "Iteration 3524, loss = 0.20900348\n",
      "Iteration 3525, loss = 0.20899581\n",
      "Iteration 3526, loss = 0.20892656\n",
      "Iteration 3527, loss = 0.20886015\n",
      "Iteration 3528, loss = 0.20876212\n",
      "Iteration 3529, loss = 0.20895877\n",
      "Iteration 3530, loss = 0.20867515\n",
      "Iteration 3531, loss = 0.20856331\n",
      "Iteration 3532, loss = 0.20852735\n",
      "Iteration 3533, loss = 0.20831648\n",
      "Iteration 3534, loss = 0.20829778\n",
      "Iteration 3535, loss = 0.20800476\n",
      "Iteration 3536, loss = 0.20800053\n",
      "Iteration 3537, loss = 0.20806732\n",
      "Iteration 3538, loss = 0.20800519\n",
      "Iteration 3539, loss = 0.20781795\n",
      "Iteration 3540, loss = 0.20761285\n",
      "Iteration 3541, loss = 0.20750670\n",
      "Iteration 3542, loss = 0.20741692\n",
      "Iteration 3543, loss = 0.20743000\n",
      "Iteration 3544, loss = 0.20706390\n",
      "Iteration 3545, loss = 0.20705141\n",
      "Iteration 3546, loss = 0.20707564\n",
      "Iteration 3547, loss = 0.20704375\n",
      "Iteration 3548, loss = 0.20699939\n",
      "Iteration 3549, loss = 0.20668443\n",
      "Iteration 3550, loss = 0.20681568\n",
      "Iteration 3551, loss = 0.20648143\n",
      "Iteration 3552, loss = 0.20685819\n",
      "Iteration 3553, loss = 0.20656376\n",
      "Iteration 3554, loss = 0.20612567\n",
      "Iteration 3555, loss = 0.20661109\n",
      "Iteration 3556, loss = 0.20626225\n",
      "Iteration 3557, loss = 0.20608253\n",
      "Iteration 3558, loss = 0.20602487\n",
      "Iteration 3559, loss = 0.20589227\n",
      "Iteration 3560, loss = 0.20577897\n",
      "Iteration 3561, loss = 0.20545520\n",
      "Iteration 3562, loss = 0.20556126\n",
      "Iteration 3563, loss = 0.20550543\n",
      "Iteration 3564, loss = 0.20549491\n",
      "Iteration 3565, loss = 0.20542294\n",
      "Iteration 3566, loss = 0.20531482\n",
      "Iteration 3567, loss = 0.20503408\n",
      "Iteration 3568, loss = 0.20507721\n",
      "Iteration 3569, loss = 0.20536384\n",
      "Iteration 3570, loss = 0.20517960\n",
      "Iteration 3571, loss = 0.20493986\n",
      "Iteration 3572, loss = 0.20497176\n",
      "Iteration 3573, loss = 0.20453749\n",
      "Iteration 3574, loss = 0.20456670\n",
      "Iteration 3575, loss = 0.20478495\n",
      "Iteration 3576, loss = 0.20446633\n",
      "Iteration 3577, loss = 0.20444829\n",
      "Iteration 3578, loss = 0.20406340\n",
      "Iteration 3579, loss = 0.20390688\n",
      "Iteration 3580, loss = 0.20395401\n",
      "Iteration 3581, loss = 0.20410393\n",
      "Iteration 3582, loss = 0.20402437\n",
      "Iteration 3583, loss = 0.20367771\n",
      "Iteration 3584, loss = 0.20342209\n",
      "Iteration 3585, loss = 0.20350141\n",
      "Iteration 3586, loss = 0.20343453\n",
      "Iteration 3587, loss = 0.20333354\n",
      "Iteration 3588, loss = 0.20327731\n",
      "Iteration 3589, loss = 0.20357506\n",
      "Iteration 3590, loss = 0.20350113\n",
      "Iteration 3591, loss = 0.20340767\n",
      "Iteration 3592, loss = 0.20308556\n",
      "Iteration 3593, loss = 0.20277293\n",
      "Iteration 3594, loss = 0.20271260\n",
      "Iteration 3595, loss = 0.20259483\n",
      "Iteration 3596, loss = 0.20248134\n",
      "Iteration 3597, loss = 0.20237015\n",
      "Iteration 3598, loss = 0.20222056\n",
      "Iteration 3599, loss = 0.20209410\n",
      "Iteration 3600, loss = 0.20194426\n",
      "Iteration 3601, loss = 0.20190193\n",
      "Iteration 3602, loss = 0.20214096\n",
      "Iteration 3603, loss = 0.20227458\n",
      "Iteration 3604, loss = 0.20216557\n",
      "Iteration 3605, loss = 0.20221142\n",
      "Iteration 3606, loss = 0.20191122\n",
      "Iteration 3607, loss = 0.20147063\n",
      "Iteration 3608, loss = 0.20150354\n",
      "Iteration 3609, loss = 0.20125306\n",
      "Iteration 3610, loss = 0.20156360\n",
      "Iteration 3611, loss = 0.20143165\n",
      "Iteration 3612, loss = 0.20101017\n",
      "Iteration 3613, loss = 0.20074170\n",
      "Iteration 3614, loss = 0.20072458\n",
      "Iteration 3615, loss = 0.20123328\n",
      "Iteration 3616, loss = 0.20147567\n",
      "Iteration 3617, loss = 0.20114873\n",
      "Iteration 3618, loss = 0.20074877\n",
      "Iteration 3619, loss = 0.20030010\n",
      "Iteration 3620, loss = 0.20018992\n",
      "Iteration 3621, loss = 0.20034436\n",
      "Iteration 3622, loss = 0.20032565\n",
      "Iteration 3623, loss = 0.20029820\n",
      "Iteration 3624, loss = 0.19986915\n",
      "Iteration 3625, loss = 0.20007665\n",
      "Iteration 3626, loss = 0.19968979\n",
      "Iteration 3627, loss = 0.19950893\n",
      "Iteration 3628, loss = 0.19943236\n",
      "Iteration 3629, loss = 0.19931151\n",
      "Iteration 3630, loss = 0.19934597\n",
      "Iteration 3631, loss = 0.19929596\n",
      "Iteration 3632, loss = 0.19894014\n",
      "Iteration 3633, loss = 0.19899228\n",
      "Iteration 3634, loss = 0.19895244\n",
      "Iteration 3635, loss = 0.19891472\n",
      "Iteration 3636, loss = 0.19861394\n",
      "Iteration 3637, loss = 0.19881134\n",
      "Iteration 3638, loss = 0.19850945\n",
      "Iteration 3639, loss = 0.19844073\n",
      "Iteration 3640, loss = 0.19828398\n",
      "Iteration 3641, loss = 0.19815821\n",
      "Iteration 3642, loss = 0.19817718\n",
      "Iteration 3643, loss = 0.19838696\n",
      "Iteration 3644, loss = 0.19836804\n",
      "Iteration 3645, loss = 0.19802976\n",
      "Iteration 3646, loss = 0.19784229\n",
      "Iteration 3647, loss = 0.19767839\n",
      "Iteration 3648, loss = 0.19752211\n",
      "Iteration 3649, loss = 0.19758668\n",
      "Iteration 3650, loss = 0.19752372\n",
      "Iteration 3651, loss = 0.19712318\n",
      "Iteration 3652, loss = 0.19724692\n",
      "Iteration 3653, loss = 0.19755212\n",
      "Iteration 3654, loss = 0.19766980\n",
      "Iteration 3655, loss = 0.19767932\n",
      "Iteration 3656, loss = 0.19749412\n",
      "Iteration 3657, loss = 0.19712653\n",
      "Iteration 3658, loss = 0.19678102\n",
      "Iteration 3659, loss = 0.19678765\n",
      "Iteration 3660, loss = 0.19670954\n",
      "Iteration 3661, loss = 0.19693638\n",
      "Iteration 3662, loss = 0.19685993\n",
      "Iteration 3663, loss = 0.19651469\n",
      "Iteration 3664, loss = 0.19622894\n",
      "Iteration 3665, loss = 0.19613124\n",
      "Iteration 3666, loss = 0.19608635\n",
      "Iteration 3667, loss = 0.19610357\n",
      "Iteration 3668, loss = 0.19603895\n",
      "Iteration 3669, loss = 0.19590171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3670, loss = 0.19594664\n",
      "Iteration 3671, loss = 0.19578445\n",
      "Iteration 3672, loss = 0.19575912\n",
      "Iteration 3673, loss = 0.19582569\n",
      "Iteration 3674, loss = 0.19570323\n",
      "Iteration 3675, loss = 0.19537899\n",
      "Iteration 3676, loss = 0.19511222\n",
      "Iteration 3677, loss = 0.19495939\n",
      "Iteration 3678, loss = 0.19509617\n",
      "Iteration 3679, loss = 0.19493622\n",
      "Iteration 3680, loss = 0.19485367\n",
      "Iteration 3681, loss = 0.19465117\n",
      "Iteration 3682, loss = 0.19451512\n",
      "Iteration 3683, loss = 0.19468814\n",
      "Iteration 3684, loss = 0.19435219\n",
      "Iteration 3685, loss = 0.19427046\n",
      "Iteration 3686, loss = 0.19415011\n",
      "Iteration 3687, loss = 0.19406046\n",
      "Iteration 3688, loss = 0.19396625\n",
      "Iteration 3689, loss = 0.19432587\n",
      "Iteration 3690, loss = 0.19397355\n",
      "Iteration 3691, loss = 0.19394259\n",
      "Iteration 3692, loss = 0.19400908\n",
      "Iteration 3693, loss = 0.19384925\n",
      "Iteration 3694, loss = 0.19381666\n",
      "Iteration 3695, loss = 0.19377565\n",
      "Iteration 3696, loss = 0.19362809\n",
      "Iteration 3697, loss = 0.19348661\n",
      "Iteration 3698, loss = 0.19334401\n",
      "Iteration 3699, loss = 0.19313361\n",
      "Iteration 3700, loss = 0.19312510\n",
      "Iteration 3701, loss = 0.19333619\n",
      "Iteration 3702, loss = 0.19358918\n",
      "Iteration 3703, loss = 0.19338940\n",
      "Iteration 3704, loss = 0.19290516\n",
      "Iteration 3705, loss = 0.19258554\n",
      "Iteration 3706, loss = 0.19304698\n",
      "Iteration 3707, loss = 0.19278217\n",
      "Iteration 3708, loss = 0.19258610\n",
      "Iteration 3709, loss = 0.19239284\n",
      "Iteration 3710, loss = 0.19210747\n",
      "Iteration 3711, loss = 0.19188211\n",
      "Iteration 3712, loss = 0.19199603\n",
      "Iteration 3713, loss = 0.19200469\n",
      "Iteration 3714, loss = 0.19235434\n",
      "Iteration 3715, loss = 0.19178643\n",
      "Iteration 3716, loss = 0.19156977\n",
      "Iteration 3717, loss = 0.19217220\n",
      "Iteration 3718, loss = 0.19226760\n",
      "Iteration 3719, loss = 0.19242318\n",
      "Iteration 3720, loss = 0.19210326\n",
      "Iteration 3721, loss = 0.19163909\n",
      "Iteration 3722, loss = 0.19118843\n",
      "Iteration 3723, loss = 0.19088417\n",
      "Iteration 3724, loss = 0.19094494\n",
      "Iteration 3725, loss = 0.19101654\n",
      "Iteration 3726, loss = 0.19104986\n",
      "Iteration 3727, loss = 0.19116133\n",
      "Iteration 3728, loss = 0.19096546\n",
      "Iteration 3729, loss = 0.19055518\n",
      "Iteration 3730, loss = 0.19024968\n",
      "Iteration 3731, loss = 0.19046643\n",
      "Iteration 3732, loss = 0.19077684\n",
      "Iteration 3733, loss = 0.19098181\n",
      "Iteration 3734, loss = 0.19133187\n",
      "Iteration 3735, loss = 0.19119908\n",
      "Iteration 3736, loss = 0.19069860\n",
      "Iteration 3737, loss = 0.18990741\n",
      "Iteration 3738, loss = 0.18979135\n",
      "Iteration 3739, loss = 0.19035413\n",
      "Iteration 3740, loss = 0.19067374\n",
      "Iteration 3741, loss = 0.19093320\n",
      "Iteration 3742, loss = 0.19052079\n",
      "Iteration 3743, loss = 0.18959354\n",
      "Iteration 3744, loss = 0.18928011\n",
      "Iteration 3745, loss = 0.18898007\n",
      "Iteration 3746, loss = 0.18902682\n",
      "Iteration 3747, loss = 0.18915795\n",
      "Iteration 3748, loss = 0.18945255\n",
      "Iteration 3749, loss = 0.19013466\n",
      "Iteration 3750, loss = 0.18990121\n",
      "Iteration 3751, loss = 0.18962021\n",
      "Iteration 3752, loss = 0.18880884\n",
      "Iteration 3753, loss = 0.18895202\n",
      "Iteration 3754, loss = 0.18869567\n",
      "Iteration 3755, loss = 0.18868518\n",
      "Iteration 3756, loss = 0.18879234\n",
      "Iteration 3757, loss = 0.18873705\n",
      "Iteration 3758, loss = 0.18875547\n",
      "Iteration 3759, loss = 0.18866684\n",
      "Iteration 3760, loss = 0.18830525\n",
      "Iteration 3761, loss = 0.18815528\n",
      "Iteration 3762, loss = 0.18770794\n",
      "Iteration 3763, loss = 0.18760307\n",
      "Iteration 3764, loss = 0.18746990\n",
      "Iteration 3765, loss = 0.18765791\n",
      "Iteration 3766, loss = 0.18740620\n",
      "Iteration 3767, loss = 0.18717403\n",
      "Iteration 3768, loss = 0.18740107\n",
      "Iteration 3769, loss = 0.18721363\n",
      "Iteration 3770, loss = 0.18718808\n",
      "Iteration 3771, loss = 0.18736936\n",
      "Iteration 3772, loss = 0.18741190\n",
      "Iteration 3773, loss = 0.18744758\n",
      "Iteration 3774, loss = 0.18705479\n",
      "Iteration 3775, loss = 0.18731382\n",
      "Iteration 3776, loss = 0.18688405\n",
      "Iteration 3777, loss = 0.18680057\n",
      "Iteration 3778, loss = 0.18672451\n",
      "Iteration 3779, loss = 0.18640177\n",
      "Iteration 3780, loss = 0.18639291\n",
      "Iteration 3781, loss = 0.18629062\n",
      "Iteration 3782, loss = 0.18606658\n",
      "Iteration 3783, loss = 0.18615508\n",
      "Iteration 3784, loss = 0.18582382\n",
      "Iteration 3785, loss = 0.18567700\n",
      "Iteration 3786, loss = 0.18577951\n",
      "Iteration 3787, loss = 0.18590342\n",
      "Iteration 3788, loss = 0.18591902\n",
      "Iteration 3789, loss = 0.18587903\n",
      "Iteration 3790, loss = 0.18570710\n",
      "Iteration 3791, loss = 0.18553877\n",
      "Iteration 3792, loss = 0.18536441\n",
      "Iteration 3793, loss = 0.18518986\n",
      "Iteration 3794, loss = 0.18505745\n",
      "Iteration 3795, loss = 0.18500131\n",
      "Iteration 3796, loss = 0.18511227\n",
      "Iteration 3797, loss = 0.18488496\n",
      "Iteration 3798, loss = 0.18528450\n",
      "Iteration 3799, loss = 0.18589725\n",
      "Iteration 3800, loss = 0.18599641\n",
      "Iteration 3801, loss = 0.18556479\n",
      "Iteration 3802, loss = 0.18518523\n",
      "Iteration 3803, loss = 0.18470472\n",
      "Iteration 3804, loss = 0.18440470\n",
      "Iteration 3805, loss = 0.18439112\n",
      "Iteration 3806, loss = 0.18459350\n",
      "Iteration 3807, loss = 0.18459843\n",
      "Iteration 3808, loss = 0.18446233\n",
      "Iteration 3809, loss = 0.18443543\n",
      "Iteration 3810, loss = 0.18394954\n",
      "Iteration 3811, loss = 0.18384823\n",
      "Iteration 3812, loss = 0.18346174\n",
      "Iteration 3813, loss = 0.18363242\n",
      "Iteration 3814, loss = 0.18390265\n",
      "Iteration 3815, loss = 0.18383697\n",
      "Iteration 3816, loss = 0.18363531\n",
      "Iteration 3817, loss = 0.18339266\n",
      "Iteration 3818, loss = 0.18349656\n",
      "Iteration 3819, loss = 0.18329012\n",
      "Iteration 3820, loss = 0.18327486\n",
      "Iteration 3821, loss = 0.18328522\n",
      "Iteration 3822, loss = 0.18327593\n",
      "Iteration 3823, loss = 0.18295269\n",
      "Iteration 3824, loss = 0.18286338\n",
      "Iteration 3825, loss = 0.18256657\n",
      "Iteration 3826, loss = 0.18293918\n",
      "Iteration 3827, loss = 0.18274201\n",
      "Iteration 3828, loss = 0.18256662\n",
      "Iteration 3829, loss = 0.18283100\n",
      "Iteration 3830, loss = 0.18292420\n",
      "Iteration 3831, loss = 0.18278868\n",
      "Iteration 3832, loss = 0.18250484\n",
      "Iteration 3833, loss = 0.18229407\n",
      "Iteration 3834, loss = 0.18199547\n",
      "Iteration 3835, loss = 0.18162696\n",
      "Iteration 3836, loss = 0.18191553\n",
      "Iteration 3837, loss = 0.18221782\n",
      "Iteration 3838, loss = 0.18269566\n",
      "Iteration 3839, loss = 0.18267966\n",
      "Iteration 3840, loss = 0.18232944\n",
      "Iteration 3841, loss = 0.18160372\n",
      "Iteration 3842, loss = 0.18103425\n",
      "Iteration 3843, loss = 0.18121874\n",
      "Iteration 3844, loss = 0.18204100\n",
      "Iteration 3845, loss = 0.18216838\n",
      "Iteration 3846, loss = 0.18195844\n",
      "Iteration 3847, loss = 0.18137156\n",
      "Iteration 3848, loss = 0.18091483\n",
      "Iteration 3849, loss = 0.18100763\n",
      "Iteration 3850, loss = 0.18158694\n",
      "Iteration 3851, loss = 0.18163796\n",
      "Iteration 3852, loss = 0.18131991\n",
      "Iteration 3853, loss = 0.18129601\n",
      "Iteration 3854, loss = 0.18051243\n",
      "Iteration 3855, loss = 0.18018431\n",
      "Iteration 3856, loss = 0.18036528\n",
      "Iteration 3857, loss = 0.18065447\n",
      "Iteration 3858, loss = 0.18043459\n",
      "Iteration 3859, loss = 0.18032360\n",
      "Iteration 3860, loss = 0.17987346\n",
      "Iteration 3861, loss = 0.17992081\n",
      "Iteration 3862, loss = 0.17967963\n",
      "Iteration 3863, loss = 0.17975948\n",
      "Iteration 3864, loss = 0.17957154\n",
      "Iteration 3865, loss = 0.17948591\n",
      "Iteration 3866, loss = 0.17947328\n",
      "Iteration 3867, loss = 0.17940042\n",
      "Iteration 3868, loss = 0.17944800\n",
      "Iteration 3869, loss = 0.17934549\n",
      "Iteration 3870, loss = 0.17906106\n",
      "Iteration 3871, loss = 0.17895010\n",
      "Iteration 3872, loss = 0.17886410\n",
      "Iteration 3873, loss = 0.17867721\n",
      "Iteration 3874, loss = 0.17870600\n",
      "Iteration 3875, loss = 0.17874139\n",
      "Iteration 3876, loss = 0.17923233\n",
      "Iteration 3877, loss = 0.17895924\n",
      "Iteration 3878, loss = 0.17873974\n",
      "Iteration 3879, loss = 0.17861028\n",
      "Iteration 3880, loss = 0.17837372\n",
      "Iteration 3881, loss = 0.17812559\n",
      "Iteration 3882, loss = 0.17814975\n",
      "Iteration 3883, loss = 0.17834879\n",
      "Iteration 3884, loss = 0.17804021\n",
      "Iteration 3885, loss = 0.17783022\n",
      "Iteration 3886, loss = 0.17763374\n",
      "Iteration 3887, loss = 0.17780337\n",
      "Iteration 3888, loss = 0.17766724\n",
      "Iteration 3889, loss = 0.17755218\n",
      "Iteration 3890, loss = 0.17758654\n",
      "Iteration 3891, loss = 0.17748417\n",
      "Iteration 3892, loss = 0.17778892\n",
      "Iteration 3893, loss = 0.17732691\n",
      "Iteration 3894, loss = 0.17724595\n",
      "Iteration 3895, loss = 0.17723509\n",
      "Iteration 3896, loss = 0.17712895\n",
      "Iteration 3897, loss = 0.17715101\n",
      "Iteration 3898, loss = 0.17705536\n",
      "Iteration 3899, loss = 0.17668727\n",
      "Iteration 3900, loss = 0.17668023\n",
      "Iteration 3901, loss = 0.17695681\n",
      "Iteration 3902, loss = 0.17722919\n",
      "Iteration 3903, loss = 0.17757602\n",
      "Iteration 3904, loss = 0.17771241\n",
      "Iteration 3905, loss = 0.17745348\n",
      "Iteration 3906, loss = 0.17685282\n",
      "Iteration 3907, loss = 0.17662829\n",
      "Iteration 3908, loss = 0.17615218\n",
      "Iteration 3909, loss = 0.17620862\n",
      "Iteration 3910, loss = 0.17625251\n",
      "Iteration 3911, loss = 0.17645449\n",
      "Iteration 3912, loss = 0.17639724\n",
      "Iteration 3913, loss = 0.17646592\n",
      "Iteration 3914, loss = 0.17599198\n",
      "Iteration 3915, loss = 0.17592618\n",
      "Iteration 3916, loss = 0.17548263\n",
      "Iteration 3917, loss = 0.17552835\n",
      "Iteration 3918, loss = 0.17536168\n",
      "Iteration 3919, loss = 0.17538352\n",
      "Iteration 3920, loss = 0.17527024\n",
      "Iteration 3921, loss = 0.17527903\n",
      "Iteration 3922, loss = 0.17516069\n",
      "Iteration 3923, loss = 0.17511648\n",
      "Iteration 3924, loss = 0.17512717\n",
      "Iteration 3925, loss = 0.17509278\n",
      "Iteration 3926, loss = 0.17508526\n",
      "Iteration 3927, loss = 0.17532237\n",
      "Iteration 3928, loss = 0.17510916\n",
      "Iteration 3929, loss = 0.17480308\n",
      "Iteration 3930, loss = 0.17462051\n",
      "Iteration 3931, loss = 0.17471567\n",
      "Iteration 3932, loss = 0.17451768\n",
      "Iteration 3933, loss = 0.17481801\n",
      "Iteration 3934, loss = 0.17502151\n",
      "Iteration 3935, loss = 0.17517555\n",
      "Iteration 3936, loss = 0.17494972\n",
      "Iteration 3937, loss = 0.17472102\n",
      "Iteration 3938, loss = 0.17429019\n",
      "Iteration 3939, loss = 0.17397778\n",
      "Iteration 3940, loss = 0.17385617\n",
      "Iteration 3941, loss = 0.17364173\n",
      "Iteration 3942, loss = 0.17365300\n",
      "Iteration 3943, loss = 0.17362588\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3944, loss = 0.17351508\n",
      "Iteration 3945, loss = 0.17340643\n",
      "Iteration 3946, loss = 0.17356128\n",
      "Iteration 3947, loss = 0.17366685\n",
      "Iteration 3948, loss = 0.17400282\n",
      "Iteration 3949, loss = 0.17375834\n",
      "Iteration 3950, loss = 0.17341964\n",
      "Iteration 3951, loss = 0.17285876\n",
      "Iteration 3952, loss = 0.17276473\n",
      "Iteration 3953, loss = 0.17277499\n",
      "Iteration 3954, loss = 0.17289889\n",
      "Iteration 3955, loss = 0.17317558\n",
      "Iteration 3956, loss = 0.17318344\n",
      "Iteration 3957, loss = 0.17285568\n",
      "Iteration 3958, loss = 0.17260000\n",
      "Iteration 3959, loss = 0.17265488\n",
      "Iteration 3960, loss = 0.17230745\n",
      "Iteration 3961, loss = 0.17201990\n",
      "Iteration 3962, loss = 0.17200748\n",
      "Iteration 3963, loss = 0.17232156\n",
      "Iteration 3964, loss = 0.17230706\n",
      "Iteration 3965, loss = 0.17211145\n",
      "Iteration 3966, loss = 0.17229230\n",
      "Iteration 3967, loss = 0.17221537\n",
      "Iteration 3968, loss = 0.17254049\n",
      "Iteration 3969, loss = 0.17287977\n",
      "Iteration 3970, loss = 0.17294640\n",
      "Iteration 3971, loss = 0.17248216\n",
      "Iteration 3972, loss = 0.17194970\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=600000, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=True, warm_start=False)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## NEURAL NETWORK\n",
    "mlp = MLPClassifier(verbose=True, max_iter=600000)\n",
    "mlp.fit(train_data, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ANOTHER NEURAL NETWORK\n",
    "model = Sequential([\n",
    "    Dense(20, activation = 'relu'),\n",
    "    Dense(3, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "263/263 [==============================] - 0s 2ms/sample - loss: 0.7884 - acc: 0.5133\n",
      "Epoch 2/200\n",
      "263/263 [==============================] - 0s 83us/sample - loss: 0.7848 - acc: 0.5437\n",
      "Epoch 3/200\n",
      "263/263 [==============================] - 0s 68us/sample - loss: 0.7822 - acc: 0.5779\n",
      "Epoch 4/200\n",
      "263/263 [==============================] - 0s 83us/sample - loss: 0.7792 - acc: 0.5817\n",
      "Epoch 5/200\n",
      "263/263 [==============================] - 0s 65us/sample - loss: 0.7768 - acc: 0.5932\n",
      "Epoch 6/200\n",
      "263/263 [==============================] - 0s 76us/sample - loss: 0.7739 - acc: 0.6122\n",
      "Epoch 7/200\n",
      "263/263 [==============================] - 0s 80us/sample - loss: 0.7714 - acc: 0.6122\n",
      "Epoch 8/200\n",
      "263/263 [==============================] - 0s 80us/sample - loss: 0.7688 - acc: 0.6274\n",
      "Epoch 9/200\n",
      "263/263 [==============================] - 0s 80us/sample - loss: 0.7658 - acc: 0.6464\n",
      "Epoch 10/200\n",
      "263/263 [==============================] - 0s 72us/sample - loss: 0.7624 - acc: 0.6882\n",
      "Epoch 11/200\n",
      "263/263 [==============================] - 0s 80us/sample - loss: 0.7591 - acc: 0.7072\n",
      "Epoch 12/200\n",
      "263/263 [==============================] - 0s 95us/sample - loss: 0.7561 - acc: 0.7186\n",
      "Epoch 13/200\n",
      "263/263 [==============================] - 0s 76us/sample - loss: 0.7529 - acc: 0.7262\n",
      "Epoch 14/200\n",
      "263/263 [==============================] - 0s 87us/sample - loss: 0.7497 - acc: 0.7148\n",
      "Epoch 15/200\n",
      "263/263 [==============================] - 0s 76us/sample - loss: 0.7465 - acc: 0.6920\n",
      "Epoch 16/200\n",
      "263/263 [==============================] - 0s 76us/sample - loss: 0.7429 - acc: 0.6882\n",
      "Epoch 17/200\n",
      "263/263 [==============================] - 0s 83us/sample - loss: 0.7391 - acc: 0.6882\n",
      "Epoch 18/200\n",
      "263/263 [==============================] - 0s 83us/sample - loss: 0.7354 - acc: 0.6920\n",
      "Epoch 19/200\n",
      "263/263 [==============================] - 0s 80us/sample - loss: 0.7315 - acc: 0.7110\n",
      "Epoch 20/200\n",
      "263/263 [==============================] - 0s 91us/sample - loss: 0.7279 - acc: 0.6996\n",
      "Epoch 21/200\n",
      "263/263 [==============================] - 0s 83us/sample - loss: 0.7235 - acc: 0.7072\n",
      "Epoch 22/200\n",
      "263/263 [==============================] - 0s 83us/sample - loss: 0.7194 - acc: 0.7072\n",
      "Epoch 23/200\n",
      "263/263 [==============================] - 0s 80us/sample - loss: 0.7143 - acc: 0.6996\n",
      "Epoch 24/200\n",
      "263/263 [==============================] - 0s 91us/sample - loss: 0.7100 - acc: 0.7072\n",
      "Epoch 25/200\n",
      "263/263 [==============================] - 0s 78us/sample - loss: 0.7054 - acc: 0.7034\n",
      "Epoch 26/200\n",
      "263/263 [==============================] - 0s 87us/sample - loss: 0.7004 - acc: 0.7224\n",
      "Epoch 27/200\n",
      "263/263 [==============================] - 0s 95us/sample - loss: 0.6956 - acc: 0.7224\n",
      "Epoch 28/200\n",
      "263/263 [==============================] - 0s 83us/sample - loss: 0.6907 - acc: 0.7186\n",
      "Epoch 29/200\n",
      "263/263 [==============================] - 0s 106us/sample - loss: 0.6853 - acc: 0.7186\n",
      "Epoch 30/200\n",
      "263/263 [==============================] - 0s 114us/sample - loss: 0.6802 - acc: 0.7338\n",
      "Epoch 31/200\n",
      "263/263 [==============================] - 0s 121us/sample - loss: 0.6745 - acc: 0.7224\n",
      "Epoch 32/200\n",
      "263/263 [==============================] - 0s 87us/sample - loss: 0.6693 - acc: 0.7224\n",
      "Epoch 33/200\n",
      "263/263 [==============================] - 0s 87us/sample - loss: 0.6647 - acc: 0.7452\n",
      "Epoch 34/200\n",
      "263/263 [==============================] - 0s 83us/sample - loss: 0.6589 - acc: 0.7414\n",
      "Epoch 35/200\n",
      "263/263 [==============================] - 0s 110us/sample - loss: 0.6533 - acc: 0.7376\n",
      "Epoch 36/200\n",
      "263/263 [==============================] - 0s 83us/sample - loss: 0.6479 - acc: 0.7490\n",
      "Epoch 37/200\n",
      "263/263 [==============================] - 0s 83us/sample - loss: 0.6439 - acc: 0.7452\n",
      "Epoch 38/200\n",
      "263/263 [==============================] - 0s 83us/sample - loss: 0.6381 - acc: 0.7414\n",
      "Epoch 39/200\n",
      "263/263 [==============================] - 0s 80us/sample - loss: 0.6326 - acc: 0.7376\n",
      "Epoch 40/200\n",
      "263/263 [==============================] - 0s 83us/sample - loss: 0.6267 - acc: 0.7300\n",
      "Epoch 41/200\n",
      "263/263 [==============================] - 0s 80us/sample - loss: 0.6216 - acc: 0.7338\n",
      "Epoch 42/200\n",
      "263/263 [==============================] - 0s 78us/sample - loss: 0.6162 - acc: 0.7338\n",
      "Epoch 43/200\n",
      "263/263 [==============================] - 0s 76us/sample - loss: 0.6105 - acc: 0.7376\n",
      "Epoch 44/200\n",
      "263/263 [==============================] - 0s 83us/sample - loss: 0.6052 - acc: 0.7452\n",
      "Epoch 45/200\n",
      "263/263 [==============================] - 0s 76us/sample - loss: 0.5997 - acc: 0.7376\n",
      "Epoch 46/200\n",
      "263/263 [==============================] - 0s 93us/sample - loss: 0.5945 - acc: 0.7338\n",
      "Epoch 47/200\n",
      "263/263 [==============================] - 0s 83us/sample - loss: 0.5892 - acc: 0.7529\n",
      "Epoch 48/200\n",
      "263/263 [==============================] - 0s 72us/sample - loss: 0.5846 - acc: 0.7567\n",
      "Epoch 49/200\n",
      "263/263 [==============================] - 0s 83us/sample - loss: 0.5797 - acc: 0.7567\n",
      "Epoch 50/200\n",
      "263/263 [==============================] - 0s 76us/sample - loss: 0.5748 - acc: 0.7605\n",
      "Epoch 51/200\n",
      "263/263 [==============================] - 0s 87us/sample - loss: 0.5695 - acc: 0.7643\n",
      "Epoch 52/200\n",
      "263/263 [==============================] - 0s 80us/sample - loss: 0.5645 - acc: 0.7529\n",
      "Epoch 53/200\n",
      "263/263 [==============================] - 0s 95us/sample - loss: 0.5592 - acc: 0.7490\n",
      "Epoch 54/200\n",
      "263/263 [==============================] - 0s 76us/sample - loss: 0.5545 - acc: 0.7529\n",
      "Epoch 55/200\n",
      "263/263 [==============================] - 0s 102us/sample - loss: 0.5492 - acc: 0.7605\n",
      "Epoch 56/200\n",
      "263/263 [==============================] - 0s 99us/sample - loss: 0.5452 - acc: 0.7643\n",
      "Epoch 57/200\n",
      "263/263 [==============================] - 0s 87us/sample - loss: 0.5412 - acc: 0.7643\n",
      "Epoch 58/200\n",
      "263/263 [==============================] - 0s 80us/sample - loss: 0.5367 - acc: 0.7567\n",
      "Epoch 59/200\n",
      "263/263 [==============================] - 0s 87us/sample - loss: 0.5316 - acc: 0.7567\n",
      "Epoch 60/200\n",
      "263/263 [==============================] - 0s 80us/sample - loss: 0.5272 - acc: 0.7719\n",
      "Epoch 61/200\n",
      "263/263 [==============================] - 0s 72us/sample - loss: 0.5229 - acc: 0.7681\n",
      "Epoch 62/200\n",
      "263/263 [==============================] - 0s 72us/sample - loss: 0.5196 - acc: 0.7681\n",
      "Epoch 63/200\n",
      "263/263 [==============================] - 0s 76us/sample - loss: 0.5147 - acc: 0.7643\n",
      "Epoch 64/200\n",
      "263/263 [==============================] - 0s 76us/sample - loss: 0.5105 - acc: 0.7757\n",
      "Epoch 65/200\n",
      "263/263 [==============================] - 0s 87us/sample - loss: 0.5063 - acc: 0.7681\n",
      "Epoch 66/200\n",
      "263/263 [==============================] - 0s 72us/sample - loss: 0.5025 - acc: 0.7719\n",
      "Epoch 67/200\n",
      "263/263 [==============================] - 0s 80us/sample - loss: 0.4993 - acc: 0.7909\n",
      "Epoch 68/200\n",
      "263/263 [==============================] - 0s 80us/sample - loss: 0.4953 - acc: 0.8099\n",
      "Epoch 69/200\n",
      "263/263 [==============================] - 0s 76us/sample - loss: 0.4910 - acc: 0.8023\n",
      "Epoch 70/200\n",
      "263/263 [==============================] - 0s 76us/sample - loss: 0.4871 - acc: 0.8023\n",
      "Epoch 71/200\n",
      "263/263 [==============================] - 0s 80us/sample - loss: 0.4843 - acc: 0.7871\n",
      "Epoch 72/200\n",
      "263/263 [==============================] - 0s 80us/sample - loss: 0.4799 - acc: 0.7947\n",
      "Epoch 73/200\n",
      "263/263 [==============================] - 0s 80us/sample - loss: 0.4763 - acc: 0.7909\n",
      "Epoch 74/200\n",
      "263/263 [==============================] - 0s 83us/sample - loss: 0.4730 - acc: 0.7985\n",
      "Epoch 75/200\n",
      "263/263 [==============================] - 0s 83us/sample - loss: 0.4690 - acc: 0.7947\n",
      "Epoch 76/200\n",
      "263/263 [==============================] - 0s 80us/sample - loss: 0.4662 - acc: 0.7985\n",
      "Epoch 77/200\n",
      "263/263 [==============================] - 0s 80us/sample - loss: 0.4627 - acc: 0.8023\n",
      "Epoch 78/200\n",
      "263/263 [==============================] - 0s 76us/sample - loss: 0.4605 - acc: 0.7909\n",
      "Epoch 79/200\n",
      "263/263 [==============================] - 0s 83us/sample - loss: 0.4574 - acc: 0.8137\n",
      "Epoch 80/200\n",
      "263/263 [==============================] - 0s 76us/sample - loss: 0.4534 - acc: 0.8099\n",
      "Epoch 81/200\n",
      "263/263 [==============================] - 0s 80us/sample - loss: 0.4498 - acc: 0.8137\n",
      "Epoch 82/200\n",
      "263/263 [==============================] - 0s 76us/sample - loss: 0.4475 - acc: 0.8099\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/200\n",
      "263/263 [==============================] - 0s 76us/sample - loss: 0.4442 - acc: 0.8099\n",
      "Epoch 84/200\n",
      "263/263 [==============================] - 0s 87us/sample - loss: 0.4414 - acc: 0.8099\n",
      "Epoch 85/200\n",
      "263/263 [==============================] - 0s 66us/sample - loss: 0.4390 - acc: 0.8061\n",
      "Epoch 86/200\n",
      "263/263 [==============================] - 0s 68us/sample - loss: 0.4371 - acc: 0.8137\n",
      "Epoch 87/200\n",
      "263/263 [==============================] - 0s 76us/sample - loss: 0.4327 - acc: 0.8175\n",
      "Epoch 88/200\n",
      "263/263 [==============================] - 0s 68us/sample - loss: 0.4292 - acc: 0.8175\n",
      "Epoch 89/200\n",
      "263/263 [==============================] - 0s 68us/sample - loss: 0.4269 - acc: 0.8213\n",
      "Epoch 90/200\n",
      "263/263 [==============================] - 0s 72us/sample - loss: 0.4249 - acc: 0.8175\n",
      "Epoch 91/200\n",
      "263/263 [==============================] - 0s 72us/sample - loss: 0.4224 - acc: 0.8251\n",
      "Epoch 92/200\n",
      "263/263 [==============================] - 0s 61us/sample - loss: 0.4210 - acc: 0.8213\n",
      "Epoch 93/200\n",
      "263/263 [==============================] - 0s 68us/sample - loss: 0.4179 - acc: 0.8251\n",
      "Epoch 94/200\n",
      "263/263 [==============================] - 0s 68us/sample - loss: 0.4145 - acc: 0.8213\n",
      "Epoch 95/200\n",
      "263/263 [==============================] - 0s 72us/sample - loss: 0.4126 - acc: 0.8251\n",
      "Epoch 96/200\n",
      "263/263 [==============================] - 0s 68us/sample - loss: 0.4101 - acc: 0.8327\n",
      "Epoch 97/200\n",
      "263/263 [==============================] - 0s 72us/sample - loss: 0.4076 - acc: 0.8251\n",
      "Epoch 98/200\n",
      "263/263 [==============================] - 0s 72us/sample - loss: 0.4047 - acc: 0.8213\n",
      "Epoch 99/200\n",
      "263/263 [==============================] - 0s 72us/sample - loss: 0.4023 - acc: 0.8213\n",
      "Epoch 100/200\n",
      "263/263 [==============================] - ETA: 0s - loss: 0.5070 - acc: 0.875 - 0s 72us/sample - loss: 0.4003 - acc: 0.8289\n",
      "Epoch 101/200\n",
      "263/263 [==============================] - 0s 83us/sample - loss: 0.3982 - acc: 0.8327\n",
      "Epoch 102/200\n",
      "263/263 [==============================] - 0s 72us/sample - loss: 0.3962 - acc: 0.8175\n",
      "Epoch 103/200\n",
      "263/263 [==============================] - 0s 76us/sample - loss: 0.3939 - acc: 0.8137\n",
      "Epoch 104/200\n",
      "263/263 [==============================] - 0s 68us/sample - loss: 0.3918 - acc: 0.8175\n",
      "Epoch 105/200\n",
      "263/263 [==============================] - 0s 76us/sample - loss: 0.3894 - acc: 0.8213\n",
      "Epoch 106/200\n",
      "263/263 [==============================] - 0s 76us/sample - loss: 0.3874 - acc: 0.8251\n",
      "Epoch 107/200\n",
      "263/263 [==============================] - 0s 68us/sample - loss: 0.3851 - acc: 0.8289\n",
      "Epoch 108/200\n",
      "263/263 [==============================] - 0s 70us/sample - loss: 0.3834 - acc: 0.8403\n",
      "Epoch 109/200\n",
      "263/263 [==============================] - 0s 64us/sample - loss: 0.3811 - acc: 0.8213\n",
      "Epoch 110/200\n",
      "263/263 [==============================] - 0s 72us/sample - loss: 0.3795 - acc: 0.8175\n",
      "Epoch 111/200\n",
      "263/263 [==============================] - 0s 72us/sample - loss: 0.3780 - acc: 0.8137\n",
      "Epoch 112/200\n",
      "263/263 [==============================] - 0s 68us/sample - loss: 0.3754 - acc: 0.8061\n",
      "Epoch 113/200\n",
      "263/263 [==============================] - 0s 70us/sample - loss: 0.3739 - acc: 0.8023\n",
      "Epoch 114/200\n",
      "263/263 [==============================] - 0s 68us/sample - loss: 0.3715 - acc: 0.8099\n",
      "Epoch 115/200\n",
      "263/263 [==============================] - 0s 72us/sample - loss: 0.3700 - acc: 0.8175\n",
      "Epoch 116/200\n",
      "263/263 [==============================] - 0s 72us/sample - loss: 0.3677 - acc: 0.8289\n",
      "Epoch 117/200\n",
      "263/263 [==============================] - 0s 72us/sample - loss: 0.3659 - acc: 0.8365\n",
      "Epoch 118/200\n",
      "263/263 [==============================] - 0s 80us/sample - loss: 0.3647 - acc: 0.8479\n",
      "Epoch 119/200\n",
      "263/263 [==============================] - 0s 72us/sample - loss: 0.3628 - acc: 0.8517\n",
      "Epoch 120/200\n",
      "263/263 [==============================] - 0s 87us/sample - loss: 0.3608 - acc: 0.8441\n",
      "Epoch 121/200\n",
      "263/263 [==============================] - 0s 110us/sample - loss: 0.3587 - acc: 0.8327\n",
      "Epoch 122/200\n",
      "263/263 [==============================] - 0s 102us/sample - loss: 0.3571 - acc: 0.8251\n",
      "Epoch 123/200\n",
      "263/263 [==============================] - 0s 99us/sample - loss: 0.3558 - acc: 0.8175\n",
      "Epoch 124/200\n",
      "263/263 [==============================] - 0s 72us/sample - loss: 0.3539 - acc: 0.8099\n",
      "Epoch 125/200\n",
      "263/263 [==============================] - 0s 80us/sample - loss: 0.3518 - acc: 0.8213\n",
      "Epoch 126/200\n",
      "263/263 [==============================] - 0s 72us/sample - loss: 0.3512 - acc: 0.8213\n",
      "Epoch 127/200\n",
      "263/263 [==============================] - 0s 80us/sample - loss: 0.3484 - acc: 0.8251\n",
      "Epoch 128/200\n",
      "263/263 [==============================] - 0s 72us/sample - loss: 0.3472 - acc: 0.8289\n",
      "Epoch 129/200\n",
      "263/263 [==============================] - 0s 80us/sample - loss: 0.3462 - acc: 0.8327\n",
      "Epoch 130/200\n",
      "263/263 [==============================] - 0s 83us/sample - loss: 0.3444 - acc: 0.8289\n",
      "Epoch 131/200\n",
      "263/263 [==============================] - 0s 76us/sample - loss: 0.3428 - acc: 0.8289\n",
      "Epoch 132/200\n",
      "263/263 [==============================] - 0s 72us/sample - loss: 0.3416 - acc: 0.8479\n",
      "Epoch 133/200\n",
      "263/263 [==============================] - 0s 76us/sample - loss: 0.3399 - acc: 0.8479\n",
      "Epoch 134/200\n",
      "263/263 [==============================] - 0s 72us/sample - loss: 0.3381 - acc: 0.8555\n",
      "Epoch 135/200\n",
      "263/263 [==============================] - 0s 68us/sample - loss: 0.3370 - acc: 0.8441\n",
      "Epoch 136/200\n",
      "263/263 [==============================] - 0s 68us/sample - loss: 0.3353 - acc: 0.8251\n",
      "Epoch 137/200\n",
      "263/263 [==============================] - 0s 72us/sample - loss: 0.3337 - acc: 0.8213\n",
      "Epoch 138/200\n",
      "263/263 [==============================] - 0s 91us/sample - loss: 0.3323 - acc: 0.8289\n",
      "Epoch 139/200\n",
      "263/263 [==============================] - 0s 83us/sample - loss: 0.3316 - acc: 0.8251\n",
      "Epoch 140/200\n",
      "263/263 [==============================] - 0s 83us/sample - loss: 0.3303 - acc: 0.8289\n",
      "Epoch 141/200\n",
      "263/263 [==============================] - 0s 87us/sample - loss: 0.3279 - acc: 0.8289\n",
      "Epoch 142/200\n",
      "263/263 [==============================] - 0s 83us/sample - loss: 0.3268 - acc: 0.8289\n",
      "Epoch 143/200\n",
      "263/263 [==============================] - 0s 106us/sample - loss: 0.3252 - acc: 0.8289\n",
      "Epoch 144/200\n",
      "263/263 [==============================] - 0s 91us/sample - loss: 0.3238 - acc: 0.8289\n",
      "Epoch 145/200\n",
      "263/263 [==============================] - 0s 83us/sample - loss: 0.3224 - acc: 0.8251\n",
      "Epoch 146/200\n",
      "263/263 [==============================] - 0s 83us/sample - loss: 0.3209 - acc: 0.8251\n",
      "Epoch 147/200\n",
      "263/263 [==============================] - 0s 110us/sample - loss: 0.3199 - acc: 0.8327\n",
      "Epoch 148/200\n",
      "263/263 [==============================] - 0s 116us/sample - loss: 0.3181 - acc: 0.8289\n",
      "Epoch 149/200\n",
      "263/263 [==============================] - 0s 99us/sample - loss: 0.3173 - acc: 0.8251\n",
      "Epoch 150/200\n",
      "263/263 [==============================] - 0s 87us/sample - loss: 0.3164 - acc: 0.8251\n",
      "Epoch 151/200\n",
      "263/263 [==============================] - 0s 91us/sample - loss: 0.3152 - acc: 0.8289\n",
      "Epoch 152/200\n",
      "263/263 [==============================] - 0s 91us/sample - loss: 0.3136 - acc: 0.8289\n",
      "Epoch 153/200\n",
      "263/263 [==============================] - 0s 95us/sample - loss: 0.3124 - acc: 0.8289\n",
      "Epoch 154/200\n",
      "263/263 [==============================] - 0s 99us/sample - loss: 0.3111 - acc: 0.8289\n",
      "Epoch 155/200\n",
      "263/263 [==============================] - 0s 102us/sample - loss: 0.3098 - acc: 0.8289\n",
      "Epoch 156/200\n",
      "263/263 [==============================] - 0s 83us/sample - loss: 0.3087 - acc: 0.8251\n",
      "Epoch 157/200\n",
      "263/263 [==============================] - 0s 87us/sample - loss: 0.3076 - acc: 0.8289\n",
      "Epoch 158/200\n",
      "263/263 [==============================] - 0s 102us/sample - loss: 0.3065 - acc: 0.8289\n",
      "Epoch 159/200\n",
      "263/263 [==============================] - ETA: 0s - loss: 0.2817 - acc: 0.843 - 0s 95us/sample - loss: 0.3051 - acc: 0.8251\n",
      "Epoch 160/200\n",
      "263/263 [==============================] - 0s 99us/sample - loss: 0.3040 - acc: 0.8289\n",
      "Epoch 161/200\n",
      "263/263 [==============================] - 0s 106us/sample - loss: 0.3032 - acc: 0.8251\n",
      "Epoch 162/200\n",
      "263/263 [==============================] - 0s 83us/sample - loss: 0.3030 - acc: 0.8289\n",
      "Epoch 163/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263/263 [==============================] - 0s 95us/sample - loss: 0.3007 - acc: 0.8327\n",
      "Epoch 164/200\n",
      "263/263 [==============================] - 0s 114us/sample - loss: 0.2991 - acc: 0.8327\n",
      "Epoch 165/200\n",
      "263/263 [==============================] - 0s 110us/sample - loss: 0.2986 - acc: 0.8251\n",
      "Epoch 166/200\n",
      "263/263 [==============================] - 0s 95us/sample - loss: 0.2981 - acc: 0.8213\n",
      "Epoch 167/200\n",
      "263/263 [==============================] - 0s 91us/sample - loss: 0.2971 - acc: 0.8213\n",
      "Epoch 168/200\n",
      "263/263 [==============================] - 0s 87us/sample - loss: 0.2958 - acc: 0.8251\n",
      "Epoch 169/200\n",
      "263/263 [==============================] - 0s 102us/sample - loss: 0.2945 - acc: 0.8251\n",
      "Epoch 170/200\n",
      "263/263 [==============================] - 0s 99us/sample - loss: 0.2931 - acc: 0.8251\n",
      "Epoch 171/200\n",
      "263/263 [==============================] - 0s 76us/sample - loss: 0.2921 - acc: 0.8251\n",
      "Epoch 172/200\n",
      "263/263 [==============================] - 0s 99us/sample - loss: 0.2917 - acc: 0.8251\n",
      "Epoch 173/200\n",
      "263/263 [==============================] - 0s 83us/sample - loss: 0.2911 - acc: 0.8251\n",
      "Epoch 174/200\n",
      "263/263 [==============================] - 0s 76us/sample - loss: 0.2894 - acc: 0.8251\n",
      "Epoch 175/200\n",
      "263/263 [==============================] - 0s 102us/sample - loss: 0.2882 - acc: 0.8327\n",
      "Epoch 176/200\n",
      "263/263 [==============================] - 0s 83us/sample - loss: 0.2868 - acc: 0.8327\n",
      "Epoch 177/200\n",
      "263/263 [==============================] - 0s 83us/sample - loss: 0.2857 - acc: 0.8289\n",
      "Epoch 178/200\n",
      "263/263 [==============================] - 0s 95us/sample - loss: 0.2855 - acc: 0.8289\n",
      "Epoch 179/200\n",
      "263/263 [==============================] - 0s 99us/sample - loss: 0.2838 - acc: 0.8327\n",
      "Epoch 180/200\n",
      "263/263 [==============================] - 0s 87us/sample - loss: 0.2828 - acc: 0.8289\n",
      "Epoch 181/200\n",
      "263/263 [==============================] - 0s 80us/sample - loss: 0.2821 - acc: 0.8327\n",
      "Epoch 182/200\n",
      "263/263 [==============================] - 0s 91us/sample - loss: 0.2815 - acc: 0.8289\n",
      "Epoch 183/200\n",
      "263/263 [==============================] - 0s 102us/sample - loss: 0.2804 - acc: 0.8289\n",
      "Epoch 184/200\n",
      "263/263 [==============================] - 0s 87us/sample - loss: 0.2799 - acc: 0.8289\n",
      "Epoch 185/200\n",
      "263/263 [==============================] - 0s 91us/sample - loss: 0.2786 - acc: 0.8251\n",
      "Epoch 186/200\n",
      "263/263 [==============================] - 0s 87us/sample - loss: 0.2778 - acc: 0.8365\n",
      "Epoch 187/200\n",
      "263/263 [==============================] - 0s 91us/sample - loss: 0.2767 - acc: 0.8251\n",
      "Epoch 188/200\n",
      "263/263 [==============================] - 0s 87us/sample - loss: 0.2755 - acc: 0.8289\n",
      "Epoch 189/200\n",
      "263/263 [==============================] - 0s 87us/sample - loss: 0.2743 - acc: 0.8289\n",
      "Epoch 190/200\n",
      "263/263 [==============================] - 0s 99us/sample - loss: 0.2742 - acc: 0.8251\n",
      "Epoch 191/200\n",
      "263/263 [==============================] - 0s 80us/sample - loss: 0.2725 - acc: 0.8289\n",
      "Epoch 192/200\n",
      "263/263 [==============================] - 0s 91us/sample - loss: 0.2720 - acc: 0.8289\n",
      "Epoch 193/200\n",
      "263/263 [==============================] - 0s 106us/sample - loss: 0.2711 - acc: 0.8289\n",
      "Epoch 194/200\n",
      "263/263 [==============================] - 0s 83us/sample - loss: 0.2700 - acc: 0.8365\n",
      "Epoch 195/200\n",
      "263/263 [==============================] - 0s 87us/sample - loss: 0.2697 - acc: 0.8365\n",
      "Epoch 196/200\n",
      "263/263 [==============================] - 0s 83us/sample - loss: 0.2691 - acc: 0.8327\n",
      "Epoch 197/200\n",
      "263/263 [==============================] - 0s 83us/sample - loss: 0.2677 - acc: 0.8403\n",
      "Epoch 198/200\n",
      "263/263 [==============================] - 0s 99us/sample - loss: 0.2673 - acc: 0.8403\n",
      "Epoch 199/200\n",
      "263/263 [==============================] - 0s 114us/sample - loss: 0.2666 - acc: 0.8327\n",
      "Epoch 200/200\n",
      "263/263 [==============================] - 0s 114us/sample - loss: 0.2659 - acc: 0.8327\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(np.array(train_data), np.array(train_labels), epochs=200)#### CHANGE PARAMS DUE TO ACC\\LOSS GRAPH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graphs(history, string):\n",
    "    plt.plot(history.history[string])\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(string)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_graphs(history, 'acc')\n",
    "plot_graphs(history, 'loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_serg_test = 'F://test_serg/'\n",
    "path_nick_test = 'F://test_nick/'\n",
    "path_roma_test = 'F://test_roma/'\n",
    "path_noobject_test = 'F://test_noobject/'\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}